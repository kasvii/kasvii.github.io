
<!DOCTYPE html>
<html lang="en" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>【论文阅读】MEVA - Kasvii Blog</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="kasvii,"> 
    <meta name="description" content="MEVA: 3D Human Motion Estimation via Motion Compression and Refifinementgithub：https://github.com/Z,"> 
    <meta name="author" content="kasvii"> 
    <link rel="alternative" href="atom.xml" title="Kasvii Blog" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

    
<link rel="stylesheet" href="/css/diaspora.css">

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({
              google_ad_client: "ca-pub-8691406134231910",
              enable_page_level_ads: true
         });
    </script>
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
<meta name="generator" content="Hexo 5.2.0"></head>

<body class="loading">
    <span id="config-title" style="display:none">Kasvii Blog</span>
    <div id="loader"></div>
    <script type="text/javascript" src="/js/clicksakura.js"></script>
    <script async src="/js/sakurafall.js"></script>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="iconfont icon-home image-icon" href="javascript:;" data-url="https://kasvii.github.io"></a>
    <div title="播放/暂停" class="iconfont icon-play"></div>
    <h3 class="subtitle">【论文阅读】MEVA</h3>
    <div class="social">
        <div>
            <div class="share">
                <a title="获取二维码" class="iconfont icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">【论文阅读】MEVA</h1>
        <div class="stuff">
            <span>十月 19, 2021</span>
            
  <ul class="post-tags-list" itemprop="keywords"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/GRU/" rel="tag">GRU</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/SMPL/" rel="tag">SMPL</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/VAE/" rel="tag">VAE</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/%E4%B8%89%E7%BB%B4%E4%BA%BA%E4%BD%93%E9%87%8D%E5%BB%BA/" rel="tag">三维人体重建</a></li></ul>


        </div>
        <div class="content markdown">
            <h1 id="MEVA-3D-Human-Motion-Estimation-via-Motion-Compression-and-Refifinement"><a href="#MEVA-3D-Human-Motion-Estimation-via-Motion-Compression-and-Refifinement" class="headerlink" title="MEVA: 3D Human Motion Estimation via Motion Compression and Refifinement"></a>MEVA: 3D Human Motion Estimation via Motion Compression and Refifinement</h1><p>github：<a target="_blank" rel="noopener" href="https://github.com/ZhengyiLuo/MEVA">https://github.com/ZhengyiLuo/MEVA</a></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们提出了一个方法对于RGB视频序列输入，产生光滑和准确的人体姿态和运动估计。使用<strong>基于自动编码器的运动压缩</strong>和<strong>通过运动细化学习到的残差表示</strong>，将人类运动的时间序列分解为平滑运动表示。这两步编码将人类运动的捕获分为两个阶段：1. <strong>全局人体运动估计</strong>：用来捕获粗糙的一般运动；2. <strong>残差估计</strong>：添加人的特定运动细节。实验展示了我们的方法在三维人体姿态和运动的平滑性和准确性上有较好的性能。</p>
<h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>从单个视频中估计三维人体姿态序列需要一个计算模型，该模型可以提取人体运动潜在的运动学，也可以保留被捕获人所特有的运动。人类具有相同的人体结构（相同的关节数量）和相似的物理约束（关节约束），所以能够学习一个<strong>广义的运动学模型</strong>与图像相匹配来推断一个人的一般运动。但是广义的运动模型不能给特定个人的运动建模，因此需要使用图像信息添加或细化广义运动的估计结果。我们提出了两阶段三维估计方法：<strong>首先，从视频中提取粗糙的运动序列</strong>；<strong>然后，细化这个序列来产生更准确的三维运动估计</strong>。我们将推理过程分解为（1）一般运动模型和（2）特定的运动模型，来获取更准确和平滑的估计。</p>
<p>近几年，人体三维姿态估计获得了很大的进展，在<strong>单张图片</strong>和<strong>视频</strong>的人体网格重建有很好的结果。主要的指标有： Mean Per Joint Position Error (MPJPE)：平均每个关节位置误差。但缺少运动的时序平滑性的体现，存在抖动的现象，导致不自然的运动估计，优化的方法是使用抖动的姿态估计。</p>
<p>时序平滑性的问题已经有部分被解决，包括，<strong>大尺度的运动数据集</strong>（比如AMAAS）和引入<strong>对抗性损失</strong>[1] [2]来监督学习，[3]预测帧顺序（？？？）。但是只在损失函数中使用先验知识难以在平滑性和准确性中找到平衡。</p>
<p>我们认为在平滑度和精确度之间达到平衡可以通过<strong>粗糙</strong>和<strong>精细</strong>运动的分解来实现。首先我们通过观察人类运动的大数据集来学习粗糙的运动模型，因为<strong>人类的运动通常是平滑</strong>的。如果要将一个模型拟合到大量的人类运动中，那么大多数数据将位于一个运动平滑的子空间中。这意味着，如果我们要压缩人类运动的数据，它应该学习一个潜在的子空间，其中的运动是固有特性是平滑和粗糙的。以这个<strong>潜在子空间为回归目标</strong>，我们可以直接从输入视频中推理出粗糙的人体运动。但是这个运动子空间中，罕见的运动，比如突然的运动，被从运动数据中移除。为了保留这样的运动，我们还认为，产生最后的三维运动估计可以被视为一个<strong>细化步骤</strong>，将细节添加到粗运动序列中。</p>
<p>为了验证我们的论点，我们提出两阶段人体运动估计方法，1. 首先使用<strong>变分自动编码器（VAE）</strong>从数据压缩中估计粗糙的人体姿态序列，称为<strong>变分运动估计器（VME）</strong>；2. 然后把VME的输出作为姿态回归器的输入，细化姿态估计，称为<strong>运动细化回归器（MRR）</strong>。</p>
<p>我们提出一个基于视频的人体三维运动估计方法，聚焦产生平滑和准确的人体运动序列。主要贡献有：</p>
<ul>
<li>提出一个两阶段运动估计方法，保证时序平滑性和姿态估计准确定</li>
<li>描述一个鲁棒的变分自动编码器的学习过程，作为潜在人体运动子空间，从视频中估计三维人体的粗运动</li>
<li>我们在真实场景的3DPW数据集中进行姿态和运动估计的性能评测，减少了54.3%的加速度误差，同时实现了最先进的WPJPE结果。</li>
</ul>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>相关工作将介绍单张图像的<strong>人体形状和姿态恢复</strong>和视频<strong>人体运动恢复</strong>，后者是前者的子集，因为人体运动是人体姿态的一个序列。然后介绍现有方法是如何使用人体运动作为先验，和如何将运动序列映射到低维空间。</p>
<h3 id="单张图像的三维人体姿态和形状恢复"><a href="#单张图像的三维人体姿态和形状恢复" class="headerlink" title="单张图像的三维人体姿态和形状恢复"></a>单张图像的三维人体姿态和形状恢复</h3><p>这里聚焦基于模型的方法，能够同时恢复形状和姿态，我们选择使用<strong>参数化三维人体模型</strong>（SMPL、SCAPE、SMPLify-X），因为它能够容易地转换成三维人体网格。近年来，参数化人体三维重建获得了大量关注，从剪影或者人工输入，到直接拟合模型参数到二维关节位置，到直接从图像估计形状和位姿。因为缺少三维标签的这是你hi之，这些方法使用弱监督的方式来拟合三维人体到二维关节位置、身体部位分割或者稠密像素关联[4]。这些方法由于缺乏<strong>时序信息</strong>，提取的运动往往不稳定。</p>
<h3 id="基于视频的三维人体姿态和形状恢复"><a href="#基于视频的三维人体姿态和形状恢复" class="headerlink" title="基于视频的三维人体姿态和形状恢复"></a>基于视频的三维人体姿态和形状恢复</h3><p>视频人体三维重建是图像的自然拓展，[5]等（1）预测从二维到三维的关节位置，使用<strong>LSTM</strong>、<strong>时序信息</strong>和<strong>全连接层</strong>来探索时序信息。（2）HMMR、VIBE直接从图像中预测三维关节位置，并使用<strong>时序滤波器</strong>对运动序列进行后处理。<strong>HMMR</strong>通过让模型预测未来和过去帧的运动，来增强时序一致性；[4] <strong>Denserac</strong>预测被打乱的帧的顺序；<strong>VIBE</strong>使用GRU将特征的输入帧转换为一系列时间相关的潜在特征。</p>
<h3 id="人体姿态和动作先验"><a href="#人体姿态和动作先验" class="headerlink" title="人体姿态和动作先验"></a>人体姿态和动作先验</h3><p>使用预先记录的人体运动序列作为先验，已经用在许多人体运动相关的任务上。（1）学习<strong>运动捕获MoCap</strong>来帮助<strong>三维运动跟踪</strong>；（2）使用<strong>对抗性鉴别器</strong>对每帧结果进行处理，保证恢复有效的人体姿态；（3）<strong>使用预训练的VAE</strong>潜在空间；（4）时序上的鉴别器，鉴别整个运动序列。以上的方法都是用对抗的方式使用姿态和运动先验，用在损失函数中。</p>
<h3 id="人体运动表达"><a href="#人体运动表达" class="headerlink" title="人体运动表达"></a>人体运动表达</h3><p>将人体运动压缩到一个紧凑的潜在表达，能够用来生成人体运动和轨迹预测。目前的生成模型：1. <strong>VAE</strong> 2. <strong>生成流（generative flow）</strong> 3. <strong>GAN</strong></p>
<p>【生成（generation）和表达（representation）的关系是什么？？？】</p>
<h2 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h2><p>现有方法的问题：难以实现时序平滑性和精确性的平衡</p>
<p><img src="1.png" alt="1"></p>
<p>本文解决方法L提出MEVA，通过变分自动编码器进行运动估计，首先估计整体粗运动，然后添加细节运动作为残差。MEVA分三步处理输入：</p>
<ul>
<li>首先，使用<strong>时空特征提取器（STE）</strong>提取相关的<strong>时序特征</strong></li>
<li>然后，使用<strong>变分运动估计器（VME）</strong>捕获<strong>全局粗运动</strong></li>
<li>最后，使用<strong>运动残差回归器（MRR）</strong>增加精细运动的细节</li>
</ul>
<h3 id="3-1-公式"><a href="#3-1-公式" class="headerlink" title="3.1 公式"></a>3.1 公式</h3><p>$V_T=\{I_t\}^T_{t=1}$：视频输入</p>
<p>$M_T=\{\theta_t\}^T_{t=1}$：运动序列，需要重建的</p>
<p>直观上，从视频中重建人体运动不需要恢复人体形状，可以直接从视频映射到人体运动。但是和真实运动注释匹配的视频需要专业的捕捉设备，与二维姿态数据相比仍然比较罕见。在缺乏三维数据样本的情况下，我们的模型以半监督的方式学习运动序列（利用经过三维或者二维标记过关节位置的视频）。因此我们的运动估计目标就是学习函数：$V_T-&gt;M_T$</p>
<h3 id="3-2-时空特征提取器（STE）"><a href="#3-2-时空特征提取器（STE）" class="headerlink" title="3.2 时空特征提取器（STE）"></a>3.2 时空特征提取器（STE）</h3><p>过去的运动可以提供关于未来工作的线索。因此，除了单独使用前馈卷积网络来提取每一帧的视觉特征，我们还能够产生时序相关特征来更好地进行运动序列建模。</p>
<h3 id="3-3-变分运动估计器（VME）"><a href="#3-3-变分运动估计器（VME）" class="headerlink" title="3.3 变分运动估计器（VME）"></a>3.3 变分运动估计器（VME）</h3><h4 id="人体运动变分自动编码器"><a href="#人体运动变分自动编码器" class="headerlink" title="人体运动变分自动编码器"></a>人体运动变分自动编码器</h4><p>为了学习一个能够封装广泛<strong>人体运动的子空间</strong>，我们使用<strong>变分自动编码器（VAE）</strong>。VAE显示地将<strong>每个数据点映射到潜在的代码</strong>，能够高效地捕获大量可能的数据模式，对学习到的潜在空间施加一个<strong>高斯先验</strong>，相似运动的潜在代码将会相互接近。因此，VAE的潜在空间允许代码间更多的重合，能够加强潜在空间的平滑性。【具有一个平滑的潜在代码对于提高模型的通用性至关重要，因为可能的人体运动空间是高度相关和有限的？？？】。根据之前VAE的工作，目标是最大化对数似然的evidence lower bound（ELBO）</p>
<p><img src="2.png" alt="2"></p>
<p>编码器$E_{vae}$接收一系列SMPL姿态参数的人体运动的帧W，输出潜在代码$z$。解码器$D_{vae}$输入潜在代码$z$并重建运动$\hat M_W$，编码器$E_{vae}$和解码器$D_{vae}$使用GRUs。</p>
<p><img src="3.png" alt="3"></p>
<p>目标函数可以写成：</p>
<p><img src="4.png" alt="4"></p>
<h4 id="人体运动数据增强"><a href="#人体运动数据增强" class="headerlink" title="人体运动数据增强"></a>人体运动数据增强</h4><p>我们学习到的VAE应该能够推广到<strong>看不到的人体运动序列</strong>，并实现较高精度的重建，以确保学习到的潜在空间确实可以作为一个全面的人类运动子空间。我们用大规模人类运动数据集<strong>AMASS</strong>（13k不同长度的运动样本），但我们训练的VAE在看不见的序列上<strong>通用性</strong>很差。于是我们进行<strong>数据增强</strong>，来增强现有的运动并产生可行<strong>独特的人体运动</strong>序列。<strong>虽然数据增强在图像处理中得到了广泛的研究，但是在人体运动数据集方面的尝试却很少</strong>。在轨迹预测和人体运动生成的应用中，VAE潜在空间的通用性没有被广泛讨论，因为模型只需要生成新的运动序列，而不强调其编码看不见的运动序列的能力。</p>
<p>我们采用以下的数据增强方案：</p>
<ul>
<li><strong>加速和减速</strong>：对帧进行均匀上采样或者下采样（加速和减速），仍能产生合理且自然的人体运动</li>
<li><strong>左右翻转</strong>：对于相同的动作，使用左手或者右手，将仍是一个有效的人体运动，所以我们遵循SMPL模型的运动学树，<strong>镜像左右运动</strong>，生成一个新的运动序列。</li>
<li><strong>随机根旋转</strong>：从单位球中随机选取一个根旋转，来捕获可能的人体运动中不同的根方向。不同的姿态估计器可能假设不同的基平面和坐标系，采样随机根旋转有助于模型处理不同坐标系的选择。</li>
</ul>
<h4 id="从视频中学习平滑运动"><a href="#从视频中学习平滑运动" class="headerlink" title="从视频中学习平滑运动"></a>从视频中学习平滑运动</h4><p>使用VAE学习了一个全面人体运动子空间之后，我们学习了一个额外的编码器$E_{motion}$，它可以直接从视频特征中提取出粗运动序列， 映射到与$E_{vae}$相同的潜在空间。</p>
<p>输入一个视频特征序列$f_W=\{f_w\}^W_{w=1}$，编码器$E_{motion}$的作用是将输入特征压缩成一个潜在的代码$z$，将当前的观测结果表达为一个粗糙的人体运动序列。在VAE中预训练的解码器$D_{vae}$迫使编码器$E_{motion}$从预训练的运动子空间中采样，将编码器的潜在空间限制在预训练的人体运动子空间中提供一个很强的人体运动先验，有助于编码器$E_{motion}$的学习过程。结合编码器$E_{motion}$和解码器$D_{vae}$，构成了变分运动估计器（VME）</p>
<h3 id="3-4-运动残差回归器（MRR）"><a href="#3-4-运动残差回归器（MRR）" class="headerlink" title="3.4 运动残差回归器（MRR）"></a>3.4 运动残差回归器（MRR）</h3><p>使用VAE的潜在空间作为目标学习的运动序列具备固有的平滑性和粗糙性，通过信息压缩捕获当前视频帧的整体运动特征。为了捕获细节，采用SMPL回归器来<strong>迭代细化</strong>估计的姿态。MEVA中回归变量用<strong>VME计算的姿态来初始化</strong>，而之前的方法都是用平均SMPL姿态来初始化。因此，回归变量的任务只是对粗糙估计做微小的修改，来增加在压缩步骤中丢失的运动细节。</p>
<p>回归器的输入特征跟VIBE一样，使用时序视觉编码器，所以及时每帧的估计是分别计算的，但视觉特征已经是时序相关的。</p>
<p>VME使用一般的运动模型从视频中计算<strong>全局粗运动</strong>，回归器（MRR）联合细化运动和人体形状估计，相当于在每帧中<strong>添加特定的运动细节</strong>。</p>
<h3 id="3-5-训练和损失函数"><a href="#3-5-训练和损失函数" class="headerlink" title="3.5 训练和损失函数"></a>3.5 训练和损失函数</h3><p><img src="5.png" alt="5"></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>介绍了三部分：1.数据集  2. 实验  3. 消融实验</p>
<h4 id="4-1-数据集"><a href="#4-1-数据集" class="headerlink" title="4.1 数据集"></a>4.1 数据集</h4><p>VAE：</p>
<ul>
<li><p>训练AMASS</p>
</li>
<li><p>测试3DPW</p>
</li>
</ul>
<p>视频训练：</p>
<ul>
<li><p>MPI-INF-3DPH, 3DPW and human 3.6M：有三维关节标注</p>
</li>
<li><p>InstaVariety、PennAction：有二维关节标注</p>
</li>
</ul>
<h3 id="4-2-评估结果和分析"><a href="#4-2-评估结果和分析" class="headerlink" title="4.2 评估结果和分析"></a>4.2 评估结果和分析</h3><h4 id="运动变分编码器的生成"><a href="#运动变分编码器的生成" class="headerlink" title="运动变分编码器的生成"></a>运动变分编码器的生成</h4><p>记录了VAE对3DPW数据集中看不见的运动序列的重建误差运动VAE模型是使用所有三种形式的数据增强技术训练的性能最好的模型。结果表明，我们的VAE模型能够推广到看不见的序列，学习的子空间可以合理地展示人体运动空间。</p>
<p><img src="6.png" alt="6"></p>
<h4 id="定量结果"><a href="#定量结果" class="headerlink" title="定量结果"></a>定量结果</h4><p>表2展示了我们的方法在位置误差（MPJP和PA-MPJPE）取得了可比性的结果，同时显著提高了平滑度（加速度误差ACC-ERR），意味着在不牺牲精度的情况下进行更平滑、更自然的运动估计。为了进行更直接的比较，我们使用与我们方法完全相同的数据集重新训练了之前最先进的方法VIBE。之前平滑性最先进的模型是HMMR。</p>
<p><img src="7.png" alt="7"></p>
<h4 id="定性结果"><a href="#定性结果" class="headerlink" title="定性结果"></a>定性结果</h4><p><a target="_blank" rel="noopener" href="https://youtu.be/YBb9NDz3ngM">https://youtu.be/YBb9NDz3ngM</a></p>
<h3 id="4-3-消融实验"><a href="#4-3-消融实验" class="headerlink" title="4.3 消融实验"></a>4.3 消融实验</h3><h4 id="数据增强对运动VAE训练的效果"><a href="#数据增强对运动VAE训练的效果" class="headerlink" title="数据增强对运动VAE训练的效果"></a>数据增强对运动VAE训练的效果</h4><p>表格3展示了VAE在3DPW数据集中，使用不同数据增强方法后，对看不见序列的重建误差</p>
<p><img src="8.png" alt="8"></p>
<h4 id="粗运动与精细运动检索"><a href="#粗运动与精细运动检索" class="headerlink" title="粗运动与精细运动检索"></a>粗运动与精细运动检索</h4><p>MEVA从粗细运动的显式分解中受益，使用时序压缩来捕获给定序列的全局运动。表4展示了粗细运动信息的检索量。</p>
<p><img src="9.png" alt="9"></p>
<p><img src="10.png" alt="10"></p>
<h4 id="预训练运动VAE的效果"><a href="#预训练运动VAE的效果" class="headerlink" title="预训练运动VAE的效果"></a>预训练运动VAE的效果</h4><p>MEVA从预训练的运动VAE的潜在空间中受益。预训练的VAE提供了一个人体运动子空间，帮助约束运动序列更自然可信。</p>
<p><img src="11.png" alt="11"></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>为了实现时序平滑和精确的三维人体姿态估计，（1）学习一个压缩模型，编码一般人体运动的光滑性，（2）同时学习一个基于图像的回归模型，来捕获特定的运动。我们提出两阶段模型，首先，训练一个变分自动编码器给粗糙/光滑的人体运动进行建模；然后学习人体特殊运动细化回归模块，来保留一般运动模型没有捕捉到的运动。通过全面的实验，验证了我们方法的平滑性和准确性。</p>
<h2 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h2><p> <strong>Mean Per Joint Position Error (MPJPE)</strong>：平均每个关节位置误差</p>
<ul>
<li>根据视频每一帧计算的相对关节位置</li>
<li><p>但缺少运动的时序平滑性的体现</p>
<p><strong>MPJPE after Procrustes Alignment (PA-MPJPE)</strong>：执行程序对齐后的MPJPE</p>
</li>
<li><p>对齐三维关节的根位置（盆骨）后计算MPJPE</p>
<p>MPJPE和PA-MPJPE都是运动估计器的准确性指标（$mm$）</p>
</li>
</ul>
<p><strong>Acceleration error (ACC-ERR)</strong>：加速度误差</p>
<ul>
<li><p>测量预测结果和真实值中每个关键点的三维加速度误差（单位$mm/s^2$）</p>
</li>
<li><p>ACC-ERR是估计运动序列的主要的平滑度指标</p>
</li>
<li>加速度使用有限差分方法计算得到</li>
</ul>
<p>以上指标需要共同使用：低的<strong>位置误差</strong>表明运动的捕获<strong>整体的正确性</strong>，而好的<strong>加速度误差</strong>标志着<strong>平滑和自然</strong>的人体运动估计。</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><ul>
<li>Archive of Motion Capture as Surface Shapes（AMAAS）：Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: Amass: Archive of motion capture as surface shapes. 2019 IEEE/CVF International Conference on Computer Vision (ICCV) (2019) 5441–5450</li>
<li>MPI-INF-3DPH</li>
<li>3DPW</li>
<li>human 3.6M</li>
<li>InstaVariety</li>
<li>PennAction</li>
</ul>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>smooth motion representation - 平滑运动表示</p>
<p>auto-encoder-based motion compression - 基于自动编码器的运动压缩</p>
<p>residual representation - 残差表示</p>
<p>motion refinement - 运动细化</p>
<p>generalized kinematic model - 广义运动学模型</p>
<p>human mesh recovery - 人体三维重建</p>
<p>adversarial loss - 对抗性损失</p>
<p>data compression - 数据压缩</p>
<p>【Variational Autoencoder（VAE）- 变分自动编码器】</p>
<p>Variational Motion Estimator（VME）- 变分运动估计器</p>
<p>Motion Residual Regressor（MRR）- 运动细化回归器</p>
<p>body part segmentation - 身体部分分割</p>
<p>dense pixel correspondence - 图像稠密像素关联</p>
<p>LSTM</p>
<p>temporal convolution - 时序卷积</p>
<p>fully connected layers - 全连接层</p>
<p>motion capture（MoCap）- 运动捕获</p>
<p>3D motion tracking - 三维运动跟踪</p>
<p>adversarial discriminator - 对抗性鉴别器</p>
<p>human motion representation - 人体运动表达</p>
<p>compact latent representation  - 紧凑的潜在表达</p>
<p>spatio-temporal feature extractor（STE）- 时空特征提取器</p>
<p>linear coefficient - 线性系数</p>
<p>weak perspective camera - 弱视角相机</p>
<p>feed-forward convolutional network - 前馈卷积网络</p>
<p>human motion subspace - 人体运动子空间</p>
<p>Gaussian prior - 高斯先验</p>
<p>Gaussian parameterization of the VAE - VAE的高斯参数化</p>
<p>data augmentation - 数据增强</p>
<p>kinematic tree - 运动学树</p>
<p>finite difference - 有限差分</p>
<p>explicit breakdown - 显式分解</p>
<p>kinematically invalid - 运动学无效</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Kocabas, M., Athanasiou, N., Black, M.J.: Vibe: Video inference for human body pose and shape estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. (2020) 5253–5263</p>
<p>[2] Kanazawa, A., Zhang, J.Y., Felsen, P., Malik, J.: Learning 3d human dynamics from video. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 5607–5616</p>
<p>[3] Xu, Y., Zhu, S.C., Tung, T.: Denserac: Joint 3d pose and shape estimation by dense render-and-compare. 2019 IEEE/CVF International Conference on Computer Vision (ICCV) (2019) 7759–7769</p>
<p>[4] Xu, Y., Zhu, S.C., Tung, T.: Denserac: Joint 3d pose and shape estimation by dense render-and-compare. 2019 IEEE/CVF International Conference on Computer Vision (ICCV) (2019) 7759–7769</p>
<p>[5] Xu, J., Yu, Z., Ni, B., Yang, J., Yang, X., Zhang, W.: Deep kinematics analysis for monocular 3d human pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). (2020)</p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="true">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        
                            <li title='0' data-url='http://music.163.com/song/media/outer/url?id=1432130357.mp3'></li>
                        
                    
                        
                            <li title='1' data-url='http://music.163.com/song/media/outer/url?id=1430898876.mp3'></li>
                        
                    
                </ul>
            
        </div>
        
    <div id='gitalk-container' class="comment link"
		data-enable='true'
        data-ae='false'
        data-ci='a3a33c2a30c8f056252b'
        data-cs='7e3c9dce2acd3d4d59d679c980f33c24de2c29ad'
        data-r='CommentData'
        data-o='kasvii'
        data-a='kasvii'
        data-d='false'
    >查看评论</div>


    </div>
    
        <div class='side'>
			<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#MEVA-3D-Human-Motion-Estimation-via-Motion-Compression-and-Refifinement"><span class="toc-number">1.</span> <span class="toc-text">MEVA: 3D Human Motion Estimation via Motion Compression and Refifinement</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E5%85%A5"><span class="toc-number">1.2.</span> <span class="toc-text">引入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">1.3.</span> <span class="toc-text">相关工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E5%BC%A0%E5%9B%BE%E5%83%8F%E7%9A%84%E4%B8%89%E7%BB%B4%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E5%92%8C%E5%BD%A2%E7%8A%B6%E6%81%A2%E5%A4%8D"><span class="toc-number">1.3.1.</span> <span class="toc-text">单张图像的三维人体姿态和形状恢复</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E8%A7%86%E9%A2%91%E7%9A%84%E4%B8%89%E7%BB%B4%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E5%92%8C%E5%BD%A2%E7%8A%B6%E6%81%A2%E5%A4%8D"><span class="toc-number">1.3.2.</span> <span class="toc-text">基于视频的三维人体姿态和形状恢复</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E5%92%8C%E5%8A%A8%E4%BD%9C%E5%85%88%E9%AA%8C"><span class="toc-number">1.3.3.</span> <span class="toc-text">人体姿态和动作先验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E4%BD%93%E8%BF%90%E5%8A%A8%E8%A1%A8%E8%BE%BE"><span class="toc-number">1.3.4.</span> <span class="toc-text">人体运动表达</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.</span> <span class="toc-text">3. 方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E5%85%AC%E5%BC%8F"><span class="toc-number">1.4.1.</span> <span class="toc-text">3.1 公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%97%B6%E7%A9%BA%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8%EF%BC%88STE%EF%BC%89"><span class="toc-number">1.4.2.</span> <span class="toc-text">3.2 时空特征提取器（STE）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%8F%98%E5%88%86%E8%BF%90%E5%8A%A8%E4%BC%B0%E8%AE%A1%E5%99%A8%EF%BC%88VME%EF%BC%89"><span class="toc-number">1.4.3.</span> <span class="toc-text">3.3 变分运动估计器（VME）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%BA%E4%BD%93%E8%BF%90%E5%8A%A8%E5%8F%98%E5%88%86%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">1.4.3.1.</span> <span class="toc-text">人体运动变分自动编码器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%BA%E4%BD%93%E8%BF%90%E5%8A%A8%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">1.4.3.2.</span> <span class="toc-text">人体运动数据增强</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8E%E8%A7%86%E9%A2%91%E4%B8%AD%E5%AD%A6%E4%B9%A0%E5%B9%B3%E6%BB%91%E8%BF%90%E5%8A%A8"><span class="toc-number">1.4.3.3.</span> <span class="toc-text">从视频中学习平滑运动</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E8%BF%90%E5%8A%A8%E6%AE%8B%E5%B7%AE%E5%9B%9E%E5%BD%92%E5%99%A8%EF%BC%88MRR%EF%BC%89"><span class="toc-number">1.4.4.</span> <span class="toc-text">3.4 运动残差回归器（MRR）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.4.5.</span> <span class="toc-text">3.5 训练和损失函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">1.5.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.5.0.1.</span> <span class="toc-text">4.1 数据集</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E8%AF%84%E4%BC%B0%E7%BB%93%E6%9E%9C%E5%92%8C%E5%88%86%E6%9E%90"><span class="toc-number">1.5.1.</span> <span class="toc-text">4.2 评估结果和分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%90%E5%8A%A8%E5%8F%98%E5%88%86%E7%BC%96%E7%A0%81%E5%99%A8%E7%9A%84%E7%94%9F%E6%88%90"><span class="toc-number">1.5.1.1.</span> <span class="toc-text">运动变分编码器的生成</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E9%87%8F%E7%BB%93%E6%9E%9C"><span class="toc-number">1.5.1.2.</span> <span class="toc-text">定量结果</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E6%80%A7%E7%BB%93%E6%9E%9C"><span class="toc-number">1.5.1.3.</span> <span class="toc-text">定性结果</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-number">1.5.2.</span> <span class="toc-text">4.3 消融实验</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%AF%B9%E8%BF%90%E5%8A%A8VAE%E8%AE%AD%E7%BB%83%E7%9A%84%E6%95%88%E6%9E%9C"><span class="toc-number">1.5.2.1.</span> <span class="toc-text">数据增强对运动VAE训练的效果</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B2%97%E8%BF%90%E5%8A%A8%E4%B8%8E%E7%B2%BE%E7%BB%86%E8%BF%90%E5%8A%A8%E6%A3%80%E7%B4%A2"><span class="toc-number">1.5.2.2.</span> <span class="toc-text">粗运动与精细运动检索</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E8%BF%90%E5%8A%A8VAE%E7%9A%84%E6%95%88%E6%9E%9C"><span class="toc-number">1.5.2.3.</span> <span class="toc-text">预训练运动VAE的效果</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">1.6.</span> <span class="toc-text">结论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8C%87%E6%A0%87"><span class="toc-number">1.7.</span> <span class="toc-text">指标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.8.</span> <span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E8%AF%8D"><span class="toc-number">1.9.</span> <span class="toc-text">关键词</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">1.10.</span> <span class="toc-text">参考文献</span></a></li></ol></li></ol>	
        </div>
    
</div>


    </div>
</div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>


<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/typed.js"></script>
<script src="/js/diaspora.js"></script>


<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">


<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>




</html>

