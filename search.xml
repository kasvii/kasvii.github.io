<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2020/10/07/hello-world/</url>
    <content><![CDATA[<p>Welcome to my blog!</p>
]]></content>
  </entry>
  <entry>
    <title>矩阵求导</title>
    <url>/2020/10/07/matrix-derivative/</url>
    <content><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>第一次接触矩阵导数是暑期课Frontier Approaches of Control Science的作业用最小二乘法做线性回归。在控制理论和机器学习领域，矩阵求导更是一个常用的数学工具。</p>
<h2 id="矩阵求导的本质"><a href="#矩阵求导的本质" class="headerlink" title="矩阵求导的本质"></a>矩阵求导的本质</h2><p>矩阵A对矩阵B求导，本质上是多元函数求导，也就是矩阵A中的每一个元素对矩阵B中的每一个元素求导，再将求导的结果排列成矩阵的形式。到这里，矩阵求导似乎就讲完了，剩下的就是复合函数求导和求偏导的计算。但是我们很快就发现这种逐元素求导的方法很复杂，随着元素的增加，计算量也极大地增加。那么，有没有直接用矩阵运算，从整体出发的算法。</p>
<h2 id="矩阵求导的形式"><a href="#矩阵求导的形式" class="headerlink" title="矩阵求导的形式"></a>矩阵求导的形式</h2><p>常见的矩阵求导有以下六种，分别是标量对标量求导、标量对向量求导、标量对矩阵求导、向量对标量求导、向量对向量求导和矩阵对标量求导。</p>
<p><img src="pic1.png" alt="pic1"></p>
<h2 id="两种布局"><a href="#两种布局" class="headerlink" title="两种布局"></a>两种布局</h2><p>我们上面提到矩阵求导的本质是矩阵A中的每一个元素对矩阵B中的每一个元素求导，再将求导结果排列成矩阵的形式。对于两个向量的求导结果一般有两种排列方式，分别是分子布局（XY拉伸术）和分母布局（YX拉伸术）。</p>
<p>$\frac{\partial Y}{\partial X}$的布局规则：1.标量不变，向量拉伸 2.前面横向拉，后面纵向拉</p>
<h3 id="分子布局（XY拉伸术）"><a href="#分子布局（XY拉伸术）" class="headerlink" title="分子布局（XY拉伸术）"></a>分子布局（XY拉伸术）</h3><p>对于$\frac{\partial Y}{\partial X}$，分子布局的方法是XY拉伸术。先判断X和Y是不是向量，若二者都是向量，根据布局规则，X在前所以横向拉伸，Y在后所以纵向拉伸，具体过程如下：</p>
<p>标量/向量（在分子布局下，Y是标量，不变；X是向量，横向拉伸）</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial \mathbf x}=[\frac{\partial y}{\partial x_1}...\frac{\partial y}{\partial x_n}]</script><p>向量/标量（在分子布局下，Y是向量，纵向拉伸；X是标量，不变）</p>
<script type="math/tex; mode=display">
\frac{\partial \mathbf y}{\partial x}=\large \begin{bmatrix} \frac{\partial y_1}{\partial x}\\ \vdots\\ \frac{\partial y_m}{\partial x}\end{bmatrix}</script><p>向量/向量（在分子布局下，Y是向量，纵向拉伸；X也是向量，横向拉伸）</p>
<script type="math/tex; mode=display">
\frac{\partial \mathbf y}{\partial \mathbf x}=\large \begin{bmatrix} \frac{\partial y_1}{\partial x_1} \dotso \frac{\partial y_1}{\partial x_n} \\ \vdots \: \: \:  \ddots \: \: \: \vdots \\ \frac{\partial y_m}{\partial x_1} \dotso \frac{\partial y_m}{\partial x_n}\end{bmatrix}</script><h3 id="分母布局（YX拉伸术）"><a href="#分母布局（YX拉伸术）" class="headerlink" title="分母布局（YX拉伸术）"></a>分母布局（YX拉伸术）</h3><p>同样对于$\frac{\partial Y}{\partial X}$，分母布局的方法是YX拉伸术。若二者都是向量，根据布局规则，Y在前所以横向拉伸，X在后所以纵向拉伸，具体过程如下：</p>
<p>标量/向量（在分母布局下，Y是标量，不变；X是向量，纵向拉伸）</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial \mathbf x}=\large \begin{bmatrix} \frac{\partial y}{\partial x_1}\\ \vdots\\ \frac{\partial y}{\partial x_n}\end{bmatrix}</script><p>向量/标量（在分母布局下，Y是向量，横向拉伸；X是标量，不变）</p>
<script type="math/tex; mode=display">
\frac{\partial \mathbf y}{\partial x}=[\frac{\partial y_1}{\partial x}...\frac{\partial y_m}{\partial x}]</script><p>向量/向量（在分母布局下，Y是向量，横向拉伸；X也是向量，纵向拉伸）</p>
<script type="math/tex; mode=display">
\frac{\partial \mathbf y}{\partial \mathbf x}=\large \begin{bmatrix} \frac{\partial y_1}{\partial x_1} \dotso \frac{\partial y_m}{\partial x_1} \\ \vdots \: \: \:  \ddots \: \: \: \vdots \\ \frac{\partial y_1}{\partial x_n} \dotso \frac{\partial y_m}{\partial x_n}\end{bmatrix}</script><p>分子布局和分母布局互为转置的关系：</p>
<ul>
<li>(分子布局)$^{T}$=分母布局</li>
<li>(分母布局)$^{T}$=分子布局</li>
</ul>
<p>在控制理论等领域的雅可比矩阵采用的是分子布局</p>
<p>在机器学习的梯度矩阵中采用的是分母布局</p>
<h2 id="常用的公式"><a href="#常用的公式" class="headerlink" title="常用的公式"></a>常用的公式</h2><p>（a, <strong>a</strong>, A分别是与标量x和向量<strong>x</strong>无关的标量、向量和矩阵）</p>
<p><img src="pic2.png" alt="pic2"></p>
<h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p>这里利用最小二乘法的例子来演示矩阵的整体求导。</p>
<p><img src="pic3.png" alt="pic3"></p>
<p>损失函数定义为y的实际值与拟合曲线对应值之差的平方：</p>
<script type="math/tex; mode=display">
L(\mathbf b)=\sum_{i=1}^n(y_i-\mathbf x_i^T\mathbf b)^2</script><p>用矩阵表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned}L(\mathbf b) &=(\mathbf Y-\mathbf x\mathbf b)^T(\mathbf Y-\mathbf x\mathbf b)\\ &=(\mathbf Y^T-\mathbf b^T \mathbf x^T)(\mathbf Y-\mathbf x\mathbf b)\\ &=\mathbf Y^T\mathbf Y-\mathbf Y^T \mathbf x \mathbf b-\mathbf b^T \mathbf x^T \mathbf Y+\mathbf b^T \mathbf x^T\mathbf x\mathbf b\\ &=\mathbf Y^T\mathbf Y-2\mathbf Y^T \mathbf x \mathbf b+\mathbf b^T \mathbf x^T\mathbf x\mathbf b
\end{aligned}</script><p>要找到一组系数向量<strong>b</strong>使得损失函数最小，将损失函数对<strong>b</strong>求导：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\text{d}L(\mathbf b)}{\text{d}\mathbf b} &=\frac{\text{d}\mathbf Y^T \mathbf Y}{\text{d}\mathbf b}-2\frac{\text{d}\mathbf Y^T \mathbf x \mathbf b}{\text{d}\mathbf b}+\frac{\text{d}\mathbf b^T \mathbf x^T \mathbf x \mathbf b}{\text{d}\mathbf b}\\ &=\mathbf 0-2(\mathbf Y^T \mathbf x)^T+2\mathbf x^T \mathbf x\mathbf b\\ &=-2 \mathbf x^T \mathbf Y+2\mathbf x^T \mathbf x \mathbf b=\mathbf 0
\end{aligned}</script><p>得到</p>
<script type="math/tex; mode=display">
\hat b=(\mathbf x^T \mathbf x)^{-1}\mathbf x^T\mathbf Y</script>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>矩阵</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
</search>
