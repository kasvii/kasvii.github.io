<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello！</title>
    <url>/2021/10/26/hello-world/</url>
    <content><![CDATA[<p>Welcome to my blog!</p>
]]></content>
  </entry>
  <entry>
    <title>【论文阅读】TCMR</title>
    <url>/2021/10/25/TCMR/</url>
    <content><![CDATA[<h1 id="TCMR-Beyond-Static-Features-for-Temporally-Consistent-3D-Human-Pose-and-Shape-from-a-Video"><a href="#TCMR-Beyond-Static-Features-for-Temporally-Consistent-3D-Human-Pose-and-Shape-from-a-Video" class="headerlink" title="TCMR: Beyond Static Features for Temporally Consistent 3D Human Pose and Shape from a Video"></a>TCMR: Beyond Static Features for Temporally Consistent 3D Human Pose and Shape from a Video</h1><p>github：<a href="https://github.com/hongsukchoi/TCMR_RELEASE">https://github.com/hongsukchoi/TCMR_RELEASE</a></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>尽管近期在单图像人体三维重建上有很多成功的工作，但是从视频中重建具有时间一致性和光滑三维运动的人体仍然具有很大的挑战性。由于强烈地依赖当前帧的静态特征，基于视频的方法难以解决单图像方法中的时序不一致问题。在这点上，我们提出了一个时序连续网格重建系统（TCMR），能够高效地关注于过去帧和未来帧的时序信息，不被静态特征所主导。我们的TCMR比之前基于视频的方法有更好的每帧三维位姿和形状。</p>
<h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>目前有许多工作从图像来分析人体，从简单的二维骨架估计到重建人体的三维姿态和形状。尽管有所进展，但是从二维图像重建三维人体依然充满挑战，特别是在单目的情况下，存在深度不确定、数据集有限、人体关节复杂的困难。</p>
<p>之前的许多工作，比如Pose2Mesh[1]、I2L-MeshNet[2]基于参数化三维人体网格模型，直接从输入的单张图片回归模型参数。虽然在静态图片上有合理的输出，但是应用到视频上就会产生时序不一致和运动不平滑的问题。时序不稳定性是由于连续帧中不一致的三维位姿误差。误差可能大声在不同的三维方向，或者后续帧位姿输出可能保持相对不变，不能反映运动。</p>
<p>[3] [4] [5]扩展了单张图像重建到视频的方法，他们把视频序列输入到预训练的单帧图像三维人体重建网络[6] [7]来获得序列的静态特征，然后再送到时序编码器，对每个输入帧编码一个时序特征。然后，人体参数回归器从相应时间步长的时序特征输出每帧SMPL参数。</p>
<p>尽管这些工作极大提高了每帧三维位姿正确性和运动的平滑性，但仍然存在时序不一致性。重建失败的原因是因为对<strong>当前帧静态特征</strong>的强烈依赖。强烈依赖的原因是：</p>
<ul>
<li><p>当前帧静态特征和时序特征的残差连接。虽然参擦汗链接已经被验证能够促进学习过程，但是纯粹地将残差连接用于时间编码可能会阻碍系统学习有用的时序信息。假设静态特征是从预训练网络提取出来的，它包含了当前帧很重的SMPL参数，静态特征的残差连接恒等映射可能会使得SMPL参数回归器严重依赖于残差连接，而只是略微利用到时序特征，限制了时序编码器编码更有用的时序特征。</p>
</li>
<li><p>时序编码使用了所有帧的静态特征，当前帧的静态特征对当前时序特征有最大的影响潜力。当前工作尽管能够重建当前帧很高的人体精度，但是阻碍了时序编码器学习过去和未来的时序信息。因此基于视频的人体三维重建存在时序不一致的问题。</p>
</li>
</ul>
<p>于是，我们提出TCMR - 时序连续网格重建系统，用来解决对当前静态特征的强烈依赖，以实现时序一致性和视频的平滑的三维人体运动输出。</p>
<ul>
<li>首先，fellow之前基于视频的工作[3] [4] [5]，但是移除了静态特征和时序特征之间的残差连接。</li>
<li>引入PoseForecast，由两个时序编码器构成，分别利用过去和未来帧来预测当前帧的位姿。因此PoseForecast的时序特征不受当前静态特征影响。</li>
<li>当前时序特征是从所有输入帧提取出来的。</li>
<li>PoseForecast预测的时序特征和输入帧的时序特征融合，用来预测当前的SMPL参数。</li>
</ul>
<p>通过移除当前静态特征的强依赖，我们的SMPL回归器能够专注于过去和未来帧，而不是只有当前帧。</p>
<p>论文的贡献：</p>
<ul>
<li>提出了一个时序一致的网格重建系统（TCMR），能够从视频中产生时序一致性和平滑的三维人体运动，高效地利用了过去和未来的时序信息，而不是只关注与当前帧的静态信息。</li>
<li>虽然简单，TCRM不仅提高了三维人体运动的时序一致性，也提高了每帧的位姿和形状的准确性</li>
</ul>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="基于单张图像的三维人体姿态和形状估计"><a href="#基于单张图像的三维人体姿态和形状估计" class="headerlink" title="基于单张图像的三维人体姿态和形状估计"></a>基于单张图像的三维人体姿态和形状估计</h3><p>许多单张图像的人体三维重建都是基于模型的方法，来预测预定义的三维人体网格模型SMPL的参数。[6]提出了一个<strong>端到端可训练的人体网格重建（HMR）</strong>系统，使用<strong>对抗性损失</strong>使得输出的人体三维网格是解剖学上合理的。[8]使用<strong>二维关节热图</strong>和轮廓来预测正确的SMPL参数。[9]使用<strong>人体部分分割</strong>来回归SMPL。[10]使用<strong>多视图颜色一致性</strong>来监督网络形成多视图几何。[11]引入<strong>自我改善系统</strong>来约束SMPL参数回归器并<strong>迭代拟合框架</strong>。[12]融合人体<strong>层次运动学先验</strong>到网络中。</p>
<p>相反，无模型的方法直接估计形状而不是回归模型的参数。[13] BodyNet在<strong>三维体积空间</strong>中，评估三维人体形状。[14] 设计了一种<strong>图卷积人体网格回归系统</strong>，以<strong>模板人体网格</strong>作为输入，利用来自ResNet的图像特征预测网格顶点坐标系。[2] I2L-MeshNet引入了一种基于<strong>lixel</strong>（线+像素）的一维热图，以全卷积的方式来定位网格顶点。[1] Pose2Mesh提出从二维人体位姿重建三维人体位姿的<strong>图卷积网络</strong>。</p>
<p>但基于图片的工作存在时序不一致性。</p>
<h3 id="基于视频的三维人体位姿和形状估计"><a href="#基于视频的三维人体位姿和形状估计" class="headerlink" title="基于视频的三维人体位姿和形状估计"></a>基于视频的三维人体位姿和形状估计</h3><p>[15] HMMR使用<strong>一维全卷积时序编码器</strong>来提取静态特征并编码到时序特征。它通过预测附近的过去和未来帧的三维位姿，来学习时序上下文表达来减少三维预测的时序不一致性。[16] 利用<strong>光流</strong>和二维位姿序列来训练他们的网络，从而在视频上有很好的表现。[17] 提出<strong>骨架解耦框架</strong>，把三维人体姿态和形状分解成多层次的时空子问题。他们排序打乱的帧，来鼓励时间特征的学习。[4] VIBE使用<strong>双向GRU</strong>将输入帧的静态特征编码到时序特征，并将时序特征输入SMPL参数回归器，利用<strong>运动鉴别器</strong>来鼓励回归器产生真实的三维人体运动。[5]MEVA使用<strong>由粗到细的方式</strong>，他们的系统首先使用<strong>变分运动估计器（VME）</strong>评估粗糙的三维人体运动，然后使用运动<strong>残余回归器（MRR）</strong>预测残余的运动。</p>
<h3 id="视频的时序一致三维人体运动"><a href="#视频的时序一致三维人体运动" class="headerlink" title="视频的时序一致三维人体运动"></a>视频的时序一致三维人体运动</h3><p>[15] HMMR引入了时序一致性和人体运动平滑性<strong>三维姿态加速度误差</strong>，和单张图像的方法相比，HMMR和VIBE降低了加速度误差。但是，他们在每帧的正确性和时序的一致性上做了<strong>权衡（trade-off）</strong>，HMMR输出了更加平滑的三维人体运动但是减少了每帧三维位姿的准确性。VIBE展示了较高的每帧三维位姿准确性，但是和HMMR相比在定量指标和定性结果上具有时序不一致性。</p>
<p>[5] MEVA尝试建立每帧三维位姿准确性和时序平滑性的平衡，尽管在两个指标上都展示了更好的结果，但是定性结果依然暴露出不平滑的三维运动。原因是系统过于依赖当前的静态特征来估计当前三维位姿和形状。</p>
<ul>
<li>首先，MEVA建立了当前帧的静态特征和时序特征的残差连接；</li>
<li>用<strong>运动残差回归器（MRR）</strong>来细化初始三维姿态和形状的时序特征，是从所有帧的静态特征编码来的，包括当前帧。这个过程会导致时序特征被当前静态特征主导。</li>
<li>因此，这个精细化过程会收到当前静态特征的严重影响，连续帧的三维误差出现不一致。</li>
</ul>
<p>相反，TCMR被有意设计来减小对静态特征的严重依赖，残差连接被移除，PoseForecast网络预测来自过去和未来帧额外的时序特征。我们的方法减少了依赖并提供三维人体运动的时序一致性和准确性。</p>
<h3 id="从图像中预测三维人体位姿"><a href="#从图像中预测三维人体位姿" class="headerlink" title="从图像中预测三维人体位姿"></a>从图像中预测三维人体位姿</h3><p>[15] [18] [19]提出从RGB输入预测人体未来三维位姿。[15]HMMR使用hallucinator（幻觉器？）从当前输入图像预测当前、未来和过去的三维姿态，并由以为全卷积时序编码器的输出进行自监督。[19] 提出神经自回归框架，以过去的视频帧为输入来预测未来的三维运动。 [18] 采用深度强化学习来预测未来三维人体位姿 。虽然目标大欧式预测未来三维位姿，我们的系统目标是从静态特征中解放出来，学习有用的时序特征。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>TCMR的框架如下：</p>
<p><img src="1.png" alt="1"></p>
<h3 id="1-所有帧的时序编码"><a href="#1-所有帧的时序编码" class="headerlink" title="1. 所有帧的时序编码"></a>1. 所有帧的时序编码</h3><p>给定视频的RGB序列，经过[11]预训练的残差网络，提取每帧的静态特征。然后将残差网络的输出进行全局平均池化，得到$f$。ResNet网络的权重由所所有帧共享。</p>
<p>利用所有输入帧的静态特征我们利用双向GRU计算当前帧的时序特征，这两个双向GRU从相反的时间方向上从输入静态特征中提取时序特征。</p>
<ul>
<li>两个双向GRU的输入分别是第一个$f_1$和最后一个的$f_T$，他们的初始隐藏状态都是零张量。</li>
<li>然后，他们通过聚合下一帧的静态特征来反复更新隐藏状态。</li>
<li>在当前帧的GRU的连接隐藏状态成为当前时序特征$g_{all}$。不像VIBE，我们没有在$f_{T/2}$和$g_{all}$之间加入残差连接，所以时序特征不会被静态特征主导。</li>
</ul>
<h3 id="2-PoseForecast的时序编码"><a href="#2-PoseForecast的时序编码" class="headerlink" title="2. PoseForecast的时序编码"></a>2. PoseForecast的时序编码</h3><p>PoseForecast网络使用两个额外的GRU，记为$G_{past}$和$G_{future}$，预测来自过去和未来帧额外的时序特征。过去帧定义为：1到当前帧-1，未来帧定义为：当前帧+1到结束帧。$G_{past}$的初始输入是$f1$，初始隐藏状态是零张量。然后通过不断聚合接下来的帧$f_2,…f_{T/2-1}$来反复更新隐藏状态。同样，$G_{future}$的初始输入是$f_T$，初始隐藏状态是零张量。然后通过不断聚合接下来的帧$f_{T-1},…f_{T/2+1}$来反复更新隐藏状态。</p>
<h3 id="3-时序特征融合"><a href="#3-时序特征融合" class="headerlink" title="3. 时序特征融合"></a>3. 时序特征融合</h3><p>我们将所有帧的时序特征$g_{all}$、过去帧的时序特征$g_{past}$、未来帧的时序特征$g_{future}$进行融合得到最后的三维网格估计，如下图。</p>
<p><img src="2.png" alt="2"></p>
<ul>
<li>每个时序特征都经过ReLU激活函数和全连接层，来改变维数到2048，输出记为$g’_{all}$、$g’_{past}$、$g’_{future}$</li>
<li>然后，通过共享的全连接层将输出特征调整到256并连接</li>
<li>连接后的特征通过许多全连接层，后面都跟着softmax激活函数，产生注意值$a_{all}$、$a_{past}$、$a_{future}$。注意值代表特征应该得到的权重。</li>
<li>最后的的融合时序特征为：$g’_{int}=a_{all}g’_{all}+a_{past}g’_{past}+a_{future}g’_{future}$</li>
</ul>
<p>训练的时候，我们将$g’_{all}$、$g’_{past}$、$g’_{future}$扔到SMPL参数回归器，分别得到每个时序特征的参数。最后使用$\Theta_{int}$作为最后的三维人体网格。</p>
<h3 id="4-损失函数"><a href="#4-损失函数" class="headerlink" title="4. 损失函数"></a>4. 损失函数</h3><p>L2损失函数：（1）预测值和真实值SMPL参数；（2）二维和三维关节坐标，跟VIBE一样，三维坐标由SMPL参数获得，二维关节坐标由三维关节坐标通过相机参数的反投影获得。</p>
<h2 id="实施细节"><a href="#实施细节" class="headerlink" title="实施细节"></a>实施细节</h2><p>用预训练好的[20]SPIN作为backbone和回归器。权重使用Adam优化器优化。人体区域用之前的工作裁剪出来，并调整到224*224。对对象进行遮挡，以增强数据。用ResNet从裁剪的图像中计算静态特征。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验指标"><a href="#实验指标" class="headerlink" title="实验指标"></a>实验指标</h3><h4 id="每帧准确性的指标："><a href="#每帧准确性的指标：" class="headerlink" title="每帧准确性的指标："></a>每帧准确性的指标：</h4><ul>
<li>mean per joint position error（MPJPE）、Procrustes-aligned MPJPE（PA-MPJPE）、mean per vertex position error（MPVPE）</li>
<li>每个关节的平均位置误差，首先对齐根关节，然后测量估计值和真实值（mm）的误差</li>
<li>程序对齐的MPJPE，作为每帧正确率的主要指标，因为它包括输出的尺度模糊性对误差的影响。</li>
</ul>
<h4 id="时序评估的指标"><a href="#时序评估的指标" class="headerlink" title="时序评估的指标"></a>时序评估的指标</h4><ul>
<li>加速度误差：计算每个关节的平均预测和真实加速度的误差（$mm/s^2$）</li>
</ul>
<h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><h4 id="去除残差连接的有效性"><a href="#去除残差连接的有效性" class="headerlink" title="去除残差连接的有效性"></a>去除残差连接的有效性</h4><ul>
<li><p>使用了和VIBE一样的baseline：只有一个双向GRU，编码所有输入帧的时序特征，静态和时序特征之间有残差连接。它可以预测所有输入帧的三维位姿和形状，但是没有使用运动鉴别器。</p>
<p><img src="3.png" alt="3"></p>
</li>
<li><p>去除残差连接明显降低了加速度误差，说明残差连接中静态误差的恒等映射阻碍了模型学习有意义的时序特征</p>
</li>
<li><p>三维运动的时序一致性的提高也促进了每帧三维位姿的准确性。</p>
</li>
</ul>
<h4 id="PoseForecast的有效性"><a href="#PoseForecast的有效性" class="headerlink" title="PoseForecast的有效性"></a>PoseForecast的有效性</h4><ul>
<li><p>无论有残差连接如何，PoseForecast连续提高每帧和时序的指标。</p>
<p><img src="4.png" alt="4"></p>
</li>
<li><p>保持时序特征和当前帧静态特征的无关性，能够提高时序一致性和运动平滑性</p>
</li>
<li><p>使用不用的监督方式对结果的影响，用当前帧参数监督效果最好</p>
</li>
</ul>
<h4 id="和SOTA方法的对比"><a href="#和SOTA方法的对比" class="headerlink" title="和SOTA方法的对比"></a>和SOTA方法的对比</h4><ul>
<li><p>和基于视频的方法比较</p>
<p><img src="5.png" alt="5"></p>
<p><img src="6.png" alt="6"></p>
<p><img src="8.png" alt="8"></p>
<p><img src="9.png" alt="9"></p>
</li>
<li><p>和基于单帧图像和视频方法的比较</p>
<p><img src="7.png" alt="7"></p>
</li>
</ul>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>我们提出了从RGB视频中重建三维人体网格的TCMR模型。之前基于视频的工作由于强烈地依赖当前帧的静态特征，存在时序不一致的问题。我们通过移除静态特征和时序特征的残差连接，采用PoseForecast来预测过去帧和未来帧的时序特征，融合得到当前帧的时序特征。和其他基于视频的方法相比，TCMR提供了时序一致的三维运动和更准确的三维姿态估计。</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><ul>
<li>3DPW：Timo von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn,  and Gerard Pons-Moll. Recovering accurate 3d human pose in the wild using imus and a moving camera. <em>ECCV</em>, 2018.</li>
<li>Human3.6M：Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. <em>TPAMI</em>, 2014.</li>
<li>MPI-INF-3DHP：Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian Theobalt. Monocular 3d human pose estimation in the wild using improved cnn supervision. <em>3DV</em>, 2017. </li>
<li>Insta Variety：Angjoo Kanazawa, Jason Y Zhang, Panna Felsen, and Jitendra Malik. Learning 3D human dynamics from video. <em>CVPR</em>, 2019. </li>
<li>Penn Ation：Weiyu Zhang, Menglong Zhu, and Konstantinos G Derpanis. From actemes to action: A strongly-supervised representation for detailed action understanding. <em>ICCV</em>, 2013.</li>
<li>PoseTrack： Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton Milan, Juergen Gall, and Bernt Schiele. Posetrack: A benchmark for human pose estimation and tracking. <em>CVPR</em>, 2018.</li>
</ul>
<p>3DPW是只有包括真实SMPL参数的数据集</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>temporal consistency - 时序一致性</p>
<p>smooth 3D human motion - 光滑三维人体运动</p>
<p>static feature - 静态特征</p>
<p>depth ambiguity - 深度不确定性</p>
<p><strong>residual connection - 残差连接</strong></p>
<p>pretrained network - 预训练网络</p>
<p>identity mapping - 恒等映射</p>
<p>human mesh recovery system（HMR）- 人体网格重建系统</p>
<p>anatomically plausible - 解剖学上合理</p>
<p>2D joint heatmap - 二维关节热图</p>
<p>human part segmentation - 人体部分分割</p>
<p>multi-view color consistency - 多视图颜色一致性</p>
<p>self-improving system - 自我改善系统</p>
<p>3D volumetric space - 三维体积空间</p>
<p>graph convolutional network - 图卷积网络</p>
<p>lixel - 线+像素</p>
<p>optical flow - 光流</p>
<p>generalize well - 很好的推广</p>
<p>skeleton-disentangling framework - 骨架解耦框架？？？</p>
<p>multi-level spatial and temporal subproblems - 多层次的时空子问题</p>
<p>variational motion estimator（VME）- 变分运动估计器</p>
<p>motion residual regressor（MRR）- 运动残余回归器</p>
<p>3D pose acceleration error - 三维姿态加速度误差</p>
<p>trade-off - 权衡</p>
<p>quantitative metrics - 定量指标</p>
<p>qualitative results - 定性结果</p>
<p>consecutive frame - 连续帧</p>
<p>hallucinator - 幻觉器？？？</p>
<p>neural autoregressive framework - 神经自回归框架</p>
<p>global average pooling - 全局平均池化</p>
<p>attention values - 注意值</p>
<p>average filter - 平均过滤器</p>
<p>spherical linear interpolation - 球面线性插值</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p> [1] Hongsuk Choi, Gyeongsik Moon, and Kyoung Mu Lee.</p>
<p>Pose2Mesh: Graph convolutional network for 3D human</p>
<p>pose and mesh recovery from a 2D human pose. <em>ECCV</em>,</p>
<p>2020.</p>
<p>[2] Gyeongsik Moon and Kyoung Mu Lee. I2L-MeshNet:</p>
<p>Image-to-Lixel prediction network for accurate 3D human</p>
<p>pose and mesh estimation from a single RGB image. <em>ECCV</em>,</p>
<p>2020.</p>
<p>[3] Angjoo Kanazawa, Jason Y Zhang, Panna Felsen, and Jiten</p>
<p>dra Malik. Learning 3D human dynamics from video. <em>CVPR</em>,</p>
<p>2019.</p>
<p>[4] Muhammed Kocabas, Nikos Athanasiou, and Michael J</p>
<p>Black. VIBE: Video inference for human body pose and</p>
<p>shape estimation. <em>CVPR</em>, 2020. </p>
<p>[5] Zhengyi Luo, S Alireza Golestaneh, and Kris M Kitani. 3D</p>
<p>human motion estimation via motion compression and re-</p>
<p>fifinement. <em>ACCV</em>, 2020. </p>
<p>[6] Angjoo Kanazawa, Michael J Black, David W Jacobs, and</p>
<p>Jitendra Malik. End-to-end recovery of human shape and</p>
<p>pose. <em>CVPR</em>, 2018</p>
<p>[7] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and</p>
<p>Kostas Daniilidis. Learning to reconstruct 3D human pose</p>
<p>and shape via model-fifitting in the loop. <em>ICCV</em>, 2019.</p>
<p>[8] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas</p>
<p>Daniilidis. Learning to estimate 3D human pose and shape</p>
<p>from a single color image. <em>CVPR</em>, 2018. </p>
<p>[9] Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Pe</p>
<p>ter Gehler, and Bernt Schiele. Neural Body Fitting: Unifying</p>
<p>deep learning and model based human pose and shape esti</p>
<p>mation. <em>3DV</em>, 2018. </p>
<p>[10] Georgios Pavlakos, Nikos Kolotouros, and Kostas Daniilidis.</p>
<p>TexturePose: Supervising human mesh estimation with tex</p>
<p>ture consistency. <em>ICCV</em>, 2019.</p>
<p>[11] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and</p>
<p>Kostas Daniilidis. Learning to reconstruct 3D human pose</p>
<p>and shape via model-fifitting in the loop. <em>ICCV</em>, 2019. </p>
<p>[12] Georgios Georgakis, Ren Li, Srikrishna Karanam, Terrence</p>
<p>Chen, Jana Kosecka, and Ziyan Wu. Hierarchical kinematic</p>
<p>human mesh recovery. <em>ECCV</em>, 2020.</p>
<p>[13] Gul Varol, Duygu Ceylan, Bryan Russell, Jimei Yang, Ersin</p>
<p>Yumer, Ivan Laptev, and Cordelia Schmid. BodyNet: Vol</p>
<p>umetric inference of 3D human body shapes. <em>ECCV</em>, 2018.</p>
<p>[14] Nikos Kolotouros, Georgios Pavlakos, and Kostas Dani</p>
<p>ilidis. Convolutional mesh regression for single-image hu</p>
<p>man shape reconstruction. <em>CVPR</em>, 2019. </p>
<p>[15] Angjoo Kanazawa, Jason Y Zhang, Panna Felsen, and Jiten</p>
<p>dra Malik. Learning 3D human dynamics from video. <em>CVPR</em>,</p>
<p>2019.</p>
<p>[16] Carl Doersch and Andrew Zisserman. Sim2real transfer</p>
<p>learning for 3D human pose estimation: motion to the res</p>
<p>cue. <em>NeurIPS</em>, 2019.</p>
<p>[17]  </p>
<p>Yu Sun, Yun Ye, Wu Liu, Wenpeng Gao, YiLi Fu, and Tao</p>
<p>Mei. Human mesh recovery from monocular images via a</p>
<p>skeleton-disentangled representation. <em>ICCV</em>, 2019.</p>
<p>[18] Ye Yuan and Kris Kitani. Ego-pose estimation and forecast</p>
<p>ing as real-time pd control. <em>ICCV</em>, 2019. 3</p>
<p>[19] Jason Y Zhang, Panna Felsen, Angjoo Kanazawa, and Jiten</p>
<p>dra Malik. Predicting 3d human dynamics from video. <em>ICCV</em>,</p>
<p>\2019. </p>
<p>[20] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and</p>
<p>Kostas Daniilidis. Learning to reconstruct 3D human pose</p>
<p>and shape via model-fifitting in the loop. <em>ICCV</em>, 2019.</p>
<p>[21] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao</p>
<p>Li. On the continuity of rotation representations in neural</p>
<p>networks. <em>CVPR</em>, 2019.</p>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>人体三维重建</category>
      </categories>
      <tags>
        <tag>人体三维重建</tag>
        <tag>SMPL</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读】MEVA</title>
    <url>/2021/10/19/MEVA/</url>
    <content><![CDATA[<h1 id="MEVA-3D-Human-Motion-Estimation-via-Motion-Compression-and-Refifinement"><a href="#MEVA-3D-Human-Motion-Estimation-via-Motion-Compression-and-Refifinement" class="headerlink" title="MEVA: 3D Human Motion Estimation via Motion Compression and Refifinement"></a>MEVA: 3D Human Motion Estimation via Motion Compression and Refifinement</h1><p>github：<a href="https://github.com/ZhengyiLuo/MEVA">https://github.com/ZhengyiLuo/MEVA</a></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们提出了一个方法对于RGB视频序列输入，产生光滑和准确的人体姿态和运动估计。使用<strong>基于自动编码器的运动压缩</strong>和<strong>通过运动细化学习到的残差表示</strong>，将人类运动的时间序列分解为平滑运动表示。这两步编码将人类运动的捕获分为两个阶段：1. <strong>全局人体运动估计</strong>：用来捕获粗糙的一般运动；2. <strong>残差估计</strong>：添加人的特定运动细节。实验展示了我们的方法在三维人体姿态和运动的平滑性和准确性上有较好的性能。</p>
<h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>从单个视频中估计三维人体姿态序列需要一个计算模型，该模型可以提取人体运动潜在的运动学，也可以保留被捕获人所特有的运动。人类具有相同的人体结构（相同的关节数量）和相似的物理约束（关节约束），所以能够学习一个<strong>广义的运动学模型</strong>与图像相匹配来推断一个人的一般运动。但是广义的运动模型不能给特定个人的运动建模，因此需要使用图像信息添加或细化广义运动的估计结果。我们提出了两阶段三维估计方法：<strong>首先，从视频中提取粗糙的运动序列</strong>；<strong>然后，细化这个序列来产生更准确的三维运动估计</strong>。我们将推理过程分解为（1）一般运动模型和（2）特定的运动模型，来获取更准确和平滑的估计。</p>
<p>近几年，人体三维姿态估计获得了很大的进展，在<strong>单张图片</strong>和<strong>视频</strong>的人体网格重建有很好的结果。主要的指标有： Mean Per Joint Position Error (MPJPE)：平均每个关节位置误差。但缺少运动的时序平滑性的体现，存在抖动的现象，导致不自然的运动估计，优化的方法是使用抖动的姿态估计。</p>
<p>时序平滑性的问题已经有部分被解决，包括，<strong>大尺度的运动数据集</strong>（比如AMAAS）和引入<strong>对抗性损失</strong>[1] [2]来监督学习，[3]预测帧顺序（？？？）。但是只在损失函数中使用先验知识难以在平滑性和准确性中找到平衡。</p>
<p>我们认为在平滑度和精确度之间达到平衡可以通过<strong>粗糙</strong>和<strong>精细</strong>运动的分解来实现。首先我们通过观察人类运动的大数据集来学习粗糙的运动模型，因为<strong>人类的运动通常是平滑</strong>的。如果要将一个模型拟合到大量的人类运动中，那么大多数数据将位于一个运动平滑的子空间中。这意味着，如果我们要压缩人类运动的数据，它应该学习一个潜在的子空间，其中的运动是固有特性是平滑和粗糙的。以这个<strong>潜在子空间为回归目标</strong>，我们可以直接从输入视频中推理出粗糙的人体运动。但是这个运动子空间中，罕见的运动，比如突然的运动，被从运动数据中移除。为了保留这样的运动，我们还认为，产生最后的三维运动估计可以被视为一个<strong>细化步骤</strong>，将细节添加到粗运动序列中。</p>
<p>为了验证我们的论点，我们提出两阶段人体运动估计方法，1. 首先使用<strong>变分自动编码器（VAE）</strong>从数据压缩中估计粗糙的人体姿态序列，称为<strong>变分运动估计器（VME）</strong>；2. 然后把VME的输出作为姿态回归器的输入，细化姿态估计，称为<strong>运动细化回归器（MRR）</strong>。</p>
<p>我们提出一个基于视频的人体三维运动估计方法，聚焦产生平滑和准确的人体运动序列。主要贡献有：</p>
<ul>
<li>提出一个两阶段运动估计方法，保证时序平滑性和姿态估计准确定</li>
<li>描述一个鲁棒的变分自动编码器的学习过程，作为潜在人体运动子空间，从视频中估计三维人体的粗运动</li>
<li>我们在真实场景的3DPW数据集中进行姿态和运动估计的性能评测，减少了54.3%的加速度误差，同时实现了最先进的WPJPE结果。</li>
</ul>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>相关工作将介绍单张图像的<strong>人体形状和姿态恢复</strong>和视频<strong>人体运动恢复</strong>，后者是前者的子集，因为人体运动是人体姿态的一个序列。然后介绍现有方法是如何使用人体运动作为先验，和如何将运动序列映射到低维空间。</p>
<h3 id="单张图像的三维人体姿态和形状恢复"><a href="#单张图像的三维人体姿态和形状恢复" class="headerlink" title="单张图像的三维人体姿态和形状恢复"></a>单张图像的三维人体姿态和形状恢复</h3><p>这里聚焦基于模型的方法，能够同时恢复形状和姿态，我们选择使用<strong>参数化三维人体模型</strong>（SMPL、SCAPE、SMPLify-X），因为它能够容易地转换成三维人体网格。近年来，参数化人体三维重建获得了大量关注，从剪影或者人工输入，到直接拟合模型参数到二维关节位置，到直接从图像估计形状和位姿。因为缺少三维标签的这是你hi之，这些方法使用弱监督的方式来拟合三维人体到二维关节位置、身体部位分割或者稠密像素关联[4]。这些方法由于缺乏<strong>时序信息</strong>，提取的运动往往不稳定。</p>
<h3 id="基于视频的三维人体姿态和形状恢复"><a href="#基于视频的三维人体姿态和形状恢复" class="headerlink" title="基于视频的三维人体姿态和形状恢复"></a>基于视频的三维人体姿态和形状恢复</h3><p>视频人体三维重建是图像的自然拓展，[5]等（1）预测从二维到三维的关节位置，使用<strong>LSTM</strong>、<strong>时序信息</strong>和<strong>全连接层</strong>来探索时序信息。（2）HMMR、VIBE直接从图像中预测三维关节位置，并使用<strong>时序滤波器</strong>对运动序列进行后处理。<strong>HMMR</strong>通过让模型预测未来和过去帧的运动，来增强时序一致性；[4] <strong>Denserac</strong>预测被打乱的帧的顺序；<strong>VIBE</strong>使用GRU将特征的输入帧转换为一系列时间相关的潜在特征。</p>
<h3 id="人体姿态和动作先验"><a href="#人体姿态和动作先验" class="headerlink" title="人体姿态和动作先验"></a>人体姿态和动作先验</h3><p>使用预先记录的人体运动序列作为先验，已经用在许多人体运动相关的任务上。（1）学习<strong>运动捕获MoCap</strong>来帮助<strong>三维运动跟踪</strong>；（2）使用<strong>对抗性鉴别器</strong>对每帧结果进行处理，保证恢复有效的人体姿态；（3）<strong>使用预训练的VAE</strong>潜在空间；（4）时序上的鉴别器，鉴别整个运动序列。以上的方法都是用对抗的方式使用姿态和运动先验，用在损失函数中。</p>
<h3 id="人体运动表达"><a href="#人体运动表达" class="headerlink" title="人体运动表达"></a>人体运动表达</h3><p>将人体运动压缩到一个紧凑的潜在表达，能够用来生成人体运动和轨迹预测。目前的生成模型：1. <strong>VAE</strong> 2. <strong>生成流（generative flow）</strong> 3. <strong>GAN</strong></p>
<p>【生成（generation）和表达（representation）的关系是什么？？？】</p>
<h2 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h2><p>现有方法的问题：难以实现时序平滑性和精确性的平衡</p>
<p><img src="1.png" alt="1"></p>
<p>本文解决方法L提出MEVA，通过变分自动编码器进行运动估计，首先估计整体粗运动，然后添加细节运动作为残差。MEVA分三步处理输入：</p>
<ul>
<li>首先，使用<strong>时空特征提取器（STE）</strong>提取相关的<strong>时序特征</strong></li>
<li>然后，使用<strong>变分运动估计器（VME）</strong>捕获<strong>全局粗运动</strong></li>
<li>最后，使用<strong>运动残差回归器（MRR）</strong>增加精细运动的细节</li>
</ul>
<h3 id="3-1-公式"><a href="#3-1-公式" class="headerlink" title="3.1 公式"></a>3.1 公式</h3><p>$V_T=\{I_t\}^T_{t=1}$：视频输入</p>
<p>$M_T=\{\theta_t\}^T_{t=1}$：运动序列，需要重建的</p>
<p>直观上，从视频中重建人体运动不需要恢复人体形状，可以直接从视频映射到人体运动。但是和真实运动注释匹配的视频需要专业的捕捉设备，与二维姿态数据相比仍然比较罕见。在缺乏三维数据样本的情况下，我们的模型以半监督的方式学习运动序列（利用经过三维或者二维标记过关节位置的视频）。因此我们的运动估计目标就是学习函数：$V_T-&gt;M_T$</p>
<h3 id="3-2-时空特征提取器（STE）"><a href="#3-2-时空特征提取器（STE）" class="headerlink" title="3.2 时空特征提取器（STE）"></a>3.2 时空特征提取器（STE）</h3><p>过去的运动可以提供关于未来工作的线索。因此，除了单独使用前馈卷积网络来提取每一帧的视觉特征，我们还能够产生时序相关特征来更好地进行运动序列建模。</p>
<h3 id="3-3-变分运动估计器（VME）"><a href="#3-3-变分运动估计器（VME）" class="headerlink" title="3.3 变分运动估计器（VME）"></a>3.3 变分运动估计器（VME）</h3><h4 id="人体运动变分自动编码器"><a href="#人体运动变分自动编码器" class="headerlink" title="人体运动变分自动编码器"></a>人体运动变分自动编码器</h4><p>为了学习一个能够封装广泛<strong>人体运动的子空间</strong>，我们使用<strong>变分自动编码器（VAE）</strong>。VAE显示地将<strong>每个数据点映射到潜在的代码</strong>，能够高效地捕获大量可能的数据模式，对学习到的潜在空间施加一个<strong>高斯先验</strong>，相似运动的潜在代码将会相互接近。因此，VAE的潜在空间允许代码间更多的重合，能够加强潜在空间的平滑性。【具有一个平滑的潜在代码对于提高模型的通用性至关重要，因为可能的人体运动空间是高度相关和有限的？？？】。根据之前VAE的工作，目标是最大化对数似然的evidence lower bound（ELBO）</p>
<p><img src="2.png" alt="2"></p>
<p>编码器$E_{vae}$接收一系列SMPL姿态参数的人体运动的帧W，输出潜在代码$z$。解码器$D_{vae}$输入潜在代码$z$并重建运动$\hat M_W$，编码器$E_{vae}$和解码器$D_{vae}$使用GRUs。</p>
<p><img src="3.png" alt="3"></p>
<p>目标函数可以写成：</p>
<p><img src="4.png" alt="4"></p>
<h4 id="人体运动数据增强"><a href="#人体运动数据增强" class="headerlink" title="人体运动数据增强"></a>人体运动数据增强</h4><p>我们学习到的VAE应该能够推广到<strong>看不到的人体运动序列</strong>，并实现较高精度的重建，以确保学习到的潜在空间确实可以作为一个全面的人类运动子空间。我们用大规模人类运动数据集<strong>AMASS</strong>（13k不同长度的运动样本），但我们训练的VAE在看不见的序列上<strong>通用性</strong>很差。于是我们进行<strong>数据增强</strong>，来增强现有的运动并产生可行<strong>独特的人体运动</strong>序列。<strong>虽然数据增强在图像处理中得到了广泛的研究，但是在人体运动数据集方面的尝试却很少</strong>。在轨迹预测和人体运动生成的应用中，VAE潜在空间的通用性没有被广泛讨论，因为模型只需要生成新的运动序列，而不强调其编码看不见的运动序列的能力。</p>
<p>我们采用以下的数据增强方案：</p>
<ul>
<li><strong>加速和减速</strong>：对帧进行均匀上采样或者下采样（加速和减速），仍能产生合理且自然的人体运动</li>
<li><strong>左右翻转</strong>：对于相同的动作，使用左手或者右手，将仍是一个有效的人体运动，所以我们遵循SMPL模型的运动学树，<strong>镜像左右运动</strong>，生成一个新的运动序列。</li>
<li><strong>随机根旋转</strong>：从单位球中随机选取一个根旋转，来捕获可能的人体运动中不同的根方向。不同的姿态估计器可能假设不同的基平面和坐标系，采样随机根旋转有助于模型处理不同坐标系的选择。</li>
</ul>
<h4 id="从视频中学习平滑运动"><a href="#从视频中学习平滑运动" class="headerlink" title="从视频中学习平滑运动"></a>从视频中学习平滑运动</h4><p>使用VAE学习了一个全面人体运动子空间之后，我们学习了一个额外的编码器$E_{motion}$，它可以直接从视频特征中提取出粗运动序列， 映射到与$E_{vae}$相同的潜在空间。</p>
<p>输入一个视频特征序列$f_W=\{f_w\}^W_{w=1}$，编码器$E_{motion}$的作用是将输入特征压缩成一个潜在的代码$z$，将当前的观测结果表达为一个粗糙的人体运动序列。在VAE中预训练的解码器$D_{vae}$迫使编码器$E_{motion}$从预训练的运动子空间中采样，将编码器的潜在空间限制在预训练的人体运动子空间中提供一个很强的人体运动先验，有助于编码器$E_{motion}$的学习过程。结合编码器$E_{motion}$和解码器$D_{vae}$，构成了变分运动估计器（VME）</p>
<h3 id="3-4-运动残差回归器（MRR）"><a href="#3-4-运动残差回归器（MRR）" class="headerlink" title="3.4 运动残差回归器（MRR）"></a>3.4 运动残差回归器（MRR）</h3><p>使用VAE的潜在空间作为目标学习的运动序列具备固有的平滑性和粗糙性，通过信息压缩捕获当前视频帧的整体运动特征。为了捕获细节，采用SMPL回归器来<strong>迭代细化</strong>估计的姿态。MEVA中回归变量用<strong>VME计算的姿态来初始化</strong>，而之前的方法都是用平均SMPL姿态来初始化。因此，回归变量的任务只是对粗糙估计做微小的修改，来增加在压缩步骤中丢失的运动细节。</p>
<p>回归器的输入特征跟VIBE一样，使用时序视觉编码器，所以及时每帧的估计是分别计算的，但视觉特征已经是时序相关的。</p>
<p>VME使用一般的运动模型从视频中计算<strong>全局粗运动</strong>，回归器（MRR）联合细化运动和人体形状估计，相当于在每帧中<strong>添加特定的运动细节</strong>。</p>
<h3 id="3-5-训练和损失函数"><a href="#3-5-训练和损失函数" class="headerlink" title="3.5 训练和损失函数"></a>3.5 训练和损失函数</h3><p><img src="5.png" alt="5"></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>介绍了三部分：1.数据集  2. 实验  3. 消融实验</p>
<h4 id="4-1-数据集"><a href="#4-1-数据集" class="headerlink" title="4.1 数据集"></a>4.1 数据集</h4><p>VAE：</p>
<ul>
<li><p>训练AMASS</p>
</li>
<li><p>测试3DPW</p>
</li>
</ul>
<p>视频训练：</p>
<ul>
<li><p>MPI-INF-3DPH, 3DPW and human 3.6M：有三维关节标注</p>
</li>
<li><p>InstaVariety、PennAction：有二维关节标注</p>
</li>
</ul>
<h3 id="4-2-评估结果和分析"><a href="#4-2-评估结果和分析" class="headerlink" title="4.2 评估结果和分析"></a>4.2 评估结果和分析</h3><h4 id="运动变分编码器的生成"><a href="#运动变分编码器的生成" class="headerlink" title="运动变分编码器的生成"></a>运动变分编码器的生成</h4><p>记录了VAE对3DPW数据集中看不见的运动序列的重建误差运动VAE模型是使用所有三种形式的数据增强技术训练的性能最好的模型。结果表明，我们的VAE模型能够推广到看不见的序列，学习的子空间可以合理地展示人体运动空间。</p>
<p><img src="6.png" alt="6"></p>
<h4 id="定量结果"><a href="#定量结果" class="headerlink" title="定量结果"></a>定量结果</h4><p>表2展示了我们的方法在位置误差（MPJP和PA-MPJPE）取得了可比性的结果，同时显著提高了平滑度（加速度误差ACC-ERR），意味着在不牺牲精度的情况下进行更平滑、更自然的运动估计。为了进行更直接的比较，我们使用与我们方法完全相同的数据集重新训练了之前最先进的方法VIBE。之前平滑性最先进的模型是HMMR。</p>
<p><img src="7.png" alt="7"></p>
<h4 id="定性结果"><a href="#定性结果" class="headerlink" title="定性结果"></a>定性结果</h4><p><a href="https://youtu.be/YBb9NDz3ngM">https://youtu.be/YBb9NDz3ngM</a></p>
<h3 id="4-3-消融实验"><a href="#4-3-消融实验" class="headerlink" title="4.3 消融实验"></a>4.3 消融实验</h3><h4 id="数据增强对运动VAE训练的效果"><a href="#数据增强对运动VAE训练的效果" class="headerlink" title="数据增强对运动VAE训练的效果"></a>数据增强对运动VAE训练的效果</h4><p>表格3展示了VAE在3DPW数据集中，使用不同数据增强方法后，对看不见序列的重建误差</p>
<p><img src="8.png" alt="8"></p>
<h4 id="粗运动与精细运动检索"><a href="#粗运动与精细运动检索" class="headerlink" title="粗运动与精细运动检索"></a>粗运动与精细运动检索</h4><p>MEVA从粗细运动的显式分解中受益，使用时序压缩来捕获给定序列的全局运动。表4展示了粗细运动信息的检索量。</p>
<p><img src="9.png" alt="9"></p>
<p><img src="10.png" alt="10"></p>
<h4 id="预训练运动VAE的效果"><a href="#预训练运动VAE的效果" class="headerlink" title="预训练运动VAE的效果"></a>预训练运动VAE的效果</h4><p>MEVA从预训练的运动VAE的潜在空间中受益。预训练的VAE提供了一个人体运动子空间，帮助约束运动序列更自然可信。</p>
<p><img src="11.png" alt="11"></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>为了实现时序平滑和精确的三维人体姿态估计，（1）学习一个压缩模型，编码一般人体运动的光滑性，（2）同时学习一个基于图像的回归模型，来捕获特定的运动。我们提出两阶段模型，首先，训练一个变分自动编码器给粗糙/光滑的人体运动进行建模；然后学习人体特殊运动细化回归模块，来保留一般运动模型没有捕捉到的运动。通过全面的实验，验证了我们方法的平滑性和准确性。</p>
<h2 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h2><p> <strong>Mean Per Joint Position Error (MPJPE)</strong>：平均每个关节位置误差</p>
<ul>
<li>根据视频每一帧计算的相对关节位置</li>
<li><p>但缺少运动的时序平滑性的体现</p>
<p><strong>MPJPE after Procrustes Alignment (PA-MPJPE)</strong>：执行程序对齐后的MPJPE</p>
</li>
<li><p>对齐三维关节的根位置（盆骨）后计算MPJPE</p>
<p>MPJPE和PA-MPJPE都是运动估计器的准确性指标（$mm$）</p>
</li>
</ul>
<p><strong>Acceleration error (ACC-ERR)</strong>：加速度误差</p>
<ul>
<li><p>测量预测结果和真实值中每个关键点的三维加速度误差（单位$mm/s^2$）</p>
</li>
<li><p>ACC-ERR是估计运动序列的主要的平滑度指标</p>
</li>
<li>加速度使用有限差分方法计算得到</li>
</ul>
<p>以上指标需要共同使用：低的<strong>位置误差</strong>表明运动的捕获<strong>整体的正确性</strong>，而好的<strong>加速度误差</strong>标志着<strong>平滑和自然</strong>的人体运动估计。</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><ul>
<li>Archive of Motion Capture as Surface Shapes（AMAAS）：Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: Amass: Archive of motion capture as surface shapes. 2019 IEEE/CVF International Conference on Computer Vision (ICCV) (2019) 5441–5450</li>
<li>MPI-INF-3DPH</li>
<li>3DPW</li>
<li>human 3.6M</li>
<li>InstaVariety</li>
<li>PennAction</li>
</ul>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>smooth motion representation - 平滑运动表示</p>
<p>auto-encoder-based motion compression - 基于自动编码器的运动压缩</p>
<p>residual representation - 残差表示</p>
<p>motion refinement - 运动细化</p>
<p>generalized kinematic model - 广义运动学模型</p>
<p>human mesh recovery - 人体三维重建</p>
<p>adversarial loss - 对抗性损失</p>
<p>data compression - 数据压缩</p>
<p>【Variational Autoencoder（VAE）- 变分自动编码器】</p>
<p>Variational Motion Estimator（VME）- 变分运动估计器</p>
<p>Motion Residual Regressor（MRR）- 运动细化回归器</p>
<p>body part segmentation - 身体部分分割</p>
<p>dense pixel correspondence - 图像稠密像素关联</p>
<p>LSTM</p>
<p>temporal convolution - 时序卷积</p>
<p>fully connected layers - 全连接层</p>
<p>motion capture（MoCap）- 运动捕获</p>
<p>3D motion tracking - 三维运动跟踪</p>
<p>adversarial discriminator - 对抗性鉴别器</p>
<p>human motion representation - 人体运动表达</p>
<p>compact latent representation  - 紧凑的潜在表达</p>
<p>spatio-temporal feature extractor（STE）- 时空特征提取器</p>
<p>linear coefficient - 线性系数</p>
<p>weak perspective camera - 弱视角相机</p>
<p>feed-forward convolutional network - 前馈卷积网络</p>
<p>human motion subspace - 人体运动子空间</p>
<p>Gaussian prior - 高斯先验</p>
<p>Gaussian parameterization of the VAE - VAE的高斯参数化</p>
<p>data augmentation - 数据增强</p>
<p>kinematic tree - 运动学树</p>
<p>finite difference - 有限差分</p>
<p>explicit breakdown - 显式分解</p>
<p>kinematically invalid - 运动学无效</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Kocabas, M., Athanasiou, N., Black, M.J.: Vibe: Video inference for human body pose and shape estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. (2020) 5253–5263</p>
<p>[2] Kanazawa, A., Zhang, J.Y., Felsen, P., Malik, J.: Learning 3d human dynamics from video. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 5607–5616</p>
<p>[3] Xu, Y., Zhu, S.C., Tung, T.: Denserac: Joint 3d pose and shape estimation by dense render-and-compare. 2019 IEEE/CVF International Conference on Computer Vision (ICCV) (2019) 7759–7769</p>
<p>[4] Xu, Y., Zhu, S.C., Tung, T.: Denserac: Joint 3d pose and shape estimation by dense render-and-compare. 2019 IEEE/CVF International Conference on Computer Vision (ICCV) (2019) 7759–7769</p>
<p>[5] Xu, J., Yu, Z., Ni, B., Yang, J., Yang, X., Zhang, W.: Deep kinematics analysis for monocular 3d human pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). (2020)</p>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>人体三维重建</category>
      </categories>
      <tags>
        <tag>人体三维重建</tag>
        <tag>GRU</tag>
        <tag>SMPL</tag>
        <tag>VAE</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读】VIBE</title>
    <url>/2021/10/15/VIBE/</url>
    <content><![CDATA[<h1 id="VIBE-Video-Inference-for-Human-Body-Pose-and-Shape-Estimation"><a href="#VIBE-Video-Inference-for-Human-Body-Pose-and-Shape-Estimation" class="headerlink" title="VIBE: Video Inference for Human Body Pose and Shape Estimation"></a>VIBE: Video Inference for Human Body Pose and Shape Estimation</h1><p>github：<a href="https://github.com/mkocabas/VIBE">https://github.com/mkocabas/VIBE</a></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>因为缺少ground truth的三维运动数据来训练，所以目前SOTA的工作不能很好地产生自然正确的运动。我们利用大型运动捕获数据集AMASS，提出了基于视频的人体姿态和形状估计网络VIBE。创新点在于，利用AMASS来区分真实人类运动和我们时序位姿和形状回归网络产生的运动。我们提出一个创新的时序网络框架，使用了自我注意机制和对抗训练，在序列的水平上产生运动学合理的运动序列。</p>
<h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>尽管目前有很多工作从单张图片中估计三维人体位姿和形状，但是人体动作才体现人的行为。我们关注于探索单目视频的时序信息来更好地评估人体运动。由于缺乏足够的训练数据，之前的时序网络没有捕获到人体复杂多变的运动。我们提出了新的时序神经网络和训练方法，来提高单目视频的三维人体位姿估计性能。</p>
<p>[1]和[2]结合室内视频和二维gd或伪gd关键点标注的数据集，但室内三维数据集物体的数量、运动范围和图片复杂度都受到限制，标注的数量仍然不够充足，伪gd标签在建模人体运动上不够可靠。</p>
<p>我们的灵感来自于[3]，[3]利用单张图片来估计位姿，只将二维关键点和未配对的静态三维人体形状和位姿用于对抗训练方法。在视频序列上，早已存在二维关键点标注的视频，接下来的问题是如何获得足够质量的真实的三维人体运动来进行对抗性训练。因此，我们使用大规模三维动作捕获数据集AMASS，该数据集足够丰富来学习人体的运动。</p>
<p>我们的方法从真实场景的视频学习视频序列的三维人体重建，不仅能够区别估计的运动和数据集的运动的区别，也使用三维关键点。我们方法的输出是一个SMPL的pose和shape参数序列。具体地就是采用两个未配对的信息源（真实视频和AMASS数据集）来训练一个序列的生成对抗网络（GAN）[4]。我们训练一个时间模型来预测每一帧的SMPL身体模型的参数，而一个运动鉴别器来区分真实序列和回归序列。因此，通过最小化对抗训练损失回归器能够输出接近真实运动的位姿，而运动鉴别器作为一个弱监督。</p>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p>输入：真实视频图像</p>
<p>预测：预训练的单图像认为i位姿和形状卷积神经网络，由[3]的时序编码器和人体参数回归器组成，在GRUs上执行，来获取人体运动的序列特性。</p>
<p>鉴别：运动鉴别器将预测的位姿和数据集采样的位姿进行对比，对每个序列输出real/fake标签，并采用了注意力机制来放大各自框架的贡献。</p>
<p>输出：SMPL人体pose和shape参数</p>
<p>损失函数：整个模型使用监督的方法，通过对抗损失和回归损失来最小化预测结果和gd在关键点、pose和shape参数上的误差。</p>
<h3 id="测试过程"><a href="#测试过程" class="headerlink" title="测试过程"></a>测试过程</h3><p>给定一个视频，使用预训练的CNN和时序模型来预测每一帧SMPL的pose和shape的参数。</p>
<h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><ul>
<li>我们使用AMASS数据集来对抗性地训练VIBE，促进回归器产生真实准确的运动。</li>
<li>在运动鉴别器中使用注意力机制，给不同的帧赋权重，来区分每帧的重要性。</li>
</ul>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="单一图像的三维位姿和形状"><a href="#单一图像的三维位姿和形状" class="headerlink" title="单一图像的三维位姿和形状"></a>单一图像的三维位姿和形状</h3><p>参数人体模型，比如SMPL、SCAPE经常用在人体姿态估计，因为它们捕获到了人体的统计数据，能够提供三维网格模型。</p>
<p>非参数人体模型，比如Bodynet用涂卷机网络直接回归顶点位置、PIFu隐函数像素对齐，但是在视频下会有不稳定的情况。</p>
<h3 id="视频的三维位姿和形状"><a href="#视频的三维位姿和形状" class="headerlink" title="视频的三维位姿和形状"></a>视频的三维位姿和形状</h3><p>基于视频的人体运动捕获有很长的历史，但受限于简单的动作。最近的深度学习方法只关注于关节位置。有些使用二阶段的方法将现有的二维关键点对应到三维的关节位置。[5]和[6]VNect使用端到端直接回归三维关节点位置，在室内数据集中表现出色，但是在真实数据集中表现不好。许多方法通过扩展SMPLify从视频中重建SMPL的shape和pose参数来计算人体的一致性和平滑运动。[8] 展示了标记SMPLify的网络视频来提高微调时的HMR。[9]通过预测前后帧来学习人体运动学，他们展示了使用二维关键点检测器标记的网络视频可以缓解真实三维位姿标签的需要。[10]提出使用基于transformer的时序模型来提高性能。他们提出了一个无监督对抗训练策略，来为乱序的帧排序。</p>
<h3 id="序列模型的生成对抗网络"><a href="#序列模型的生成对抗网络" class="headerlink" title="序列模型的生成对抗网络"></a>序列模型的生成对抗网络</h3><p>GAN对图像建模和合成有重要的意义。最近许多工作将GAN融入到序列-序列的任务，比如机器翻译。在运动建模中，结合顺序架构和对对抗训练，能够基于之前的的运动来预测未来的运动序列，或者产生人类运动序列。而我们专注于在序列输入数据下对抗性地细化预测的姿态。我们采用一个运动鉴别器，编码位姿和形状参数，来学习真实数据的优点。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><img src="1.png" alt="1"></p>
<p><strong>输入</strong>：单人视频</p>
<p><strong>预训练网络</strong>：预训练好的CNN，用来提取每帧图像的特征</p>
<p><strong>训练的网络</strong>：双向GRU组成的时序编码器，输出包含过去和未来帧信息的潜在变量。</p>
<p><strong>作用</strong>：这些特征被用来回归SMPL的参数。</p>
<h4 id="SMPL参数"><a href="#SMPL参数" class="headerlink" title="SMPL参数"></a>SMPL参数</h4><ul>
<li>pose：人体的全局旋转和23个关节的轴角，控制动作位姿</li>
<li>shape：包括主成分分析PCA空间的的前十个系数，这里使用的是SMPL中的中性模型，控制高矮胖瘦</li>
</ul>
<h4 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h4><ul>
<li>生成pose参数</li>
<li>生成shape参数</li>
<li>平均池化所有的输入序列，得到一个人体</li>
<li>时序产生器生成和AMASS采样的模型进入到运动鉴别器，来去人真假样本。</li>
</ul>
<h3 id="时序编码器"><a href="#时序编码器" class="headerlink" title="时序编码器"></a>时序编码器</h3><p>intuition：使用循环结构的原因是视频的前位姿信息对未来帧可能有帮助，尤其是人体模糊或者遮挡的情况。之前帧能够解决和约束姿态的估计。</p>
<p>时序编码器作为一个产生器，对于给定的<strong>输入序列</strong>，<strong>输出</strong>每帧对应的位姿和形状参数$\hat{\Theta} = [(\hat{\theta_1},…,\hat{\theta_T}), \hat{\beta}]$。</p>
<p>序列T进入一个预训练的作为特征提取器的卷积网络，每帧输出一组2048维的向量 <em>f</em> 。这些向量被输入GRU层，每帧再输出潜在的特征向量 <em>g</em> 。然后将 <em>g</em> 输入到回归器进行迭代反馈。回归器初始化为平均位姿并将当前的位姿和特征 <em>g</em> 输入来迭代更新参数。</p>
<p>时序编码器的损失函数由二维 <em>x</em> 、三维 X 、位姿 <em>θ</em> 和形状 <em>β</em> 误差组成：</p>
<p><img src="2.png" alt="2"></p>
<p>每个误差的计算公式为：</p>
<p><img src="3.png" alt="3"></p>
<p>其中二维关键点x_hat是通过三维关节点X_hat投影来的。</p>
<h3 id="动作鉴别器"><a href="#动作鉴别器" class="headerlink" title="动作鉴别器"></a>动作鉴别器</h3><p>动作鉴别器的设计受到了End-to-end recovery of human shape ans pose的启发，他们设计了人体鉴别器和重投影误差，促进产生器来产生和二维关节点对齐的三维位姿。但是单图像不能约束序列的位姿，当忽略运动的时间连续性时，多个不准确的姿态可能被认为是有效的。于是我们提出了动作鉴别器$D_M$，来鉴别产生的序列是否和真实序列一致。</p>
<p>输入：产生器输出的参数$\hat{\Theta} = [(\hat{\theta_1},…,\hat{\theta_T}), \hat{\beta}]$</p>
<p>网络：多层GRU模型$f_M$，用来预测隐藏的编码$h_i = f_m(\hat{\Theta_i})$，如图3，为了汇总隐藏的状态$[h_i,…,h_T]$，引入了注意力机制。</p>
<p>输出：一个线性层给出[0, 1]概率，表示$\hat{\Theta}$作为人类运动的合理性。</p>
<p><img src="4.png" alt="4"></p>
<p>传播给产生器的对抗损失为：</p>
<p><img src="5.png" alt="5"></p>
<p>其中，$p_R$是AMASS数据集中真实的运动序列，$p_G$是产生的运动序列。</p>
<p>【E是期望】</p>
<h4 id="MPoser"><a href="#MPoser" class="headerlink" title="MPoser"></a>MPoser</h4><p>把GAN换成VAE，测试效果，消融实验使用</p>
<h4 id="自我注意机制"><a href="#自我注意机制" class="headerlink" title="自我注意机制"></a>自我注意机制</h4><p>给重要的帧赋予更高的权重</p>
<h3 id="训练过程-1"><a href="#训练过程-1" class="headerlink" title="训练过程"></a>训练过程</h3><ul>
<li>图像编码器：一个ResNet-50网络，输入单帧图像，输出2048维特征$f_i$</li>
<li>时序编码器：2层GRU</li>
<li>SMPL回归器：2层全连接层，最后一层输出SMPL参数向量$\hat\Theta$，有75+10=85个参数，包括位姿、形状和相机参数</li>
<li>自我注意机制：2层MPL，学习权重，输出每个样本的真假概率</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="6.png" alt="6"></p>
<p><img src="7.png" alt="7"></p>
<p><img src="8.png" alt="8"></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><h3 id="未来的工作"><a href="#未来的工作" class="headerlink" title="未来的工作"></a>未来的工作</h3><ul>
<li>使用视频监督单帧方法，来细化HMR特征</li>
<li>检验密集运动的线索，比如光流，是否有帮助</li>
<li>使用运动来消除多人歧义</li>
<li>在遮挡情况下，利用运动来跟踪</li>
<li>尝试其他的编码方式，比如transformer，来更好地估计人体运动。</li>
</ul>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p><strong>AMASS</strong>：Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J. Black. AMASS: Archive of motion capture as surface shapes. In <em>International Conference on Computer Vision</em>, 2019.</p>
<p><strong>Human3.6M</strong>: Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments. In <em>IEEE Transaction on Pattern Analysis and Machine Intelligence</em>, 2014. </p>
<p><strong>3DPW</strong>: Timo von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3D human pose in the wild using IMUs and a moving camera. In <em>European Conference on Computer Vision</em>, 2018.</p>
<p><strong>MPI-INF-3DHP</strong>: Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian Theobalt. Monocular 3D human pose estimation in the wild using improved CNN supervision. In <em>International Conference on 3DVision</em>, 2017.</p>
<p><strong>PennAction</strong>：Weiyu Zhang, Menglong Zhu, and Konstantinos G Derpanis. From actemes to action: A strongly-supervised representation for detailed action understanding. In <em>International Conference on Computer Vision</em>, 2013.</p>
<p><strong>PoseTrack</strong>：Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton Milan, Juergen Gall, and Bernt Schiele. Posetrack: A benchmark for human pose estimation and tracking. In <em>IEEE Conference on Computer Vision and Pattern Recognition</em>, June 2018.</p>
<p><strong>InstaVariety</strong>：Angjoo Kanazawa, Jason Y. Zhang, Panna Felsen, and Jitendra Malik. Learning 3D human dynamics from video. In<em>IEEE Conference on Computer Vision and Pattern Recognition</em>, 2019.</p>
<p><strong>Kinetics-400</strong>：Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In <em>proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 2017.</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>adversarial learning - 对抗学习</p>
<p>self-attention mechanism - 自我注意机制</p>
<p>kinematically plausible motion - 运动学合理运动</p>
<p>non-trivial - 非平凡的</p>
<p>unpaired dataset - 未匹配数据集</p>
<p>generative adversarial network（GAN）- 生成对抗网络</p>
<p>Gated Recurrent Units（GRUs）- ????</p>
<p> off-the-shelf - 现有的</p>
<p>HMR - human mesh recovery缩写</p>
<p>synthesis - 合成</p>
<p>corpus - 语料库</p>
<p>recurrent architecture - 循环结构</p>
<p>ambiguous - 模糊</p>
<p>occluded - 遮挡</p>
<p>iterative feedback - 迭代反馈</p>
<p>weak-perspective - 弱视角</p>
<p> orthographic projection - 正投影</p>
<p>convex combination - 凸组合</p>
<p>VAE（variational autoencoder） - 基于变分思想的生成模型</p>
<p>Adam optimizer - Adam优化器</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Angjoo Kanazawa, Jason Y. Zhang, Panna Felsen, and Jitendra Malik. Learning 3D human dynamics from video. In <em>IEEE Conference on Computer Vision and Pattern Recognition</em>, 2019.</p>
<p>[2] Yu Sun, Yun Ye, Wu Liu, Wenpeng Gao, Yili Fu, and Tao Mei. Human mesh recovery from monocular images via a skeleton-disentangled representation. In <em>International Conference on Computer Vision</em>, 2019.</p>
<p>[3] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In <em>IEEE Conference on Computer Vision and Pattern Recognition</em>, 2018.</p>
<p>[4] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In <em>Advances in Neural Information Processing</em>, 2014.</p>
<p>[5] Dushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller, Weipeng Xu, Srinath Sridhar, Gerard Pons-Moll, and Christian Theobalt. Single-shot multi-person 3D pose estimation from monocular RGB. In <em>International Conference on 3DVision</em>, 2018.</p>
<p>[6] Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko, Helge Rhodin, Mohammad Shafifiei, Hans-Peter Seidel, Weipeng Xu, Dan Casas, and Christian Theobalt. VNect: Real-time 3D human pose estimation with a single RGB camera. In <em>SIGGRAPH</em>, July 2017.</p>
<p>[7] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J. Black. Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image. In <em>European Conference on Computer Vision</em>, 2016. </p>
<p>[8] Anurag Arnab, Carl Doersch, and Andrew Zisserman. Exploiting temporal context for 3D human pose estimation in the wild. In <em>IEEE Conference on Computer Vision and Pattern Recognition</em>, 2019. </p>
<p>[9] Angjoo Kanazawa, Jason Y. Zhang, Panna Felsen, and Jitendra Malik. Learning 3D human dynamics from video. In<em>IEEE Conference on Computer Vision and Pattern Recognition</em>, 2019.</p>
<p>[10] Yu Sun, Yun Ye, Wu Liu, Wenpeng Gao, Yili Fu, and Tao Mei. Human mesh recovery from monocular images via a skeleton-disentangled representation. In <em>International Conference on Computer Vision</em>, 2019.</p>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>人体三维重建</category>
      </categories>
      <tags>
        <tag>人体三维重建</tag>
        <tag>GAN</tag>
        <tag>GRU</tag>
        <tag>SMPL</tag>
      </tags>
  </entry>
  <entry>
    <title>【记录】学生成绩管理系统</title>
    <url>/2021/10/09/%E3%80%90%E8%AE%B0%E5%BD%95%E3%80%91%E5%AD%A6%E7%94%9F%E6%88%90%E7%BB%A9%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<h1 id="学生成绩管理系统"><a href="#学生成绩管理系统" class="headerlink" title="学生成绩管理系统"></a>学生成绩管理系统</h1><h2 id="项目和环境"><a href="#项目和环境" class="headerlink" title="项目和环境"></a>项目和环境</h2><p>项目地址：<a href="https://github.com/bojiangzhou/lyyzoo-ssms">https://github.com/bojiangzhou/lyyzoo-ssms</a></p>
<p>环境配置：</p>
<ul>
<li>操作系统：Windows10</li>
<li>开发平台：Eclipse oxygen </li>
<li>Java版本：JDK 1.8</li>
<li>服务器：tomcat 7.0</li>
<li>数据库：MySQL 5.1</li>
</ul>
<p>安装包已上传到网盘<a href="https://pan.baidu.com/s/1rjorX4O1zOSxhK89rNTQsQ，提取码：muws">https://pan.baidu.com/s/1rjorX4O1zOSxhK89rNTQsQ，提取码：muws</a></p>
<h2 id="Java安装及配置"><a href="#Java安装及配置" class="headerlink" title="Java安装及配置"></a>Java安装及配置</h2><h3 id="安装Java"><a href="#安装Java" class="headerlink" title="安装Java"></a>安装Java</h3><p>从网盘中下载安装包 jdk-8u202-windows-x64.exe，双击开始安装</p>
<p><img src="java1.jpg" alt="java1"></p>
<p>可以更改选择安装路径，注意不要有中文字符的路径，记住安装的路径，后面配置环境的时候会用到</p>
<p>选择好确定后，下一步</p>
<p><img src="java2.jpg" alt="java2"></p>
<p>这一步是选择 JRE 的安装目录，也可以更改，也要路径注意不能有中文字符</p>
<p><img src="java3.jpg" alt="java3"></p>
<p>安装成功！</p>
<p><img src="java4.jpg" alt="java4"></p>
<h3 id="Java的环境配置"><a href="#Java的环境配置" class="headerlink" title="Java的环境配置"></a>Java的环境配置</h3><p>配置 Java 环境变量</p>
<p>此电脑—&gt;计算机—&gt;系统属性—&gt;高级系统设置—&gt;环境变量—&gt;系统变量—&gt;新建</p>
<p>变量名：JAVA_HOME</p>
<p>变量值：C:\Program Files\Java\jdk1.8.0_202</p>
<p><img src="java5.png" alt="java5" style="zoom:75%;" /></p>
<p>继续在系统变量里面新建一个变量</p>
<p>变量名：CLASSPATH</p>
<p>变量值：.;%JAVA_HOME%\lib;%JAVA_HOME%\lib\tools.jar</p>
<p><img src="java6.png" alt="java6" style="zoom:75%;" /></p>
<p>确定保存配置后，win+R打开电脑运行输入cmd进入命令行窗口输入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java -vesion</span><br></pre></td></tr></table></figure>
<p>正确输出java版本，安装完成</p>
<p><img src="java7.png" alt="java7"></p>
<h2 id="Eclipse的安装"><a href="#Eclipse的安装" class="headerlink" title="Eclipse的安装"></a>Eclipse的安装</h2><p>下载Eclipse的oxygen版本的安装包 eclipse-java-oxygen-3a-win32-x86_64.zip，解压到任一位置，打开eclipse.exe即可</p>
<p><img src="eclipse1.png" alt="eclipse1" style="zoom:67%;" /></p>
<h2 id="tomcat的安装和配置"><a href="#tomcat的安装和配置" class="headerlink" title="tomcat的安装和配置"></a>tomcat的安装和配置</h2><p>下载tomcat 7.0的安装包 apache-tomcat-7.0.96.zip，解压到任一位置</p>
<p>【注意】如果文件路径在C盘，要把权限打开。右键属性—&gt;安全—&gt;编辑—&gt;Users—&gt;完全控制—&gt;确定</p>
<p><img src="tomcat1.png" alt="tomcat1"></p>
<p>点击bin\startup.bat即可打开</p>
<h2 id="MySQL安装"><a href="#MySQL安装" class="headerlink" title="MySQL安装"></a>MySQL安装</h2><p>下载MySQL 5.1安装包mysql-essential-5.1.46-winx64.msi</p>
<p>具体安装过程参考<a href="https://blog.csdn.net/tang_chuanlin/article/details/79603063">https://blog.csdn.net/tang_chuanlin/article/details/79603063</a></p>
<h2 id="使用成绩管理系统"><a href="#使用成绩管理系统" class="headerlink" title="使用成绩管理系统"></a>使用成绩管理系统</h2><h3 id="导入数据库"><a href="#导入数据库" class="headerlink" title="导入数据库"></a>导入数据库</h3><p>新建数据库ssms</p>
<p>win+R打开运行，输入cmd打开命令行，输入MySQL的路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd C:\Program Files\MySQL\MySQL Server 5.1\bin </span><br><span class="line">mysql -u root -p </span><br></pre></td></tr></table></figure>
<p>输入密码后进入MySQL</p>
<p><img src="mysql1.png" alt="mysql1"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create database ssms;</span><br><span class="line">use ssms;</span><br><span class="line">set names utf8;</span><br><span class="line">source D:\FirstTerm\lyyzoo-ssms\doc\ssms.sql</span><br></pre></td></tr></table></figure>
<p>输入show tables检查，成功导入数据库</p>
<p><img src="mysql2.png" alt="mysql2"></p>
<h3 id="启动tomcat"><a href="#启动tomcat" class="headerlink" title="启动tomcat"></a>启动tomcat</h3><p>ssms.war可以从eclipse导出，同时也已经保存在网盘中，可以直接使用</p>
<p>打开ssms.war\WEB-INF\classes\c3p0-config.xml文件，修改服务器所在数据库用户名和密码</p>
<p>将ssms.war文件放入tomcat/webapps目录下</p>
<p>打开 apache-tomcat-7.0.96\bin\startup.bat，启动tomcat服务器</p>
<p>打开浏览器，输入 localhost:8080/ssms 进入登录界面</p>
<p><img src="tomcat2.png" alt="tomcat2"></p>
<h3 id="用户名和密码"><a href="#用户名和密码" class="headerlink" title="用户名和密码"></a>用户名和密码</h3><p><strong>管理员账号</strong>：admin</p>
<p><strong>教师账号：</strong><br>        2012<br>        2011<br>        2010<br>        2009<br>        2008<br><strong>学生账号：</strong><br>        201301001<br>        201302002<br>        201401001<br>        201402002<br><strong>密码都为：</strong>111111</p>
]]></content>
      <categories>
        <category>复现</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Eclipse</tag>
        <tag>tomcat</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读】HDNet</title>
    <url>/2021/09/24/HDNet/</url>
    <content><![CDATA[<h1 id="Learning-High-Fidelity-Depths-of-Dressed-Humans-by-Watching-Social-Media-Dance-Videos"><a href="#Learning-High-Fidelity-Depths-of-Dressed-Humans-by-Watching-Social-Media-Dance-Videos" class="headerlink" title="Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos"></a>Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos</h1><p>项目：<a href="https://www.yasamin.page/hdnet_tiktok">https://www.yasamin.page/hdnet_tiktok</a></p>
<p>github：<a href="https://github.com/yasaminjafarian/HDNet_TikTok">https://github.com/yasaminjafarian/HDNet_TikTok</a></p>
<p>这篇文章提出了一个端到端的自监督网络，计算视频中人的表面法向量，来得到高保真的深度值，再进行人体重建。该算法在真实场景和渲染场景都有SOTA性能。</p>
<h3 id="引入："><a href="#引入：" class="headerlink" title="引入："></a>引入：</h3><ol>
<li><p>目前的人体三维重建缺点方法有：</p>
<p>（1）设备要求高，计算复杂</p>
<p>（2）单帧图像，数据量太少</p>
<p>（3）能够建整体，但细节失败</p>
</li>
<li><p>作者提出了着衣人体的高保真3D几何建图方法，使用单一视角图像来预测深度和表面法向量。</p>
</li>
<li><p>舞蹈视频的特点：</p>
<p>（1）单人的，包括各种姿态</p>
<p>（2）没有ground truth，不能使用监督方法，</p>
<p>（3）刚体假设，使得可以利用几何一致性来学习</p>
</li>
<li><p>作者的方法的特点：</p>
<p>（1）表面法向量对细节敏感，将深度曲率和表面法向量匹配</p>
<p>（2）端到端，输入是RGB，输出是高保真深度</p>
<p>（3）HDNet：学习图像和UV坐标的<strong>空间关系</strong>，来产生中间<strong>表面法向量</strong>，预测的表面法向量用来预测<strong>高保真深度</strong></p>
</li>
<li><p>论文的贡献：</p>
<p>（1）制作了TikTok数据集：300个移动平台上公开媒体的舞蹈视频，包括人体掩膜和人体UV坐标</p>
<p>（2）提出一个扭转公式：把三维几何从一个图像扭转到另一个图像，并测量自洽性</p>
<p>（3）HDNet：通过加强几何一致性，来预测表面法向量反应出来的深度</p>
<p>作者的主要贡献在于：人体三维重建、单视图深度估计和人体三维数据集</p>
</li>
</ol>
<h3 id="相关工作："><a href="#相关工作：" class="headerlink" title="相关工作："></a>相关工作：</h3><ol>
<li><p>人体三维重建</p>
<p>（1）参数模型：SCAPE、SMPL，可以从单视图重建人体，但是分辨率不高。通过残差集合来精细化参数模型。</p>
<p>（2）非参数模型：可以用来描述着意人体，但数据集的获取困难。</p>
</li>
<li><p>单视图深度估计</p>
<p>（1）引入表面法向量来精华深度细节</p>
<p>（2）迭代最小二乘、核回归：融合深度和表面法向量</p>
<p>（3）从粗糙到精细的学习方法</p>
<p>（4）融合表面法向量到深度估计</p>
<p>（5）作者的工作：表面法向量和深度一起学习</p>
</li>
<li><p>人体三维数据集：</p>
<p>（1）缺少用于几何预测的数据集，大部分是静态模型</p>
<p>（2）作者提出真实场景的舞蹈视频，来产生深度估计从不同的视角、外表、衣服风格和位姿</p>
</li>
</ol>
<h3 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h3><p>深度是关于像素点位置和图像的一个函数<em>g</em>，现有的方法直接用训练集学习这个函数，存在以下两个缺点：</p>
<p>（1）尽管能够获得整体的深度，但是不能获得精细的局部深度；</p>
<p>（2）需要大量的3d数据，但这样大量的数据集并不存在</p>
<ol>
<li><p>自监督人体深度</p>
<p>（1）假设坐标转换函数<em>h</em>将人体表面坐标<em>u</em>映射到像素坐标<em>x</em>，重建三维点<em>p</em>，p由关于深度z、相机内参、像素坐标的函数得到。三维点<em>p</em>的随时间的变换，由转换函数<em>w</em>得到，转换包括旋转和平移。</p>
<p>（2）为什么w函数有效：（1）人体的部位大多数满足刚体变换（2）对于形变的部分，就扩大时间范围</p>
<p>（3）深度估计原本是稀疏的，通过双线性插值来获得稠密的深度</p>
<p>（4）最小化所有时间的所有点的实际三维位置和预测三维位置的距离和，损失函数<em>Lw</em>。这个让估计出的深度用来监督转换函数，所以使得转换是自监督的</p>
</li>
<li><p>表面法向量和深度的联合学习</p>
<p>（1）表面法向量对局部纹理、皱纹和阴影高度敏感。表面法向量垂直于三维点的切平面，是x轴和y轴单位向量的叉乘，单位向量与像素坐标和深度有关，因此又能够二者能够相互监督</p>
<p>（2）总损失函数为：深度误差<em>Lz</em> + 法向量误差<em>Ln</em> + 自监督法向量夹角误差<em>Ls</em> + 自监督三维点距离误差<em>Lw</em></p>
</li>
<li><p>网络设计和细节</p>
<p>（1）HDNet = 表面法向量预测器 + 深度预测器</p>
<p>表面法向量预测器</p>
<p>输入：2种数据—RGB图像和前景掩膜</p>
<p>输出：预测的表面法向量</p>
<p>深度预测器</p>
<p>输入：3种数据—RGB图像、前景掩膜和UV坐标</p>
<p>输出：预测的深度</p>
<p>两个预测器都采用堆叠沙漏网络（ stacked hourglasses network）作为骨架</p>
</li>
</ol>
<p>   （2）孪生神经网络（Siamese network）</p>
<p>   作用：同一视频两个不同时刻i和j的图像经过深度预测后，进行i-&gt;j的深度转换，来计算自监督三维点距离误差<em>Lw</em>。</p>
<p>   （3）图像对选取方法</p>
<p>   同一视频中，随机选取满足有5个共同的身体部位课件，且每个身体部位至少有50个UV坐标重合。</p>
<p>   （4）使用Adam optimizer优化</p>
<h3 id="数据集："><a href="#数据集：" class="headerlink" title="数据集："></a>数据集：</h3><p>作者做的TikTok Dataset</p>
<h3 id="实验："><a href="#实验：" class="headerlink" title="实验："></a>实验：</h3><p>在服务器上跑了一下，应该是tensorflow和cuda版本的问题，没跑起来。最后在colab上跑了一下demo</p>
<p>效果：</p>
<p><img src="pic1.png" alt="pic1"></p>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>人体三维重建</category>
      </categories>
      <tags>
        <tag>人体三维重建</tag>
        <tag>深度估计</tag>
        <tag>表面法向量</tag>
      </tags>
  </entry>
  <entry>
    <title>语义建图性能评估</title>
    <url>/2021/04/20/%E8%AF%AD%E4%B9%89%E5%BB%BA%E5%9B%BE%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0/</url>
    <content><![CDATA[<h2 id="MISD-SLAM预测轨迹与ground-truth对比"><a href="#MISD-SLAM预测轨迹与ground-truth对比" class="headerlink" title="MISD-SLAM预测轨迹与ground truth对比"></a>MISD-SLAM预测轨迹与ground truth对比</h2><h3 id="1-利用EVO工具评估"><a href="#1-利用EVO工具评估" class="headerlink" title="1.利用EVO工具评估"></a>1.利用EVO工具评估</h3><p>（1）安装evo工具</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install evo --upgrade --no-binary evo --user</span><br></pre></td></tr></table></figure>
<p>（2）拷贝MISD_CameraTrajectory.txt、ORBSLAM3_CameraTrajectory.txt和groundtruth.txt早评估文件夹下</p>
<p>（3）轨迹对比</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">evo_traj tum MISD_CameraTrajectory.txt ORBSLAM3_CameraTrajectory.txt --ref=groundtruth.txt -as --plot --plot_mode xy</span><br></pre></td></tr></table></figure>
<p>得到三条轨迹对比结果：</p>
<p><img src="png0.png" alt="1"></p>
<p>可以看到，动态物体剔除很大程度上提高了SLAM轨迹预测的准确度。</p>
<p>（4）计算绝对位姿误差</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">evo_ape tum groundtruth.txt MISD_CameraTrajectory.txt -p --plot_mode=xy -as</span><br></pre></td></tr></table></figure>
<p>（5）计算相对位姿误差</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">evo_rpe tum groundtruth.txt MISD_CameraTrajectory.txt -p --plot_mode=xy -as</span><br></pre></td></tr></table></figure>
<h3 id="2-利用TUM-RGB-D评估工具"><a href="#2-利用TUM-RGB-D评估工具" class="headerlink" title="2.利用TUM RGB-D评估工具"></a>2.利用TUM RGB-D评估工具</h3><p>（1）下载测评工具</p>
<p><a href="https://svncvpr.in.tum.de/cvpr-ros-pkg/trunk/rgbd_benchmark/rgbd_benchmark_tools/">https://svncvpr.in.tum.de/cvpr-ros-pkg/trunk/rgbd_benchmark/rgbd_benchmark_tools/</a></p>
<p>（2）运行evaluate_ate.py</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">python evaluate_ate.py groundtruth.txt ORBSLAM3_CameraTrajectory.txt --plot ORBSLAM3_result.png</span><br><span class="line">python evaluate_ate.py groundtruth.txt MISD_CameraTrajectory.txt --plot MISD_result.png</span><br></pre></td></tr></table></figure>
<p>可以看到：</p>
<p><img src="png1.png" alt="2"><img src="png2.png" alt="3"></p>
<h3 id="MISD-SLAM建图效果"><a href="#MISD-SLAM建图效果" class="headerlink" title="MISD-SLAM建图效果"></a>MISD-SLAM建图效果</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">pcl_viewer SemanticMappingResult.pcd -cam test1.cam</span><br><span class="line">pcl_viewer SemanticMappingResult.pcd -cam test2.cam</span><br><span class="line">pcl_viewer SemanticMappingResult.pcd -cam test3.cam</span><br></pre></td></tr></table></figure>
<p>MISD-SLAM通过将动态物体剔除，能够建立环境的静态地图，以下是在TUM RGB-D数据集的fr3_walking_halfsphere序列的建图效果。</p>
<p><img src="png3.png" alt="4"><img src="png4.png" alt="5"><img src="png5.png" alt="6"></p>
<h3 id=""><a href="#" class="headerlink" title=" "></a> </h3>]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>实例分割</tag>
        <tag>SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLACT++与ORBSLAM2的语义建图</title>
    <url>/2021/04/10/YOLACT++%E4%B8%8EORBSLAM2%E7%9A%84%E8%AF%AD%E4%B9%89%E5%BB%BA%E5%9B%BE/</url>
    <content><![CDATA[<h3 id="服务端"><a href="#服务端" class="headerlink" title="服务端"></a>服务端</h3><p>语义分割作为服务端，在终端1：</p>
<p>1.调整~/.bashrc至anaconda3</p>
<p>2.打开conda的yolact虚拟环境</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda activate yolact</span><br><span class="line"><span class="built_in">cd</span> ~/catkin_ws/src/YOLACT_ORBSLAM2/yolact_server/</span><br><span class="line">python server.py</span><br></pre></td></tr></table></figure>
<h3 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h3><p>ORB_SLAM2作为客户端，在终端2：</p>
<p>1.调整~/.bashrc至ROS</p>
<p>2.输入以下命令，运行ORB_SLAM2</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM1.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg1_xyz/ ./Examples/RGB-D/associations/fr1_xyz.txt</span><br></pre></td></tr></table></figure>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_long_office_household/ ./Examples/RGB-D/associations/fr3_long_ofc.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_nostructure_texture_far/ ./Examples/RGB-D/associations/fr3_nstr_tex_far.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_sitting_static/ ./Examples/RGB-D/associations/fr3_sit_stc.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM2.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg2_desk_with_person/ ./Examples/RGB-D/associations/fr2_dsk_psn.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_static/ ./Examples/RGB-D/associations/fr3_wlk_stc.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_xyz/ ./Examples/RGB-D/associations/fr3_wlk_xyz.txt</p>
<h3 id="可能出现的问题"><a href="#可能出现的问题" class="headerlink" title="可能出现的问题"></a>可能出现的问题</h3><p>1.Traceback (most recent call last):<br>  File “server.py”, line 44, in <module><br>    tcpSerSock.bind(ADDR)<br>OSError: [Errno 98] Address already in use</p>
<p>解决方法：</p>
<p>在命令行中输入</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">netstat -tunpl</span><br></pre></td></tr></table></figure>
<p>查看占用状态</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">kill</span> -9 PID号</span><br></pre></td></tr></table></figure>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM1.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg1_xyz/ /home/youyx/catkin_ws/src/YOLACT_ORBSLAM3/Examples/RGB-D/associations/fr1_xyz.txt</p>
<p>assosiation文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;home&#x2F;youyx&#x2F;catkin_ws&#x2F;src&#x2F;YOLACT_ORBSLAM2&#x2F;Examples&#x2F;RGB-D</span><br><span class="line">python associate.py &#x2F;home&#x2F;youyx&#x2F;data&#x2F;datasets&#x2F;TUM&#x2F;rgbd_dataset_freiburg3_walking_static&#x2F;rgb.txt &#x2F;home&#x2F;youyx&#x2F;data&#x2F;datasets&#x2F;TUM&#x2F;rgbd_dataset_freiburg3_walking_static&#x2F;depth.txt &gt; .&#x2F;associations&#x2F;fr3_wlk_stc.txt</span><br></pre></td></tr></table></figure>
<p>python associate.py /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_xyz/rgb.txt /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_xyz/depth.txt &gt; ./associations/fr3_wlk_xyz.txt</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>ORB-SLAM</category>
        <category>语义建图</category>
        <category>实例分割</category>
      </categories>
      <tags>
        <tag>实例分割</tag>
        <tag>SLAM</tag>
        <tag>语义建图</tag>
        <tag>ORB-SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>pspnet+ORB_SLAM2+octomap</title>
    <url>/2021/04/07/pspnet+ORB_SLAM2+octomap/</url>
    <content><![CDATA[<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> semantic_slam_floatlazer</span><br><span class="line"><span class="built_in">cd</span> ORB_SLAM2 </span><br><span class="line">./build.sh</span><br><span class="line"><span class="built_in">cd</span> ../../..</span><br><span class="line"></span><br><span class="line">catkin_make</span><br></pre></td></tr></table></figure>
<p>catkin_make的时候可能会遇到一些问题，比如</p>
<p>CMake Error at /opt/ros/indigo/share/catkin/cmake/catkinConfig.cmake:83 (find_package):<br>  Could not find a package configuration file provided by “octomap_msgs” with<br>  any of the following names:</p>
<p>​        octomap_msgsConfig.cmake<br>​        octomap_msgs-config.cmake</p>
<p>  Add the installation prefix of “octomap_msgs” to CMAKE_PREFIX_PATH or set<br>  “octomap_msgs_DIR” to a directory containing one of the above files.  If<br>  “octomap_msgs” provides a separate development package or SDK, be sure it<br>  has been installed.</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo apt install ros-indigo-octomap*</span><br></pre></td></tr></table></figure>
<p>再catkin_make一下</p>
<p>setuptools&gt;=41.0.0<br>numpy&gt;=1.15<br>scipy<br>Pillow<br>cython<br>opencv-python==3.3.1.11<br>matplotlib<br>scikit-image<br>tensorflow-gpu==1.13.1<br>keras==2.0.8<br>h5py<br>imageio==2.6.1<br>imgaug<br>pandas<br>future<br>torch<br>torchvision<br>protobuf<br>IPython[all]</p>
<p>把依赖包写入requirements.txt放在/home/youyx/catkin_ws/src/semantic_slam_floatlazer/下</p>
<p>sudo pip install -r /…/requirements.txt</p>
<h3 id="模型和数据集准备"><a href="#模型和数据集准备" class="headerlink" title="模型和数据集准备"></a>模型和数据集准备</h3><p>模型是pspnet_50_ade20k.pth，在google drive上下载</p>
<p><a href="https://drive.google.com/file/d/1u_BEWdVIYiDnpVmAxwME1z3rnWWkjxm5/view?usp=sharing">https://drive.google.com/file/d/1u_BEWdVIYiDnpVmAxwME1z3rnWWkjxm5/view?usp=sharing</a></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /semantic_slam_floatlazer/semantic_slam/params </span><br><span class="line">vim semantic_cloud.yaml </span><br><span class="line">vim semantic_cloud_tum.yaml </span><br></pre></td></tr></table></figure>
<p>将model_path修改为模型保存的路径</p>
<p>model_path: “/home/youyx/data/semantic_slam/pspnet_50_ade20k.pth”</p>
<p>下载tum数据集rgbd_dataset_freiburg1_room.bag</p>
<p><a href="https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_room.bag">https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_room.bag</a></p>
<h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>终端1：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">roslaunch floatlazer_semantic_slam semantic_mapping_tum.launch</span><br></pre></td></tr></table></figure>
<p>终端2：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home/youyx/data/semantic_slam/TUM/freiburg1</span><br><span class="line">rosbag play --clock rgbd_dataset_freiburg1_room.bag</span><br></pre></td></tr></table></figure>
<h3 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h3><p><img src="pic1.png" alt="1"></p>
]]></content>
      <categories>
        <category>实例分割</category>
        <category>SLAM</category>
        <category>语义建图</category>
      </categories>
      <tags>
        <tag>实例分割</tag>
        <tag>SLAM</tag>
        <tag>语义建图</tag>
      </tags>
  </entry>
  <entry>
    <title>ORB_SLAM2构建稠密地图-高博版</title>
    <url>/2021/03/18/ORB_SLAM2%E6%9E%84%E5%BB%BA%E7%A8%A0%E5%AF%86%E5%9C%B0%E5%9B%BE-%E9%AB%98%E5%8D%9A%E7%89%88/</url>
    <content><![CDATA[<h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>操作系统： Ubuntu 18.04</p>
<p>1.Opencv（3.2.0版本）</p>
<p>2.PCL</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install libpcl-dev</span><br></pre></td></tr></table></figure>
<p>3.Eigen3（3.2版本）</p>
<p>4.Pangolin</p>
<h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><p>1.下载源文件</p>
<p>从高博github上<a href="https://github.com/gaoxiang12/ORBSLAM2_with_pointcloud_map.git下载ORB_SLAM2_modified文件夹，将其中的ORB_SLAM2_modified子文件夹放到~/catkin_ws/src/下">https://github.com/gaoxiang12/ORBSLAM2_with_pointcloud_map.git下载ORB_SLAM2_modified文件夹，将其中的ORB_SLAM2_modified子文件夹放到~/catkin_ws/src/下</a></p>
<p>2.拷贝Vocabulary文件夹</p>
<p>将ORB_SLAM2中的Vocabulary文件夹复制到ORB_SLAM2_modified路径下</p>
<p>3.删除build文件夹</p>
<p>将~/catkin_ws/src/ORB_SLAM2_modified/build、~/catkin_ws/src/ORB_SLAM2_modified/Thirdparty/DBoW2/build 和 ~/catkin_ws/src/ORB_SLAM2_modified/Thirdparty/g2o/build删除</p>
<p>4.运行build.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ~&#x2F;catkin_ws&#x2F;src&#x2F;ORB_SLAM2_modified&#x2F;</span><br><span class="line">chmod +x .&#x2F;build.sh</span><br><span class="line">.&#x2F;build.sh</span><br></pre></td></tr></table></figure>
<h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;rgbd_tum Vocabulary&#x2F;ORBvoc.bin Examples&#x2F;RGB-D&#x2F;TUM1.yaml &#x2F;home&#x2F;youyx&#x2F;data&#x2F;datasets&#x2F;TUM&#x2F;rgbd_dataset_freiburg1_xyz&#x2F; &#x2F;home&#x2F;youyx&#x2F;catkin_ws&#x2F;src&#x2F;ORB_SLAM2_modified&#x2F;Examples&#x2F;RGB-D&#x2F;associations&#x2F;fr1_xyz.txt</span><br></pre></td></tr></table></figure>
<h3 id="彩色地图"><a href="#彩色地图" class="headerlink" title="彩色地图"></a>彩色地图</h3><p>运行后我们发现得到稠密点云地图是黑白地图。这里来构建彩色地图。</p>
<p>1.修改Tracking.h文件</p>
<p>Frame mCurrentFrame;<br>cv::Mat mImRGB;//declared<br>cv::Mat mImGray;</p>
<p>2.修改Tracking.cc文件</p>
<p>Modified place 1:<br>cv::Mat Tracking::GrabImageRGBD(const cv::Mat &amp;imRGB,const cv::Mat &amp;imD, const double &amp;timestamp)<br>{<br>mImRGB = imRGB;//Modified place 1<br>mImGray = imRGB;<br>……</p>
<p>Modified place 2:<br>mpPointCloudMapping-&gt;insertKeyFrame( pKF, this-&gt;mImGray, this-&gt;mImDepth );//change the mImGray to mImRGB as next row<br>mpPointCloudMapping-&gt;insertKeyFrame( pKF, this-&gt;mImRGB, this-&gt;mImDepth );//Modified place 2</p>
<p>3.修改后重新编译</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/ORB_SLAM2_modified/build</span><br><span class="line">make -j8</span><br></pre></td></tr></table></figure>
<h3 id="保存地图"><a href="#保存地图" class="headerlink" title="保存地图"></a>保存地图</h3><p>高博的程序只能实时查看地图，不能保存。这里修改文件pointcloudmapping.cc，调用 PCL 库的 pcl::io::savePCDFileBinary 函数就可以保存点云地图了</p>
<p>1.加入头文件</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;pcl/io/pcd_io.h&gt;</span></span></span><br></pre></td></tr></table></figure>
<p>2.调用pcl::io::savePCDFileBinary 函数</p>
<p>在 void PointCloudMapping::viewer() 函数中（ 123 行附近）加入保存地图的命令：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">for</span> ( <span class="keyword">size_t</span> i=lastKeyframeSize; i&lt;N ; i++ )</span><br><span class="line">&#123;</span><br><span class="line">    PointCloud::Ptr p = generatePointCloud( keyframes[i], colorImgs[i], depthImgs[i] );</span><br><span class="line">    *globalMap += *p;</span><br><span class="line">&#125;</span><br><span class="line">pcl::io::savePCDFileBinary(<span class="string">&quot;vslam.pcd&quot;</span>, *globalMap);   <span class="comment">// 只需要加入这一句</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>3.修改后重新编译</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/ORB_SLAM2_modified/build</span><br><span class="line">make -j8</span><br></pre></td></tr></table></figure>
<p>4.运行建图命令</p>
<p>就在 ~/ORB_SLAM2_modified 路径下产生一个名为 vslam.pcd 的点云文件。</p>
<h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>1.若运行fr2和fr3数据集，要把参数PointCloudMapping.Resolution: 0.01加入到TUMX.yaml配置文件里</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>ORB-SLAM</category>
        <category>点云地图</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>ORB-SLAM</tag>
        <tag>点云地图</tag>
      </tags>
  </entry>
  <entry>
    <title>deeplab+ORB_SLAM2的语义建图</title>
    <url>/2021/03/15/deeplab+ORB_SLAM2%E7%9A%84%E8%AF%AD%E4%B9%89%E5%BB%BA%E5%9B%BE/</url>
    <content><![CDATA[<h3 id="服务端"><a href="#服务端" class="headerlink" title="服务端"></a>服务端</h3><p>语义分割作为服务端，在终端1：</p>
<p>1.调整~/.bashrc至anaconda3</p>
<p>2.打开conda的deeplab虚拟环境</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda activate deeplab-pytorch</span><br><span class="line"><span class="built_in">cd</span> ~/catkin_ws/src/Semantic_Mapping_on_ORBSLAM2/deeplabv2_server/</span><br><span class="line">python inference.py</span><br></pre></td></tr></table></figure>
<h3 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h3><p>ORB_SLAM2作为客户端，在终端2：</p>
<p>1.调整~/.bashrc至ROS</p>
<p>2.输入以下命令，运行ORB_SLAM2</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM1.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg1_xyz/ /home/youyx/catkin_ws/src/Semantic_Mapping_on_ORBSLAM2/Examples/RGB-D/associations/fr1_xyz.txt</span><br></pre></td></tr></table></figure>
<h3 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h3><p><img src="pic1.png" alt="1"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>ORB-SLAM</category>
        <category>语义建图</category>
        <category>语义分割</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>语义建图</tag>
        <tag>ORB-SLAM</tag>
        <tag>语义分割</tag>
      </tags>
  </entry>
  <entry>
    <title>ORB_SLAM3配置</title>
    <url>/2021/03/15/ORB_SLAM3%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h3 id="安装下载工具"><a href="#安装下载工具" class="headerlink" title="安装下载工具"></a>安装下载工具</h3><p>cmake、gcc、g++和git工具</p>
<h3 id="安装Pangolin"><a href="#安装Pangolin" class="headerlink" title="安装Pangolin"></a>安装Pangolin</h3><h3 id="安装OpenCV"><a href="#安装OpenCV" class="headerlink" title="安装OpenCV"></a>安装OpenCV</h3><h3 id="安装Eigen3"><a href="#安装Eigen3" class="headerlink" title="安装Eigen3"></a>安装Eigen3</h3><p>以上详细过程见ORB_SLAM2配置</p>
<h3 id="建立ORB-SLAM3"><a href="#建立ORB-SLAM3" class="headerlink" title="建立ORB_SLAM3"></a>建立ORB_SLAM3</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/catkin_ws/src</span><br></pre></td></tr></table></figure>
<p>复制ORB_SLAM3到src文件夹下，两种方法</p>
<p>方法一：git复制</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/UZ-SLAMLab/ORB_SLAM3.git ORB_SLAM3</span><br></pre></td></tr></table></figure>
<p>方法二：手动下载，然后解压到src文件夹</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ORB_SLAM3</span><br><span class="line">chmod +x build.sh</span><br><span class="line">./build.sh</span><br></pre></td></tr></table></figure>
<h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>下载EuRoC数据集</p>
<p><a href="https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets#the_euroc_mav_dataset">https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets#the_euroc_mav_dataset</a></p>
<p>修改ORB-SLAM3/euroc_examples.sh文件中的数据集路径</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="comment">#pathDatasetEuroc=&#x27;/Datasets/EuRoC&#x27; #Example, it is necesary to change it by the dataset path</span></span><br><span class="line">pathDatasetEuroc=<span class="string">&#x27;/home/youyx/data/datasets/EuRoC/&#x27;</span> <span class="comment">#Example, it is necesary to change it by the dataset path</span></span><br></pre></td></tr></table></figure>
<h4 id="Monocular"><a href="#Monocular" class="headerlink" title="Monocular"></a>Monocular</h4><p>MH</p>
<p>./Examples/Monocular/mono_euroc ./Vocabulary/ORBvoc.txt ./Examples/Monocular/EuRoC.yaml /home/youyx/data/datasets/EuRoC/MH01 ./Examples/Monocular/EuRoC_TimeStamps/MH01.txt dataset-MH01_mono</p>
<p>./Examples/Monocular/mono_euroc ./Vocabulary/ORBvoc.txt ./Examples/Monocular/EuRoC.yaml /home/youyx/data/datasets/EuRoC/MH02 ./Examples/Monocular/EuRoC_TimeStamps/MH02.txt dataset-MH02_mono</p>
<p>./Examples/Monocular/mono_euroc ./Vocabulary/ORBvoc.txt ./Examples/Monocular/EuRoC.yaml /home/youyx/data/datasets/EuRoC/MH03 ./Examples/Monocular/EuRoC_TimeStamps/MH03.txt dataset-MH03_mono</p>
<p>./Examples/Monocular/mono_euroc ./Vocabulary/ORBvoc.txt ./Examples/Monocular/EuRoC.yaml /home/youyx/data/datasets/EuRoC/MH04 ./Examples/Monocular/EuRoC_TimeStamps/MH04.txt dataset-MH04_mono</p>
<p>./Examples/Monocular/mono_euroc ./Vocabulary/ORBvoc.txt ./Examples/Monocular/EuRoC.yaml /home/youyx/data/datasets/EuRoC/MH05 ./Examples/Monocular/EuRoC_TimeStamps/MH05.txt dataset-MH05_mono</p>
<p>V1</p>
<p>./Examples/Monocular/mono_euroc ./Vocabulary/ORBvoc.txt ./Examples/Monocular/EuRoC.yaml /home/youyx/data/datasets/EuRoC/V101 ./Examples/Monocular/EuRoC_TimeStamps/V101.txt dataset-V101_mono</p>
<p>V2</p>
<p>./Examples/Monocular/mono_euroc ./Vocabulary/ORBvoc.txt ./Examples/Monocular/EuRoC.yaml /home/youyx/data/datasets/EuRoC/V201 ./Examples/Monocular/EuRoC_TimeStamps/V201.txt dataset-V201_mono</p>
<p>MultiSession Monocular</p>
<p>./Examples/Monocular/mono_euroc ./Vocabulary/ORBvoc.txt ./Examples/Monocular/EuRoC.yaml /home/youyx/data/datasets/EuRoC/V101 ./Examples/Monocular/EuRoC_TimeStamps/V101.txt /home/youyx/data/datasets/EuRoC/V102 ./Examples/Monocular/EuRoC_TimeStamps/V102.txt /home/youyx/data/datasets/EuRoC/V103 ./Examples/Monocular/EuRoC_TimeStamps/V103.txt dataset-V101_to_V103_mono</p>
<h4 id="Stereo"><a href="#Stereo" class="headerlink" title="Stereo"></a>Stereo</h4><p>./Examples/Stereo/stereo_euroc ./Vocabulary/ORBvoc.txt ./Examples/Stereo/EuRoC.yaml /home/youyx/data/datasets/EuRoC/V101 ./Examples/Stereo/EuRoC_TimeStamps/V101.txt dataset-V101_stereo</p>
<h3 id="RGB-D"><a href="#RGB-D" class="headerlink" title="RGB-D"></a>RGB-D</h3><p>cd ./catkin_ws/src/ORB_SLAM3-dev_bugs_fixed</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM1.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg1_xyz/ ./Examples/RGB-D/associations/fr1_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_xyz/ ./Examples/RGB-D/associations/fr3_wlk_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_static/ ./Examples/RGB-D/associations/fr3_wlk_stc.txt</p>
<p>(sharon)<br>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM3.yaml /home/sharon/Documents/TUM/rgbd_dataset_freiburg3_walking_static/ ./Examples/RGB-D/associations/fr3_wlk_stc.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM3.yaml /home/sharon/Documents/TUM/rgbd_dataset_freiburg3_walking_xyz ./Examples/RGB-D/associations/fr3_wlk_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM1.yaml /home/sharon/Documents/TUM/rgbd_dataset_freiburg1_xyz/ ./Examples/RGB-D/associations/fr1_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM3.yaml /home/sharon/Documents/TUM/rgbd_dataset_freiburg3_nostructure_texture_far/ ./Examples/RGB-D/associations/fr3_nst_far.txt</p>
<h3 id="ORB-SLAM3-amp-YOLACT-的语义建图"><a href="#ORB-SLAM3-amp-YOLACT-的语义建图" class="headerlink" title="ORB_SLAM3&amp;YOLACT++的语义建图"></a>ORB_SLAM3&amp;YOLACT++的语义建图</h3><p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM1.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg1_xyz/ ./Examples/RGB-D/associations/fr1_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_static/ ./Examples/RGB-D/associations/fr3_wlk_stc.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_xyz/ ./Examples/RGB-D/associations/fr3_wlk_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_sitting_static/ ./Examples/RGB-D/associations/fr3_sit_stc.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_xyz/ ./Examples/RGB-D/associations/fr3_wlk_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_static/ ./Examples/RGB-D/associations/fr3_wlk_stc.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_rpy/ ./Examples/RGB-D/associations/fr3_wlk_rpy.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_halfsphere/ ./Examples/RGB-D/associations/fr3_wlk_half.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_sitting_xyz/ ./Examples/RGB-D/associations/fr3_sit_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_sitting_static/ ./Examples/RGB-D/associations/fr3_sit_stc.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_sitting_rpy/ ./Examples/RGB-D/associations/fr3_sit_rpy.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_sitting_halfsphere/ ./Examples/RGB-D/associations/fr3_sit_half.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM2.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg2_desk_with_person/ ./Examples/RGB-D/associations/fr2_dsk_psn.txt</p>
]]></content>
      <categories>
        <category>ORB-SLAM</category>
      </categories>
      <tags>
        <tag>ORB-SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>ORB_SLAM2配置</title>
    <url>/2021/03/09/ORB_SLAM2%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h3 id="安装工具"><a href="#安装工具" class="headerlink" title="安装工具"></a>安装工具</h3><p>下载cmake、gcc、g++和git工具</p>
<p>下载cmake</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install cmake</span><br></pre></td></tr></table></figure>
<p>下载git</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install git</span><br></pre></td></tr></table></figure>
<p>下载gcc和g++</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install gcc g++  </span><br></pre></td></tr></table></figure>
<h3 id="安装Pangolin"><a href="#安装Pangolin" class="headerlink" title="安装Pangolin"></a>安装Pangolin</h3><p>先安装依赖项</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libglew-dev</span><br><span class="line">sudo apt-get install libpython2.7-dev</span><br></pre></td></tr></table></figure>
<p>下载Pangolin</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;stevenlovegrove&#x2F;Pangolin.git</span><br><span class="line">cd Pangolin</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake -DCPP11_NO_BOOSR&#x3D;1 ..</span><br></pre></td></tr></table></figure>
<p>开始编译</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo make -j8</span><br></pre></td></tr></table></figure>
<p>编译好了后安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<h3 id="安装OpenCV"><a href="#安装OpenCV" class="headerlink" title="安装OpenCV"></a>安装OpenCV</h3><p>我装的是opencv 3.2.0</p>
<p>1.先安装依赖包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install libgtk2.0-dev  </span><br><span class="line">sudo apt install pkg-config</span><br></pre></td></tr></table></figure>
<p>2.到opencv网站下载opencv 3.2.0</p>
<p><a href="https://link.zhihu.com/?target=https%3A//opencv.org/releases.html">https://link.zhihu.com/?target=https%3A//opencv.org/releases.html</a></p>
<p>3.开始安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">unzip opencv-3.2.0.zip</span><br><span class="line">cd opencv-3.2.0</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br></pre></td></tr></table></figure>
<p>4.开始编译</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cmake -D CMAKE_BUILD_TYPE&#x3D;Release -D CMAKE_INSTALL_PREFIX&#x3D;&#x2F;usr&#x2F;local -D ENABLE_PRECOMPILED_HEADERS&#x3D;OFF ..</span><br></pre></td></tr></table></figure>
<p>这里可能会出现 以下问题</p>
<p>Downloading ippicv_linux_20151201.tgz…<br>CMake Error at 3rdparty/ippicv/downloader.cmake:73 (file):<br>  file DOWNLOAD HASH mismatch</p>
<p>手动下载ippicv_linux_20151201.tgz，替换到opencv-3.2.0/3rdparty/ippicv/downloads/linux-808b791a6eac9ed78d32a7666804320e文件下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo make -j8</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>5.添加路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo vim &#x2F;etc&#x2F;ld.so.conf.d&#x2F;opencv.conf   </span><br></pre></td></tr></table></figure>
<p>内容：</p>
<p>/usr/local/lib</p>
<p>6.添加环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo vim ~&#x2F;.profile</span><br></pre></td></tr></table></figure>
<p>.profile最后一行添加：</p>
<p>PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig<br>export PKG_CONFIG_PATH</p>
<p>7.测试</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ..&#x2F;samples&#x2F;cpp&#x2F;example_cmake</span><br><span class="line">cmake .</span><br><span class="line">make</span><br><span class="line">.&#x2F;opencv_example</span><br></pre></td></tr></table></figure>
<p>结果如下说明opencv安装成功</p>
<p><img src="pic1.png" alt="1"></p>
<h3 id="安装Eigen3"><a href="#安装Eigen3" class="headerlink" title="安装Eigen3"></a>安装Eigen3</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libeigen3-dev</span><br></pre></td></tr></table></figure>
<h3 id="安装ROS-melodic"><a href="#安装ROS-melodic" class="headerlink" title="安装ROS-melodic"></a>安装ROS-melodic</h3><p>参考以下博文：</p>
<p><a href="https://blog.csdn.net/qq_41450811/article/details/99079041">https://blog.csdn.net/qq_41450811/article/details/99079041</a></p>
<p>但是在初始化rosdep的时候，遇到了大麻烦：</p>
<p>ERROR: cannot download default sources list from:<br><a href="https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/sources.list.d/20-default.list">https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/sources.list.d/20-default.list</a><br>Website may be down.</p>
<p>网上修改hosts的方法也行不通，最后根据以下博文手动把包下载下来，并修改源码：</p>
<p><a href="https://blog.csdn.net/nanianwochengshui/article/details/105702188">https://blog.csdn.net/nanianwochengshui/article/details/105702188</a></p>
<h3 id="建立ORB-SLAM2"><a href="#建立ORB-SLAM2" class="headerlink" title="建立ORB-SLAM2"></a>建立ORB-SLAM2</h3><p>建立ROS工作区catkin_ws</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ~</span><br><span class="line">mkdir .&#x2F;catkin_ws</span><br><span class="line">cd catkin_ws</span><br><span class="line">mkdir src</span><br></pre></td></tr></table></figure>
<p>方法1：复制项目<br>cd ~/catkin_ws/src<br>git clone <a href="https://github.com/raulmur/ORB_SLAM2.git">https://github.com/raulmur/ORB_SLAM2.git</a> ORB_SLAM2</p>
<p>但是经常会下载不下来，所以可以用方法2</p>
<p>或者<br>方法2：将ORB_SLAM2项目下载到~/catkin_ws/src下，解压</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ORB_SLAM2</span><br><span class="line">chmod +x build.sh</span><br><span class="line">.&#x2F;build.sh</span><br></pre></td></tr></table></figure>
<p>这时候可能会出现以下问题：</p>
<p>error: ‘usleep’ was not declared in this scope<br>             usleep(3000);</p>
<p>方法：在./include/System.h文件中添加#include<unistd.h></p>
<h3 id="跑MONO-TUM数据集"><a href="#跑MONO-TUM数据集" class="headerlink" title="跑MONO_TUM数据集"></a>跑MONO_TUM数据集</h3><p>下载tum数据集<a href="http://vision.in.tum.de/data/datasets/rgbd-dataset/download，解压">http://vision.in.tum.de/data/datasets/rgbd-dataset/download，解压</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar zxvf rgbd_dataset_freiburg3_nostructure_texture_far.tgz -C .&#x2F;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ~&#x2F;catkin_ws&#x2F;src&#x2F;ORB_SLAM2&#x2F;</span><br></pre></td></tr></table></figure>
<p>(youyx)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;Examples&#x2F;Monocular&#x2F;mono_tum Vocabulary&#x2F;ORBvoc.txt .&#x2F;Examples&#x2F;Monocular&#x2F;TUM3.yaml &#x2F;home&#x2F;youyx&#x2F;data&#x2F;tum&#x2F;rgbd_dataset_freiburg3_nostructure_texture_far</span><br></pre></td></tr></table></figure>
<p>(sharon)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;Examples&#x2F;Monocular&#x2F;mono_tum Vocabulary&#x2F;ORBvoc.txt Examples&#x2F;Monocular&#x2F;TUM3.yaml &#x2F;home&#x2F;sharon&#x2F;Documents&#x2F;rgbd_dataset_freiburg3_nostructure_texture_far&#x2F;</span><br></pre></td></tr></table></figure>
<p>(weip)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;Examples&#x2F;Monocular&#x2F;mono_tum Vocabulary&#x2F;ORBvoc.txt Examples&#x2F;Monocular&#x2F;TUM3.yaml &#x2F;home&#x2F;youyx&#x2F;data&#x2F;tum&#x2F;rgbd_dataset_freiburg3_nostructure_texture_far&#x2F;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod +x build_ros.sh</span><br><span class="line">.&#x2F;build_ros.sh</span><br></pre></td></tr></table></figure>
<p>(PS:运行到这里的时候才发现这台新电脑还没装ROS)</p>
<p>fatal error: Eigen3/Core: 没有那个文件或目录</p>
<p>建立软链接</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo ln -s &#x2F;usr&#x2F;include&#x2F;eigen3&#x2F;Eigen &#x2F;usr&#x2F;include&#x2F;Eigen</span><br></pre></td></tr></table></figure>
<p>重新</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;build_ros.sh</span><br></pre></td></tr></table></figure>
<p>又遇到问题：undefined reference to symbol ‘_ZN5boost6system15system_categoryEv’<br>方法：在Examples/ROS/ORB_SLAM2/文件夹下的CMakeLists.txt文件中<br>set(LIBS<br>${OpenCV_LIBS}<br>${EIGEN3_LIBS}<br>${Pangolin_LIBRARIES}<br>${PROJECT_SOURCE_DIR}/../../../Thirdparty/DBoW2/lib/libDBoW2.so<br>${PROJECT_SOURCE_DIR}/../../../Thirdparty/g2o/lib/libg2o.so<br>${PROJECT_SOURCE_DIR}/../../../lib/libORB_SLAM2.so<br>改为<br>set(LIBS<br>${OpenCV_LIBS}<br>${EIGEN3_LIBS}<br>${Pangolin_LIBRARIES}<br>${PROJECT_SOURCE_DIR}/../../../Thirdparty/DBoW2/lib/libDBoW2.so<br>${PROJECT_SOURCE_DIR}/../../../Thirdparty/g2o/lib/libg2o.so<br>${PROJECT_SOURCE_DIR}/../../../lib/libORB_SLAM2.so<br>-lboost_system<br>)</p>
<p>完成！</p>
<p>用ROS启动ORB_SLAM2</p>
<p>（sharon）</p>
<p>rosrun ORB_SLAM2 Mono /home/sharon/catkin_ws/src/ORB_SLAM2/Vocabulary/ORBvoc.txt /home/sharon/catkin_ws/src/ORB_SLAM2/Examples/Monocular/TUM2.yaml</p>
<p>（youyx）</p>
<p>rosrun ORB_SLAM2 Mono /home/sharon/catkin_ws/src/ORB_SLAM2/Vocabulary/ORBvoc.txt /home/sharon/catkin_ws/src/ORB_SLAM2/Examples/Monocular/TUM2.yaml</p>
<h3 id="跑RGBD-TUM数据集"><a href="#跑RGBD-TUM数据集" class="headerlink" title="跑RGBD_TUM数据集"></a>跑RGBD_TUM数据集</h3><p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM2.yaml /home/sharon/Documents/rgbd_dataset_freiburg2_pioneer_360 associations.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM2.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg2_desk_with_person/ ./Examples/RGB-D/associations/fr2_dsk_psn.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_xyz/ ./Examples/RGB-D/associations/fr3_wlk_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_sitting_xyz/ ./Examples/RGB-D/associations/fr3_sit_xyz.txt</p>
]]></content>
      <categories>
        <category>ROS</category>
        <category>ORB_SLAM2</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>ORB_SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title>ROS-launch-camera</title>
    <url>/2021/03/08/ROS-launch-camera/</url>
    <content><![CDATA[<h2 id="ROS启动摄像头"><a href="#ROS启动摄像头" class="headerlink" title="ROS启动摄像头"></a>ROS启动摄像头</h2><h3 id="设置usb-cam节点"><a href="#设置usb-cam节点" class="headerlink" title="设置usb_cam节点"></a>设置usb_cam节点</h3><p>cd ~/catkin_ws/src<br>git clone <a href="https://github.com/bosch-ros-pkg/usb_cam.git">https://github.com/bosch-ros-pkg/usb_cam.git</a> usb_cam<br>cd ..<br>catkin_make</p>
<p>source ./devel/setup.bash</p>
<p>roslaunch usb_cam usb_cam-test.launch<br><img src="pic1.png" alt="1"></p>
<h3 id="编译ORB-SLAM2"><a href="#编译ORB-SLAM2" class="headerlink" title="编译ORB-SLAM2"></a>编译ORB-SLAM2</h3><p>1.将/home/sharon/catkin_ws/src/ORB_SLAM2/Examples/ROS/ORB_SLAM2/src的ros_mono.cc和/home/sharon/catkin_ws/src/ORB_SLAM2/Examples/ROS/ORB_SLAM2/src/AR的ros_mono_ar.cc中的ros::Subscriber sub = nodeHandler.subscribe(“/camera/image_raw”, 1, &amp;ImageGrabber::GrabImage,&amp;igb);<br>改为<br>ros::Subscriber sub = nodeHandler.subscribe(“/usb_cam/image_raw”, 1, &amp;ImageGrabber::GrabImage,&amp;igb);<br>因为ORB_SLAM默认订阅的话题为/camera/image_raw，而usb_cam节点发布的话题为/usb_cam/image_raw<br>2.改好之后再重新编译ORB_SLAM2<br>chmod +x build_ros.sh<br>./build_ros.sh</p>
<h3 id="运行单目摄像头节点"><a href="#运行单目摄像头节点" class="headerlink" title="运行单目摄像头节点"></a>运行单目摄像头节点</h3><p>roslaunch usb_cam usb_cam-test.launch<br>rosrun ORB_SLAM2 Mono /home/sharon/catkin_ws/src/ORB_SLAM2/Vocabulary/ORBvoc.txt /home/sharon/catkin_ws/src/ORB_SLAM2/Examples/Monocular/TUM1.yaml<br><img src="pic2.png" alt="2"></p>
]]></content>
      <categories>
        <category>ROS</category>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>ORB-SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title>ORB-SLAM2错误及解决方法</title>
    <url>/2021/03/08/ORB-SLAM2%E9%94%99%E8%AF%AF%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>在ubantu 14.04上跑ORB-SLAM2，遇到了一些问题记录下来。（后面师兄给我换了ubantu18.08，问题果然就少了很多）</p>
<h2 id="装OpenCV遇到的问题"><a href="#装OpenCV遇到的问题" class="headerlink" title="装OpenCV遇到的问题"></a>装OpenCV遇到的问题</h2><h3 id="1"><a href="#1" class="headerlink" title="1"></a>1</h3><p>问题：make编译时In file included from /home/youyx/Downloads/opencv-3.2.0/modules/core/src/hal_internal.cpp:49:0: /home/youyx/Downloads/opencv-3.2.0/build/opencv_lapack.h:2:45: fatal error: LAPACKE_H_PATHNOTFOUND/lapacke.h: No such file or directory #include “LAPACKE_H_PATH-NOTFOUND/lapacke.h”</p>
<p>方法：</p>
<p>sudo apt-get install liblapacke-dev checkinstall</p>
<p>然后将opencv-3.2.0/build/opencv_lapack.h第二行中的<code>#include&quot;LAPACKE_H_PATH-NOTFOUND/lapacke.h&quot;</code> 改为<code>#include&quot;lapacke.h&quot;</code></p>
<h3 id="2"><a href="#2" class="headerlink" title="2"></a>2</h3><p>问题：make编译时libopencv_highgui.so:undefined reference to `TIFFIsTiled@LIBTIFF_4.0’</p>
<p>原因：OpenCV需要libtiff4库，然而Ubuntu14.04系统安装不会自带libtiff4,因此当以OpenCV为接口时编译可能会出现这个问题。</p>
<p>方法：在cmake 编译OpenCV时</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cmake -D BUILD_TIFF=ON</span><br></pre></td></tr></table></figure>
<p>便会自动编译libtiff4，然后再以OpenCV做接口编译时，Bug消除</p>
<h3 id="3"><a href="#3" class="headerlink" title="3"></a>3</h3><p>问题：../../lib/libopencv_core.so.3.2.0: undefined reference to <code>dpotrf_&#39; ../../lib/libopencv_core.so.3.2.0: undefined reference to</code>dgesv_’<br>../../lib/libopencv_core.so.3.2.0: undefined reference to <code>sgels_&#39; ../../lib/libopencv_core.so.3.2.0: undefined reference to</code>sgesv_’<br>../../lib/libopencv_core.so.3.2.0: undefined reference to <code>sposv_&#39; ../../lib/libopencv_core.so.3.2.0: undefined reference to</code>dgetrf_’<br>../../lib/libopencv_core.so.3.2.0: undefined reference to <code>sgetrf_&#39; ../../lib/libopencv_core.so.3.2.0: undefined reference to</code>dgels_’<br>../../lib/libopencv_core.so.3.2.0: undefined reference to <code>dgeqrf_&#39; ../../lib/libopencv_core.so.3.2.0: undefined reference to</code>spotrf_’<br>../../lib/libopencv_core.so.3.2.0: undefined reference to <code>sgeqrf_&#39; ../../lib/libopencv_core.so.3.2.0: undefined reference to</code>sgesdd_’<br>../../lib/libopencv_core.so.3.2.0: undefined reference to <code>dgesdd_&#39; ../../lib/libopencv_core.so.3.2.0: undefined reference to</code>dposv_’</p>
<p>方法：在cmake 编译OpenCV时</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">-D WITH_LAPACK=OFF</span><br></pre></td></tr></table></figure>
<h3 id="4"><a href="#4" class="headerlink" title="4"></a>4</h3><p>问题：/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>PKCS12_PBE_add@OPENSSL_1.0.0&#39;                     
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>OCSP_basic_verify@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>BIO_puts@OPENSSL_1.0.0&#39;                           
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>SSL_get_peer_certificate@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>BIO_free@OPENSSL_1.0.0&#39;                           
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>SSLv3_client_method@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>ENGINE_get_id@OPENSSL_1.0.0&#39;                      
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>RAND_status@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>MD5_Final@OPENSSL_1.0.0&#39;                          
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>SSL_CTX_set_verify@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>ASN1_TIME_print@OPENSSL_1.0.0&#39;                    
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>ENGINE_ctrl@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>CONF_modules_free@OPENSSL_1.0.0&#39;                  
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>DES_set_key@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>SSL_set_session@OPENSSL_1.0.0&#39;                    
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>X509_EXTENSION_get_data@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>ERR_error_string_n@OPENSSL_1.0.0&#39;                 
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>OCSP_cert_status_str@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>SSL_CTX_free@OPENSSL_1.0.0&#39;                       
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>X509_check_issued@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>SSL_CTX_set_default_passwd_cb_userdata@OPENSSL_1.0
.0&#39;                                                                                                             
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>OCSP_RESPONSE_free@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>X509_get_pubkey@OPENSSL_1.0.0&#39;                    
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>CRYPTO_malloc@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>SSL_CTX_use_certificate_chain_file@OPENSSL_1.0.0&#39; 
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>SSLeay@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>UI_method_get_opener@OPENSSL_1.0.0&#39;               
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>X509_load_crl_file@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>ENGINE_free@OPENSSL_1.0.0&#39;                        
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>ASN1_STRING_type@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>ASN1_STRING_data@OPENSSL_1.0.0&#39;                   
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>i2t_ASN1_OBJECT@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>SSL_get_error@OPENSSL_1.0.0&#39;                      
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>X509_NAME_get_entry@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>SSL_CTX_add_client_CA@OPENSSL_1.0.0&#39;              
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>SSL_get_privatekey@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>OPENSSL_load_builtin_modules@OPENSSL_1.0.0&#39;       
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>CRYPTO_free@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>RAND_add@OPENSSL_1.0.0&#39;                           
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>ASN1_STRING_length@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>SSL_CIPHER_get_name@OPENSSL_1.0.0&#39;                
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>ERR_peek_error@OPENSSL_1.0.0’                     </p>
<p>方法：anaconda的库和系统默认的库冲突了，我暴力地把anaconda的文件名改了哈哈哈哈哈哈哈哈哈哈哈哈</p>
<h3 id="5-ubantu18-04"><a href="#5-ubantu18-04" class="headerlink" title="5(ubantu18.04)"></a>5(ubantu18.04)</h3><p>问题：error: ‘usleep’ was not declared in this scope<br>             usleep(3000);</p>
<p>方法：在/include/System.h文件中添加#include<unistd.h></p>
<h3 id="6"><a href="#6" class="headerlink" title="6"></a>6</h3><p>问题：build_ros.sh遇到undefined reference to symbol ‘_ZN5boost6system15system_categoryEv’<br>方法：在Examples/ROS/ORB_SLAM2/文件夹下的CMakeLists.txt文件中<br>set(LIBS<br>${OpenCV_LIBS}<br>${EIGEN3_LIBS}<br>${Pangolin_LIBRARIES}<br>${PROJECT_SOURCE_DIR}/../../../Thirdparty/DBoW2/lib/libDBoW2.so<br>${PROJECT_SOURCE_DIR}/../../../Thirdparty/g2o/lib/libg2o.so<br>${PROJECT_SOURCE_DIR}/../../../lib/libORB_SLAM2.so<br>改为<br>set(LIBS<br>${OpenCV_LIBS}<br>${EIGEN3_LIBS}<br>${Pangolin_LIBRARIES}<br>${PROJECT_SOURCE_DIR}/../../../Thirdparty/DBoW2/lib/libDBoW2.so<br>${PROJECT_SOURCE_DIR}/../../../Thirdparty/g2o/lib/libg2o.so<br>${PROJECT_SOURCE_DIR}/../../../lib/libORB_SLAM2.so<br>-lboost_system<br>)</p>
<h3 id="7"><a href="#7" class="headerlink" title="7"></a>7</h3><p>问题：[ERROR] [1571614395.918484166]: [registerPublisher] Failed to contact master at [localhost:11311]<br>方法：ros服务没开，再开一个终端输入<br>roscore</p>
<h3 id="8"><a href="#8" class="headerlink" title="8"></a>8</h3><p>ORB-SLAM2编译报错：rospack found package “ORB_SLAM2” at “/opt/ros/kinetic/share/ORB_SLAM2”, but the….</p>
<p><a href="https://blog.csdn.net/weixin_44401286/article/details/102752767">https://blog.csdn.net/weixin_44401286/article/details/102752767</a></p>
<p>“原因分析及解决办法：<br>这是由于在ROS环境下编译使用了多个版本的ORB-SLAM2工程造成的，比如我运行了原版的ORB-SLAM2，然后拷贝了一份改了下文件名字，添加稠密重建模块，再对新的工程编译，就报了这个错误。因为原工程已经在/opt/ros/kinetic/share文件夹下建立了一个软连接ORB_SLAM2，这里包含有原工程的一些信息，我们需要做的就是把该软连接，替换为现在工程的软连接。”<br>(1)先cd到/opt/ros/kinetic/share目录下，删除原来的ORB_SLAM2软连接</p>
<p>sudo rm -r ORB_SLAM2</p>
<p>(2)对当前工程在/opt/ros/kinetic/share目录下建立软连接</p>
<p>sudo ln -s /home/bruce/study/slam/orb/point_map/Examples/ROS/ORB_SLAM2 /opt/ros/kinetic/share/ORB_SLAM2</p>
<h3 id="9"><a href="#9" class="headerlink" title="9"></a>9</h3><p>问题：catkin_make的时候’libavcodec’没找到</p>
<p>Checking for module ‘libavcodec’<br>—   No package ‘libavcodec’ found<br>CMake Error at /usr/local/share/cmake-3.7/Modules/FindPkgConfig.cmake:415 (message):<br>  A required package was not found<br>Call Stack (most recent call first):<br>  /usr/local/share/cmake-3.7/Modules/FindPkgConfig.cmake:588 (_pkg_check_modules_internal)<br>  usb_cam/CMakeLists.txt:11 (pkg_check_modules)</p>
<p>方法：1.sudo apt install libavcodec-dev</p>
<p>如果1没用，那可能是它不在PKG_CONFIG_PATH，所以把它加进来。</p>
<p>echo $PKG_CONFIG_PATH</p>
<p>果然不在，那么</p>
<p>export PKG_CONFIG_PATH=/usr/lib/x86_64-linux-gnu/pkgconfig/:$PKG_CONFIG_PATH</p>
<h3 id="10"><a href="#10" class="headerlink" title="10"></a>10</h3><p>若在CMake中遇到该nullptr问题：</p>
<p>要在CMake中使用C++11，只要在CMakeLists.txt中添加一行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">add_definitions(-std&#x3D;c++11)</span><br></pre></td></tr></table></figure>
<p>OR</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SET(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std&#x3D;c++0x&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="11"><a href="#11" class="headerlink" title="11"></a>11</h3><p>运行 # rosrun robot_sim_demo robot_keyboard_teleop.py 时出现错误：</p>
<p>[rosrun] Found the following, but they’re either not files, [rosrun] or not</p>
<p>这是因为robot_keyboard_teleop.py权限不足</p>
<p>chmod +x /home/youyx/catkin_ws/src/ROS-Academy-for-Beginners/robot_sim_demo/scripts/robot_keyboard_teleop.py</p>
]]></content>
      <categories>
        <category>ROS</category>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>ORB-SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title>【转载】三维数据集整理</title>
    <url>/2021/03/02/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E4%B8%89%E7%BB%B4%E6%95%B0%E6%8D%AE%E9%9B%86%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>毕设需要用到视频序列的三维数据集，刚好看到一篇文章做了整理，就转了过来。</p>
<p>《Review: deep learning on 3D point clouds》<a href="https://arxiv.org/pdf/2001.06280.pdf">https://arxiv.org/pdf/2001.06280.pdf</a></p>
<h2 id="基准数据集"><a href="#基准数据集" class="headerlink" title="基准数据集"></a>基准数据集</h2><p>  近年来，已经发表了大量的点云数据集。现有的大部分数据集都是由大学和行业提供的。它们可以为测试各种方法提供一个公平的比较。这些公共基准数据集由虚拟场景或真实场景组成，其重点在于特别是在点云分类、分割、检索方面和目标检测。它们用深度学习方法特别有用，因为它们可以为训练网络提供大量的真实标签。点云可以通过不同的平台/方法获得，如Structure from Motion (SfM)、红绿蓝-深度(RGB-D)相机和光探测和测距（LiDAR）系统。随着大小和复杂程度增加，基准数据集的可用性通常会降低。在本节中，我们将介绍一些流行的用于3D研究的数据集。</p>
<h3 id="3D模型数据集"><a href="#3D模型数据集" class="headerlink" title="3D模型数据集"></a>3D模型数据集</h3><p><strong>ModelNet [13]：</strong>该数据集由普林斯顿大学视觉与机器人实验室开发。ModelNet40有40个人造物体形状的类别（如飞机、书架和椅子）用于分类和识别。它由12311个CAD模型组成。其中分为9,843个训练和2,468个测试形状。ModelNet10数据集是ModelNet40的一个子集，该子集包括只包含10个类别。它还分为3991个训练和908个测试形状。</p>
<p><strong>ShapeNet [48]：</strong>大规模数据集是由斯坦福大学等人开发，它提供了每个模型的语义类别标签，固定的走线、部件和双边对称性平面、物理尺寸、关键词，以及其他计划中的注解。ShapeNet已经为近3,000,000个模型编制了索引。当数据集公布后，有22万个模型被分为3135个类别。ShapeNetCore是ShapeNet的一个子集，其中包括近51,300个独特的3D模型。它提供了55个常见的对象类别和注释。ShapeNetSem也是ShapeNet的一个子集，它包含有12,000个模型。它的规模更小，但覆盖面更广，包括270类。</p>
<p><strong>Augmenting ShapeNet：</strong>[49]创建了详细的部件标签为来自ShapeNetCore数据集的31963个模型提供服务。它提供了16个形状类别进行部件分割。[50]已经提供了1200个来自ShapeNet数据集的虚拟局部模型。[51]提出了一种自动生成逼真3D形状的方法，它是建立在ShapeNetCore数据集。[52]是一个具有细粒度和层次性的部件注释的大规模数据集，它包括24个对象类别和26,671个三维模型，提供了573,585个部件实例标签。[53]贡献了一个大规模的3D物体识别数据集。该数据集有100个类别，其中包括有201,888个对象（来自ImageNet[54]）的90,127张图像和44,147个3D形状（来自ShapeNet）。</p>
<p><strong>Shape2Motion [55]：</strong>Shape2Motion是由北航和国防科技大学开发的。它已经创建了一个新的基准数据集，用于3D形状流动性分析。该基准包括45个形状类别与2440个模型，其中的形状是来自于ShapeNet和3D仓库[56]。所提出的方法输入单一的3D形状，然后预测运动部件的分割结果和运动对应的共同属性。</p>
<p><strong>ScanObjectNN [57]：</strong>ScanObjectNN是由香港科技大学等单位提出，是第一个点云分类的真实世界数据集。约15,000对象是从室内数据集选择出的（SceneNN[58]和ScanNet[30]）。 并将对象分为15类，其中有2902个唯一的对象实例。</p>
<h3 id="3D室内数据集"><a href="#3D室内数据集" class="headerlink" title="3D室内数据集"></a>3D室内数据集</h3><p><strong>NYUDv2 [59]：</strong>纽约大学深度数据集v2（NYUDv2）是由纽约大学等人开发的。该数据集提供了从464个各种室内场景中捕捉到的1449张RGB-D（由Kinect v1获得）图像。所有的图像是分布式分割标签。这个数据集主要是有助于了解3D线索对于室内物体如何产生更好的分割。</p>
<p><strong>SUN3D [60]：</strong>该数据集由普林斯顿大学开发。这是一个RGB-D视频数据集，其中的视频是从41栋建筑的254个不同空间中捕捉到的。SUN3D提供了415个带有摄像机姿势和物体标签的序列。点云是由运动结构(SfM)生成的。</p>
<p><strong>S3DIS [61]：</strong>斯坦福大学3D大型室内空间（S3DIS）是由斯坦福大学等人开发的，S3DIS是从3栋不同的建筑物271个房间中收集到的数据。覆盖面积在6000平方米以上。它包含超过2.15亿点，并且每个点都提供了实例级语义分割标签（13个类别）；</p>
<p><strong>SceneNN [58]：</strong>新加坡科技与设计大学等人开发了这个数据集。SceneNN是一个RGB-D（获得Kinect v2)场景数据集，它收集了101个室内场景的数据。<br>  它为室内场景提供了40个语义类，并且所有的语义标签与NYUDv2数据集相同。</p>
<p><strong>ScanNet [30]：</strong>ScanNet是一个大尺度的室内数据集，它的开发目的是为了让人们能够更清楚地了解自己的生活。由斯坦福大学等人拍摄，包含1513个扫描场景。包括近2.5M的RGB-D（由 Occipital Structure公司获得传感器）图像，来自707个不同的室内环境。该数据集为三维物体分类提供了地面真实标签。17个类别，语义分割有20个类别类别：<br>  对于对象分类，ScanNet将所有的实例划分为9,677个实例用于训练，2,606个实例用于测试。而且ScanNet将所有扫描分成1201个训练场景和312个测试场景进行语义分割。</p>
<p><strong>Matterport3D [62]：</strong>Matterport3D是普林斯顿大学等人开发的最大的室内数据集。的。该数据集的覆盖面积面积为来自2056个房间的219,399mm2，，建筑面积为46,561mm2。它包括10,800个全景视图，其中视图来自90大型建筑的194,400张RGB-D图像。标签包含表面重建、摄像机姿势和语义分割。这个数据集研究现场理解的5个任务，分别是关键点匹配、视图重叠预测、表面法线估计、区域类型分类，以及语义分割。</p>
<p><strong>3DMatch [63]：</strong>这个基准数据集是由普林斯顿大学等，它是现有数据集的一个大集合。，如Analysisby-Synthesis[64]、7-cenes[65]。SUN3D[60]、RGB-D Scenes v.2[66]和Halber等人[67]。3DMatch基准由62个场景组成，分别为54个训练场景和8个测试场景。它利用对应标签从RGB-D场景重建数据集，然后提供点云检索的地面真相标签。</p>
<p><strong>Multisensor Indoor Mapping and Positioning Dataset [68]：</strong>这个室内数据集（房间、走廊和室内停车场）是由厦门大学等人开发的。由多传感器获得，如激光扫描仪、摄像头、WIFI、蓝牙和IMU。该数据集提供了密集的激光扫描点云进行室内测绘和定位。同时。他们还提供基于多传感器校准的彩色激光扫描和SLAM映射过程。</p>
<h3 id="3D室外数据集"><a href="#3D室外数据集" class="headerlink" title="3D室外数据集"></a>3D室外数据集</h3><p><strong>KITTI [69] [70]：</strong>KITTI数据集是在自动驾驶领域最著名的数据之一。它是由卡尔斯鲁厄理工学院等开发的，可用于立体图像、光流估计、三维检测的研究、三维跟踪、视觉测距等。数据采集平台配备了两台彩色摄像机，两台灰度相机，一台Velodyne HDL-64E 3D激光扫描仪和一个高精度的GPS/IMU系统。KITTI提供原始数据有道路、城市、住宅、校园和人等五类。深度完成和预测基准包括93000多张深度图。3D物体检测基准包含7481个训练点云和7518个测试点云。视觉测距基准由22个序列组成，有11个序列（00-10）激光雷达数据进行训练和11序列（11-21）激光雷达数据进行测试。同时，最近发表了Kitti里程数据集的语义标注[71]。SemanticKITTI包含28个类，包括地、建筑、车辆、自然、人、物等。</p>
<p><strong>ASL Dataset [72]：</strong>这组数据集是由苏黎世联邦理工学院收集于2011年8月之间至2012年1月。它提供了由北洋UTM-30LX获得的8个点云序列。每个序列约有35次扫描点云和由GPS/INS系统支持的真实姿势。该数据集涵盖了结构化和非结构化环境的领域。<br>iQmulus [73]：由Mines ParisTech等于2013年1月开发了大规模城市场景数据集。整个3D点云已被分类并划分为50个类。数据是由StereopolisII MLS收集的，该系统是法国国家测绘局（IGN）开发的一个系统。他们使用Riegl LMS-Q120i传感器采集3亿个点。</p>
<p><strong>Oxford Robotcar [74]：</strong>这个数据集是由牛津大学开发的。它由2014年5月至2015年12月期间穿过牛津市中心的大约100次轨迹组成(共101,046公里的轨迹)。这一长期数据集捕捉到了许多具有挑战性的环境变化，包括季节、天气、交通等等。而数据集既提供了图像、激光雷达点云、全球定位系统和用于自动汽车的INS地面实况。LIDRA的数据是由两个SICK LMS-151 2D激光雷达扫描仪和一台SICK LD-MRS 3D激光雷达扫描仪获得。<br>NCLT [75]：它是由密歇根大学开发的。它含有27次于2012年1月至2013年4月期间通过密歇根大学北校区的轨迹。该数据集还提供了图像、激光雷达、全球定位系统和用于长期自动汽车的INS地面数据。LiDRA点云是由Velodyne-32激光雷达扫描仪收集的。</p>
<p><strong>Semantic3D [76]：</strong>由苏黎世联邦理工学院开发了高质量和高密度的数据集。它包含了超过40亿的采集点云的点位，通过静态地面观测获得。激光扫描仪提供了8个语义类，其中由人工地形、自然地形、高植被、低植被、建筑物、硬地貌、扫描文物和汽车等组成。而数据集被分为15个训练场景和15个测试场景。</p>
<p><strong>DBNet [77]：</strong>这个真实世界的LiDAR-视频数据集是由厦门大学等单位开发的，旨在学习驾驶策略，因此它与以往的户外数据集不同。DBNet提供激光雷达点云、视频记录、GPS和用于进行驾驶行为研究的驾驶员行为。它包含了1,000公里的被Velodyne激光器采集的驾驶数据。</p>
<p><strong>NPM3D [78]：</strong> NPM3D数据集是由PSL研究大学开发的。它是一个点云分类和分割的基准。所有的点云都被标记为50个不同的类别。它包含了在巴黎和里尔收集的1431万个点数据。该数据是由包括Velodyne HDL-32E激光雷达和GPS/INS系统的移动激光系统采集的。</p>
<p><strong>Apollo [79] [80]：</strong>Apollo是由百度研究开发的。它是一个大规模的自动驾驶数据集。它提供三维汽车实例理解的标签数据、激光雷达点云物体的检测和跟踪，以及基于激光雷达的定位。对于3D汽车实例理解任务，有5277张图片，6万多辆汽车实例。每辆车都有一个工业级的CAD模型。3D物体检测和跟踪基准数据集包含53分钟的训练序列和50分钟测试序列。它是在帧率为10fps/秒，标注的帧率为2fps/秒的情况下采集的。阿波罗-南湾数据集（Apollo-SouthBay dataset）提供了关于定位任务的激光雷达帧数据。它是在旧金山湾南部采集的。他们在标准林肯MKZ轿车上配备了高端的自动驾驶传感器装备(Velodyne HDL-64E、NovAtel ProPak6和IMU-ISA-100C)。</p>
<p><strong>nuScenes [81]：</strong>nuTonomy场景(nuScenes)数据集提出了一种新的三维物体检测指标，它是由nuTonomy（APTIV公司）提供。该指标由多个方面组成，分别是分类、速度、大小、定位、方向，以及对象的属性估计。这数据集是由自主车辆传感器装备在360度视野下获取的（ 6个摄像机、5个雷达和1个激光雷达）。它包含从波士顿和新加坡收集的1000个驾驶场景。其中，两座城市都是交通堵塞。在这个数据集中的对象有23个类和8个属性，它们都是标有三维边界框。</p>
<p><strong>BLVD [82]：</strong>该数据集由西安交通大学开发。并被收藏在常熟（中国）。它介绍了一个新的基准，它专注于动态4D对象跟踪、5D交互式事件识别和5D意图预测。BLVD数据集由654个视频片段组成，其中视频为120k帧，帧率为10fps/秒。所有的帧都是被注释，获得了249129个3D注释。有总共4 902个特定的跟踪对象，6 004个交互式事件识别片段，以及4900个对象用于意图预测。</p>
<p><img src="pic1.png" alt="1"></p>
<p>表1：基准数据集的分类（cls:分类，seg:分割，loc:定位，reg:配准，aut:自动驾驶，det:目标检测，dri:驾驶行为，mot:运动估计，odo:里程计）</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[13] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, J. Xiao, 3d shapenets: A deep representation for volumetric shapes, in: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, IEEE Computer Society, 2015, pp. 1912–1920. URL: <a href="https://doi.org/10.1109/CVPR.2015.7298801">https://doi.org/10.1109/CVPR.2015.7298801</a>. doi:10.1109/CVPR. 2015.7298801.</p>
<p>[30] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, M. Nießner, Scannet: Richly-annotated 3d reconstructions of indoor scenes, in: Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017.</p>
<p>[48] A. X. Chang, T. A. Funkhouser, L. J. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, F. Yu, Shapenet: An informationrich 3d model repository, CoRR abs/1512.03012 (2015). URL: <a href="http://arxiv.org/abs/1512.03012">http://arxiv.org/abs/1512.03012</a>. arXiv:1512.03012.</p>
<p>[49] L. Yi, V. G. Kim, D. Ceylan, I. Shen, M. Yan, H. Su, C. Lu, Q. Huang, A. Sheffer, L. Guibas, et al., A scalable active framework for region annotation in 3d shape collections, ACM Transactions on Graphics (TOG) 35 (2016) 210.</p>
<p>[50] A. Dai, C. R. Qi, M. Nießner, Shape completion using 3d-encoder-predictor cnns and shape synthesis, in: Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017.</p>
<p>[51] K. Park, K. Rematas, A. Farhadi, S. M. Seitz, Photoshape: Photorealistic materials for large-scale shape collections, ACM Trans. Graph. 37 (2018).</p>
<p>[52] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J. Guibas, H. Su, PartNet: A large-scale benchmark for fine-grained and hierarchical part-level 3D object understanding, in: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.<br>[53] Y. Xiang, W. Kim, W. Chen, J. Ji, C. Choy, H. Su, R. Mottaghi, L. Guibas, S. Savarese, Objectnet3d: A large scale database for 3d object recognition, in: European Conference Computer Vision (ECCV), 2016.</p>
<p>[54] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A large-scale hierarchical image database, in: 2009 IEEE conference on computer vision and pattern recognition, Ieee, 2009, pp. 248–255.</p>
<p>[55] X. Wang, B. Zhou, Y. Shi, X. Chen, Q. Zhao, K. Xu, Shape2motion: Joint analysis of motion parts and attributes from 3d shapes, in: CVPR, 2019, p. to appear.</p>
<p>[56] 3d warehouse, ???? URL: <a href="https://3dwarehouse.sketchup.com/">https://3dwarehouse.sketchup.com/</a>.</p>
<p>[57] M. A. Uy, Q.-H. Pham, B.-S. Hua, D. T. Nguyen, S.K. Yeung, Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data, in: International Conference on Computer Vision (ICCV), 2019.</p>
<p>[58] B.-S. Hua, Q.-H. Pham, D. T. Nguyen, M.-K. Tran, L.-F. Yu, S.-K. Yeung, Scenenn: A scene meshes dataset with annotations, in: International Conference on 3D Vision (3DV), 2016.</p>
<p>[59] N. Silberman, D. Hoiem, P. Kohli, R. Fergus, Indoor segmentation and support inference from rgbd images, in: European Conference on Computer Vision, Springer, 2012, pp. 746–760.</p>
<p>[60] J. Xiao, A. Owens, A. Torralba, Sun3d: A database of big spaces reconstructed using sfm and object labels, in: Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 1625–1632.</p>
<p>[61] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. K. Brilakis, M. Fischer, S. Savarese, 3d semantic parsing of large-scale indoor spaces, in: 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, IEEE Computer Society, 2016, pp. 1534–1543. URL: <a href="https://doi.org/10.1109/CVPR.2016.170">https://doi.org/10.1109/CVPR.2016.170</a>. doi:10.1109/CVPR.2016. 170.</p>
<p>[62] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, Y. Zhang, Matter port3d: Learning from rgb-d data in indoor environments, International Conference on 3D Vision (3DV) (2017).</p>
<p>[63] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, T. Funkhouser, 3dmatch: Learning local geometric descriptors from rgb-d reconstructions, in: CVPR, 2017.</p>
<p>[64] J. Valentin, A. Dai, M. Nießner, P. Kohli, P. Torr, S. Izadi, C. Keskin, Learning to navigate the energy landscape, arXiv preprint arXiv:1603.05772 (2016).</p>
<p>[65] J. Shotton, B. Glocker, C. Zach, S. Izadi, A. Criminisi, A. Fitzgibbon, Scene coordinate regression forests for camera relocalization in rgb-d images, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 2930–2937.</p>
<p>[66] M. De Deuge, A. Quadros, C. Hung, B. Douillard, Unsupervised feature learning for classification of outdoor 3d scans, in: Australasian Conference on Robitics and Automation, volume 2, 2013, p. 1.</p>
<p>[67] M. Halber, T. A. Funkhouser, Structured global registration of rgb-d scans in indoor environments, ArXiv abs/1607.08539 (2016).</p>
<p>[68] C. Wang, S. Hou, C. Wen, Z. Gong, Q. Li, X. Sun, J. Li, Semantic line framework-based indoor building modeling using backpacked laser scanning point cloud, ISPRS journal of photogrammetry and remote sensing 143 (2018) 150–166.</p>
<p>[69] A. Geiger, P. Lenz, R. Urtasun, Are we ready for autonomous driving? the kitti vision benchmark suite, in: Conference on Computer Vision and Pattern Recognition (CVPR), 2012.</p>
<p>[70] A. Geiger, P. Lenz, C. Stiller, R. Urtasun, Vision meets robotics: The kitti dataset, International Journal of Robotics Research (IJRR) (2013).</p>
<p>[71] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, J. Gall, SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences, in: Proc. of the IEEE/CVF International Conf. on Computer Vision (ICCV), 2019.</p>
<p>[72] F. Pomerleau, M. Liu, F. Colas, R. Siegwart, Challenging data sets for point cloud registration algorithms, The International Journal of Robotics Research 31 (2012) 1705–1711.</p>
<p>[73] M. Br´edif, B. Vallet, A. Serna, B. Marcotegui, N. Paparoditis, Terramobilita/iqmulus urban point cloud classification benchmark, 2014.</p>
<p>[74] W. Maddern, G. Pascoe, C. Linegar, P. Newman, 1 Year, 1000km: The Oxford RobotCar<br>Dataset, The International Journal of Robotics Research (IJRR) 36 (2017) 3–15. URL: <a href="http://dx.doi.org/10.1177/0278364916679498">http://dx.doi.org/10.1177/0278364916679498</a>. doi:10.1177/0278364916679498.</p>
<p>[75] N. Carlevaris-Bianco, A. K. Ushani, R. M. Eustice, University of michigan north campus long-term vision and lidar dataset, The International Journal of Robotics Research 35 (2016) 1023–1035.</p>
<p>[76] T. Hackel, N. Savinov, L. Ladicky, J. D. Wegner, K. Schindler, M. Pollefeys, Semantic3d. net: A new large-scale point cloud classification benchmark, arXiv preprint arXiv:1704.03847 (2017).</p>
<p>[77] Y. Chen, J. Wang, J. Li, C. Lu, Z. Luo, H. Xue, C. Wang, Lidar-video driving dataset: Learning driving policies effectively, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 5870–5878.</p>
<p>[78] X. Roynard, J.-E. Deschaud, F. Goulette, Paris-lille-3d: A large and high-quality ground-truth urban point cloud dataset for automatic segmentation and classi-fication, The International Journal of Robotics Research 37 (2018) 545–557. URL: <a href="https://doi.org/10.1177/0278364918767506">https://doi.org/10.1177/0278364918767506</a>. doi:10.1177/0278364918767506.</p>
<p>[79] X. Song, P. Wang, D. Zhou, R. Zhu, C. Guan, Y. Dai, H. Su, H. Li, R. Yang, Apollocar3d: A large 3d car instance understanding benchmark for autonomous driving, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 5452–5462.</p>
<p>[80] W. Lu, Y. Zhou, G. Wan, S. Hou, S. Song, L3-net: Towards learning based lidar localization for autonomous driving, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp.6389–6398.</p>
<p>[81] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, O. Beijbom, nuscenes: A multimodal dataset for autonomous driving, arXiv preprint arXiv:1903.11027 (2019).</p>
<p>[82] J. Xue, J. Fang, T. Li, B. Zhang, P. Zhang, Z. Ye, J. Dou, BLVD: Building a large-scale 5d semantics benchmark for autonomous driving, in: Proc. International Conference on Robotics and Automation, in press, 2019.</p>
]]></content>
      <categories>
        <category>数据集</category>
      </categories>
      <tags>
        <tag>实例分割</tag>
        <tag>深度学习</tag>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title>Gazebo仿真场景搭建+配置</title>
    <url>/2021/01/31/Gazebo%E4%BB%BF%E7%9C%9F%E5%9C%BA%E6%99%AF%E6%90%AD%E5%BB%BA+%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h2 id="搭建场景"><a href="#搭建场景" class="headerlink" title="搭建场景"></a>搭建场景</h2><h3 id="打开Gazebo"><a href="#打开Gazebo" class="headerlink" title="打开Gazebo"></a>打开Gazebo</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">gazebo</span><br></pre></td></tr></table></figure>
<h3 id="打开建筑编辑器"><a href="#打开建筑编辑器" class="headerlink" title="打开建筑编辑器"></a>打开建筑编辑器</h3><p>点击“Edit”-&gt;“Building Editor”或者使用快捷键“ctrl+B”</p>
<p><img src="pic1.png" alt="pic1"></p>
<h3 id="图形界面"><a href="#图形界面" class="headerlink" title="图形界面"></a>图形界面</h3><ul>
<li>左栏可以选择建筑材料和特征</li>
<li>上方的界面是二维视图，导入的floor plan可以在这里看到</li>
<li>下方的界面是三维视图，能够预览建筑</li>
</ul>
<h3 id="导入floor-plan"><a href="#导入floor-plan" class="headerlink" title="导入floor plan"></a>导入floor plan</h3><p>导入一个建筑模板</p>
<p>1.点击左栏的“import”</p>
<p>2.选择电脑中的一张floor plan图片</p>
<p><img src="pic2.png" alt="pic2"></p>
<p>3.设置尺度</p>
<p><img src="D:\yyx\Hexo-Blog\source\_posts\Gazebo仿真场景搭建+配置\pic3.png" alt="pic3"></p>
<p>在图片中标记一段直线，并在左边输入其实际长度</p>
<p>4.图片出现在二维界面中</p>
<h3 id="添加墙、窗户和门"><a href="#添加墙、窗户和门" class="headerlink" title="添加墙、窗户和门"></a>添加墙、窗户和门</h3><p><img src="pic5.png" alt="pic5"></p>
<p>添加窗户和门的时候有个bug，只能添加横向的窗和门，若添加纵向的gazebo就会闪退，关闭硬件加速或是升级gazebo都没解决。</p>
<h3 id="编辑建筑"><a href="#编辑建筑" class="headerlink" title="编辑建筑"></a>编辑建筑</h3><p>双击墙、窗户和门，或者右键选择，即出现参数框</p>
<p><img src="pic6.png" alt="pic6"></p>
<p>可以调整位置、大小，还可以设置墙的颜色和纹理</p>
<h3 id="保存建筑"><a href="#保存建筑" class="headerlink" title="保存建筑"></a>保存建筑</h3><p>保存会创建一个建筑的目录，SDF和配置文件。</p>
<p>点击“File”-&gt;“Save”（ctrl+s）。注意：保存的位置要在gazebo/models下</p>
<p><img src="pic7.png" alt="pic7"></p>
<p>保存后就可以退出，一旦退出这个建筑就不能再编辑，只能往里面插入模型。</p>
<p><img src="pic8.png" alt="pic8"></p>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>在/usr/share/gazebo-9/worlds目录下创建house.world文件，添加以下代码</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; ?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">sdf</span> <span class="attr">version</span>=<span class="string">&quot;1.5&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">world</span> <span class="attr">name</span>=<span class="string">&quot;default&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">include</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">uri</span>&gt;</span>model://ground_plane<span class="tag">&lt;/<span class="name">uri</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">include</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">uri</span>&gt;</span>model://sun<span class="tag">&lt;/<span class="name">uri</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">include</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">uri</span>&gt;</span>model://house<span class="tag">&lt;/<span class="name">uri</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">world</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">sdf</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>在/opt/ros/melodic/share/gazebo_ros/launch目录下创建house.launch文件，添加以下代码</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">launch</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- We resume the logic in empty_world.launch, changing only the name of the world to be launched --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">include</span> <span class="attr">file</span>=<span class="string">&quot;$(find gazebo_ros)/launch/empty_world.launch&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;world_name&quot;</span> <span class="attr">value</span>=<span class="string">&quot;worlds/house.world&quot;</span>/&gt;</span> <span class="comment">&lt;!-- <span class="doctag">Note:</span> the world_name is with respect to GAZEBO_RESOURCE_PATH environmental variable --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;paused&quot;</span> <span class="attr">value</span>=<span class="string">&quot;false&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;use_sim_time&quot;</span> <span class="attr">value</span>=<span class="string">&quot;true&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;gui&quot;</span> <span class="attr">value</span>=<span class="string">&quot;true&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;headless&quot;</span> <span class="attr">value</span>=<span class="string">&quot;false&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;debug&quot;</span> <span class="attr">value</span>=<span class="string">&quot;false&quot;</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">launch</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>在命令行中输入</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">roslaunch gazebo_ros house.launch</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>gazebo</category>
      </categories>
      <tags>
        <tag>gazebo</tag>
        <tag>ROS</tag>
        <tag>仿真</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLACT</title>
    <url>/2021/01/30/YOLACT/</url>
    <content><![CDATA[<h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>因为项目的需要，就试着跑了YOLACT，把过程记录在这儿。</p>
<p><strong>YOLACT：</strong><a href="https://github.com/dbolya/yolact">https://github.com/dbolya/yolact</a> </p>
<h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><h3 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a>创建虚拟环境</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda create -n yolact python=3.7</span><br><span class="line"><span class="built_in">source</span> activate</span><br><span class="line">conda activate yolact</span><br></pre></td></tr></table></figure>
<ul>
<li>用conda创建一个yolact虚拟环境，这样可以搭建独立的python运行环境，使的每个项目的运行互不影响；</li>
<li>激活该虚拟环境。</li>
<li>add|新电脑上source ~/anaconda3/bin/activate</li>
</ul>
<h3 id="安装pytorch和其他的包"><a href="#安装pytorch和其他的包" class="headerlink" title="安装pytorch和其他的包"></a>安装pytorch和其他的包</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda install pytorch torchvision cudatoolkit=10.0 </span><br><span class="line">pip install cython opencv-python pillow  matplotlib </span><br></pre></td></tr></table></figure>
<ul>
<li>cudatoolkit=x.x取决cuda版本，因为我用的是师兄的服务器，cuda是10.0的版本，所以cudatoolkit=10.0；</li>
<li>conda install的好处是能够根据cudatoolkit的版本安装相对应版本的pytorch和torchvision，不用我们自己匹配。</li>
</ul>
<h3 id="安装COCOAPI"><a href="#安装COCOAPI" class="headerlink" title="安装COCOAPI"></a>安装COCOAPI</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> cocoapi/PythonAPI</span><br><span class="line">python setup.py build_ext install</span><br></pre></td></tr></table></figure>
<p>【注意】setup.py文件里第12行的</p>
<p>​        extra_compile_args={‘gcc’: [‘/Qstd=c99’]},</p>
<p>  改为<br>        extra_compile_args=[‘-std=c99’],</p>
<h3 id="编译可变性卷积层（如果要使用YOLACT-）"><a href="#编译可变性卷积层（如果要使用YOLACT-）" class="headerlink" title="*编译可变性卷积层（如果要使用YOLACT++）"></a>*编译可变性卷积层（如果要使用YOLACT++）</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ../../yolact-master/external/DCNv2</span><br><span class="line">python setup.py build develop</span><br></pre></td></tr></table></figure>
<h2 id="运行YOLACT"><a href="#运行YOLACT" class="headerlink" title="运行YOLACT"></a>运行YOLACT</h2><h3 id="GPU设置"><a href="#GPU设置" class="headerlink" title="GPU设置"></a>GPU设置</h3><p>YOLACT源码默认使用多GPU，而我只用了一个GPU，所以在运行前要设置一下。</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> CUDA_VISIBLE_DEVICES=[gpus]</span><br></pre></td></tr></table></figure>
<p>比如我用的是2号GPU，所以命令就是</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> CUDA_VISIBLE_DEVICES=2</span><br></pre></td></tr></table></figure>
<p>add|换了新电脑就可以用多GPUS啦!</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> CUDA_VISIBLE_DEVICES=0,1,2,3</span><br></pre></td></tr></table></figure>
<h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>运行前把github上的<a href="https://drive.google.com/file/d/15id0Qq5eqRbkD-N3ZjDZXdCvRyIaHpFB/view?usp=sharing">yolact_plus_base_54_800000.pth</a>下载到/weights文件夹里</p>
<p>如果运行yolact，进入eval.py所在文件夹    </p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /..../yolact-master</span><br><span class="line">python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=test.jpg</span><br></pre></td></tr></table></figure>
<p>add|如果运行yolact++：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /..../yolact-master</span><br><span class="line">python eval.py --trained_model=weights/yolact_plus_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=test.jpg</span><br></pre></td></tr></table></figure>
<p>python eval.py —trained_model=weights/yolact_plus_base_54_800000.pth —score_threshold=0.15 —top_k=15 —images=/home/youyx/data/datasets/TUM/rgbd_dataset_freiburg1_xyz/rgb/:/home/youyx/data/yolact_out_fr1_xyz</p>
<p>python eval.py —trained_model=weights/yolact_plus_base_54_800000.pth —score_threshold=0.15 —top_k=15 —images=/home/youyx/data/datasets/double_3_1/:/home/youyx/data/double_3_1_out</p>
<p>python eval.py —trained_model=weights/yolact_plus_resnet50_54_800000.pth —score_threshold=0.15 —top_k=15 —images=/home/youyx/data/datasets/double_3_1/:/home/youyx/data/double_3_1_out</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="pic1.png" alt="1"></p>
<p>可以看到识别出了一些车，速度也是比较快的。</p>
<h3 id="command"><a href="#command" class="headerlink" title="command"></a>command</h3><p>配置好环境后，以后再次运行yolact就只需要使用以下命令</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> activate</span><br><span class="line"></span><br><span class="line">conda activate yolact</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /home/youyx/yolacte_tutorials/yolact-master/</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> CUDA_VISIBLE_DEVICES=2</span><br><span class="line"></span><br><span class="line">python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=test.jpg</span><br></pre></td></tr></table></figure>
<p>公众号“小鸡炖技术”整理了完整的压缩包，百度云链接：<a href="https://pan.baidu.com/s/1ZKxm9L4fqT0CqwPjEUaYBg">https://pan.baidu.com/s/1ZKxm9L4fqT0CqwPjEUaYBg</a> 提取码pket。</p>
]]></content>
      <categories>
        <category>实例分割</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>实例分割</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵求导</title>
    <url>/2020/10/07/matrix-derivative/</url>
    <content><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>第一次接触矩阵导数是暑期课Frontier Approaches of Control Science的作业用最小二乘法做线性回归。在控制理论和机器学习领域，矩阵求导更是一个常用的数学工具。</p>
<h2 id="矩阵求导的本质"><a href="#矩阵求导的本质" class="headerlink" title="矩阵求导的本质"></a>矩阵求导的本质</h2><p>矩阵A对矩阵B求导，本质上是多元函数求导，也就是矩阵A中的每一个元素对矩阵B中的每一个元素求导，再将求导的结果排列成矩阵的形式。到这里，矩阵求导似乎就讲完了，剩下的就是复合函数求导和求偏导的计算。但是我们很快就发现这种逐元素求导的方法很复杂，随着元素的增加，计算量也极大地增加。那么，有没有直接用矩阵运算，从整体出发的算法。</p>
<h2 id="矩阵求导的形式"><a href="#矩阵求导的形式" class="headerlink" title="矩阵求导的形式"></a>矩阵求导的形式</h2><p>常见的矩阵求导有以下六种，分别是标量对标量求导、标量对向量求导、标量对矩阵求导、向量对标量求导、向量对向量求导和矩阵对标量求导。</p>
<p><img src="pic1.png" alt="pic1"></p>
<h2 id="两种布局"><a href="#两种布局" class="headerlink" title="两种布局"></a>两种布局</h2><p>我们上面提到矩阵求导的本质是矩阵A中的每一个元素对矩阵B中的每一个元素求导，再将求导结果排列成矩阵的形式。对于两个向量的求导结果一般有两种排列方式，分别是分子布局（XY拉伸术）和分母布局（YX拉伸术）。</p>
<p>$\frac{\partial Y}{\partial X}$的布局规则：1.标量不变，向量拉伸 2.前面横向拉，后面纵向拉</p>
<h3 id="分子布局（XY拉伸术）"><a href="#分子布局（XY拉伸术）" class="headerlink" title="分子布局（XY拉伸术）"></a>分子布局（XY拉伸术）</h3><p>对于$\frac{\partial Y}{\partial X}$，分子布局的方法是XY拉伸术。先判断X和Y是不是向量，若二者都是向量，根据布局规则，X在前所以横向拉伸，Y在后所以纵向拉伸，具体过程如下：</p>
<p>标量/向量（在分子布局下，Y是标量，不变；X是向量，横向拉伸）</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial \mathbf x}=[\frac{\partial y}{\partial x_1}...\frac{\partial y}{\partial x_n}]</script><p>向量/标量（在分子布局下，Y是向量，纵向拉伸；X是标量，不变）</p>
<script type="math/tex; mode=display">
\frac{\partial \mathbf y}{\partial x}=\large \begin{bmatrix} \frac{\partial y_1}{\partial x}\\ \vdots\\ \frac{\partial y_m}{\partial x}\end{bmatrix}</script><p>向量/向量（在分子布局下，Y是向量，纵向拉伸；X也是向量，横向拉伸）</p>
<script type="math/tex; mode=display">
\frac{\partial \mathbf y}{\partial \mathbf x}=\large \begin{bmatrix} \frac{\partial y_1}{\partial x_1} \dotso \frac{\partial y_1}{\partial x_n} \\ \vdots \: \: \:  \ddots \: \: \: \vdots \\ \frac{\partial y_m}{\partial x_1} \dotso \frac{\partial y_m}{\partial x_n}\end{bmatrix}</script><h3 id="分母布局（YX拉伸术）"><a href="#分母布局（YX拉伸术）" class="headerlink" title="分母布局（YX拉伸术）"></a>分母布局（YX拉伸术）</h3><p>同样对于$\frac{\partial Y}{\partial X}$，分母布局的方法是YX拉伸术。若二者都是向量，根据布局规则，Y在前所以横向拉伸，X在后所以纵向拉伸，具体过程如下：</p>
<p>标量/向量（在分母布局下，Y是标量，不变；X是向量，纵向拉伸）</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial \mathbf x}=\large \begin{bmatrix} \frac{\partial y}{\partial x_1}\\ \vdots\\ \frac{\partial y}{\partial x_n}\end{bmatrix}</script><p>向量/标量（在分母布局下，Y是向量，横向拉伸；X是标量，不变）</p>
<script type="math/tex; mode=display">
\frac{\partial \mathbf y}{\partial x}=[\frac{\partial y_1}{\partial x}...\frac{\partial y_m}{\partial x}]</script><p>向量/向量（在分母布局下，Y是向量，横向拉伸；X也是向量，纵向拉伸）</p>
<script type="math/tex; mode=display">
\frac{\partial \mathbf y}{\partial \mathbf x}=\large \begin{bmatrix} \frac{\partial y_1}{\partial x_1} \dotso \frac{\partial y_m}{\partial x_1} \\ \vdots \: \: \:  \ddots \: \: \: \vdots \\ \frac{\partial y_1}{\partial x_n} \dotso \frac{\partial y_m}{\partial x_n}\end{bmatrix}</script><p>分子布局和分母布局互为转置的关系：</p>
<ul>
<li>(分子布局)$^{T}$=分母布局</li>
<li>(分母布局)$^{T}$=分子布局</li>
</ul>
<p>在控制理论等领域的雅可比矩阵采用的是分子布局</p>
<p>在机器学习的梯度矩阵中采用的是分母布局</p>
<h2 id="常用的公式"><a href="#常用的公式" class="headerlink" title="常用的公式"></a>常用的公式</h2><p>（a, <strong>a</strong>, A分别是与标量x和向量<strong>x</strong>无关的标量、向量和矩阵）</p>
<p><img src="pic2.png" alt="pic2"></p>
<h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p>这里利用最小二乘法的例子来演示矩阵的整体求导。</p>
<p><img src="pic3.png" alt="pic3"></p>
<p>损失函数定义为y的实际值与拟合曲线对应值之差的平方：</p>
<script type="math/tex; mode=display">
L(\mathbf b)=\sum_{i=1}^n(y_i-\mathbf x_i^T\mathbf b)^2</script><p>用矩阵表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned}L(\mathbf b) &=(\mathbf Y-\mathbf x\mathbf b)^T(\mathbf Y-\mathbf x\mathbf b)\\ &=(\mathbf Y^T-\mathbf b^T \mathbf x^T)(\mathbf Y-\mathbf x\mathbf b)\\ &=\mathbf Y^T\mathbf Y-\mathbf Y^T \mathbf x \mathbf b-\mathbf b^T \mathbf x^T \mathbf Y+\mathbf b^T \mathbf x^T\mathbf x\mathbf b\\ &=\mathbf Y^T\mathbf Y-2\mathbf Y^T \mathbf x \mathbf b+\mathbf b^T \mathbf x^T\mathbf x\mathbf b
\end{aligned}</script><p>要找到一组系数向量<strong>b</strong>使得损失函数最小，将损失函数对<strong>b</strong>求导：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\text{d}L(\mathbf b)}{\text{d}\mathbf b} &=\frac{\text{d}\mathbf Y^T \mathbf Y}{\text{d}\mathbf b}-2\frac{\text{d}\mathbf Y^T \mathbf x \mathbf b}{\text{d}\mathbf b}+\frac{\text{d}\mathbf b^T \mathbf x^T \mathbf x \mathbf b}{\text{d}\mathbf b}\\ &=\mathbf 0-2(\mathbf Y^T \mathbf x)^T+2\mathbf x^T \mathbf x\mathbf b\\ &=-2 \mathbf x^T \mathbf Y+2\mathbf x^T \mathbf x \mathbf b=\mathbf 0
\end{aligned}</script><p>得到</p>
<script type="math/tex; mode=display">
\hat b=(\mathbf x^T \mathbf x)^{-1}\mathbf x^T\mathbf Y</script>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>矩阵</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>手写数字识别器</title>
    <url>/2020/10/11/num-recognizer/</url>
    <content><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>  手写数字识别器是一个经典的卷积神经网络问题，这里将利用PyTorch实验手写数字识别器的任务。</p>
<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>  首先导入所有需要的库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> dsets</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]=<span class="string">&quot;TRUE&quot;</span></span><br></pre></td></tr></table></figure>
<p>  接着定义一些训练用的超参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">image_size = <span class="number">28</span> <span class="comment">#训练图像的总尺寸为28*28</span></span><br><span class="line">num_classes = <span class="number">10</span> <span class="comment">#标签种类数</span></span><br><span class="line">num_epochs = <span class="number">20</span> <span class="comment">#训练的总循环周期</span></span><br><span class="line">batch_size = <span class="number">64</span> <span class="comment">#一个批次的大小，64张图片</span></span><br></pre></td></tr></table></figure>
<p>  然后导入数据，Pytorch中自带了我们需要的手写数据集MNIST。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#加载MNIST数据，如果没有下载过，系统会在当前路径下新建/data子目录，并把文件存放在其中（压缩的格式）</span></span><br><span class="line"><span class="comment">#MNIST数据属于torchvision包自带的数据，可以直接调用</span></span><br><span class="line">train_dataset = dsets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">                            train=<span class="literal">True</span>,</span><br><span class="line">                            transform=transforms.ToTensor(),</span><br><span class="line">                            download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载测试数据集</span></span><br><span class="line">test_dataset = dsets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">                           train=<span class="literal">False</span>,</span><br><span class="line">                           transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练数据集的加载器，自动将数据分成批，顺序随机打乱</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset,</span><br><span class="line">                                           batch_size=batch_size,</span><br><span class="line">                                           shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将数据分为两部分，一部分作为校验数据，用于检测模型是否过拟合并调整参数</span></span><br><span class="line"><span class="comment">#另一部分作为测试数据，检验整个模型</span></span><br><span class="line"><span class="comment">#首先，定义下标数组indices，相当于test_dataset中数据的编码</span></span><br><span class="line"><span class="comment">#然后，定义下表indices_val表示校验集数据的下标，indices_test表示测试集的下标</span></span><br><span class="line">indices = range(len(test_dataset))</span><br><span class="line">indices_val = indices[:<span class="number">5000</span>]</span><br><span class="line">indices_test = indices[<span class="number">5000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment">#根据两个下标构造两个数据集的SubsetRandomSampler采样器，他会对下标进行采样</span></span><br><span class="line">sampler_val = torch.utils.data.sampler.SubsetRandomSampler(indices_val)</span><br><span class="line">sampler_test = torch.utils.data.sampler.SubsetRandomSampler(indices_test)</span><br><span class="line"></span><br><span class="line"><span class="comment">#根据两个采样器定义加载器</span></span><br><span class="line"><span class="comment">#将sampler_val和sampler_test分别赋值给validation_loader和test_loader</span></span><br><span class="line">validation_loader = torch.utils.data.DataLoader(dataset=test_dataset,</span><br><span class="line">                                                batch_size=batch_size,</span><br><span class="line">                                                shuffle=<span class="literal">False</span>,</span><br><span class="line">                                                sampler=sampler_val)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset,</span><br><span class="line">                                          batch_size=batch_size,</span><br><span class="line">                                          shuffle=<span class="literal">False</span>,</span><br><span class="line">                                          sampler=sampler_test)</span><br></pre></td></tr></table></figure>
<ul>
<li>数据集（dataset）是对整个数据的封装，无论原始数据是图像还是张量，数据集都将对其进行统一处理。</li>
<li>加载器（dataloader）主要负责在程序中对数据集的使用。</li>
<li>采样器（sampler）为加载器提供了一个每一批抽取数据集中样本的方法。</li>
</ul>
<h2 id="构建网络"><a href="#构建网络" class="headerlink" title="构建网络"></a>构建网络</h2><p>  这里主要利用PyTorch的nn.Module类来构建卷积神经网络。</p>
<p>  首先，构造ConvNet类，它是对nn.Module类的继承。</p>
<p>  其次，复写init()和forward()两个函数。init()为构造函数，每当类ConvNet被具体化一个实例的时候就会被调用。forward()函数则是在正向运行神经网络时被自动调用，负责数据的向前传递，并同时构造计算图。</p>
<p>  然后，定义一个retrieve_features()函数，用来提取网络中各个卷积层的权重。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义卷积神经网络：4和8为认为指定的两个卷积层的厚度（feature map的数量）</span></span><br><span class="line">depth = [<span class="number">4</span>, <span class="number">8</span>]</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 该函数在创建一个ConvNet对象，即调用语句net=ConvNet()时就会被调用</span></span><br><span class="line">        <span class="comment"># 首先调用父类相应的构造函数</span></span><br><span class="line">        super(ConvNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#其次构造ConvNet要用到的各个神经网络模块</span></span><br><span class="line">        <span class="comment">#注意，构造组件并不是真正搭建组件，只是把基本建筑砖块先找好</span></span><br><span class="line">        <span class="comment">#定义一个卷积层，输入通道为1，输出通道为4，窗口大小为5，padding为2</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>, padding = <span class="number">2</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)<span class="comment">#定义一个池化层，一个窗口为2x2的池化运算</span></span><br><span class="line">        <span class="comment">#第二个卷积层，输入通道为depth[0],输出通道为depth[1]，窗口为5，padding为2</span></span><br><span class="line">        self.conv2 = nn.Conv2d(depth[<span class="number">0</span>], depth[<span class="number">1</span>], <span class="number">5</span>, padding = <span class="number">2</span>)</span><br><span class="line">        <span class="comment">#一个线性连接层，输入尺寸为最后一层立方体的线性平铺，输出层512个节点</span></span><br><span class="line">        self.fc1 = nn.Linear(image_size // <span class="number">4</span> * image_size // <span class="number">4</span> * depth[<span class="number">1</span>], <span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">512</span>, num_classes) <span class="comment">#最后一层线性分类单元，输入为512，输出为要做分类的类别数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span> <span class="comment">#该函数完成神经网络真正的前向运算，在这里把各个组件进行实际的拼装</span></span><br><span class="line">        <span class="comment"># x的尺寸：（batch_size, image_channels, image_width, image_height）</span></span><br><span class="line">        x = self.conv1(x)    <span class="comment">#第一层卷积</span></span><br><span class="line">        x = F.relu(x)      <span class="comment">#激活函数用ReLU，防止过拟合</span></span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, num_filters, image_width, image_height）</span></span><br><span class="line"></span><br><span class="line">        x = self.pool(x) <span class="comment">#第二层池化，把图片变小</span></span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, depth[0], image_width/2, image_height/2）</span></span><br><span class="line"></span><br><span class="line">        x = self.conv2(x) <span class="comment">#第三层卷积，窗口为5，输入输出通道分别为depth[0]=4, depth[1]=8</span></span><br><span class="line">        x = F.relu(x) <span class="comment">#非线性函数</span></span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, depth[0], image_width/2, image_height/2）</span></span><br><span class="line"></span><br><span class="line">        x = self.pool(x) <span class="comment">#第四层池化，将图片缩小到原来的1/4</span></span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, depth[1], image_width/4, image_heigth/4）</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#将立体的特征图tensor压成一个一维向量</span></span><br><span class="line">        <span class="comment">#view函数可以将一个tensor按指定的方式重新排布</span></span><br><span class="line">        <span class="comment">#下面这个命令就是要让x按照batch_size * (image_size//4)^2*depth[1]的方式来排布向量</span></span><br><span class="line">        x = x.view(<span class="number">-1</span>, image_size // <span class="number">4</span> * image_size // <span class="number">4</span> *depth[<span class="number">1</span>])</span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, depth[1]*image_width/4*image_height/4）</span></span><br><span class="line"></span><br><span class="line">        x = F.relu(self.fc1(x))<span class="comment">#第五层为全连接，ReLU激活函数</span></span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, 512）</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#以默认0.5的概率对这一层进行dropout操作，防止过拟合</span></span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.fc2(x) <span class="comment">#全连接</span></span><br><span class="line">        <span class="comment">#x的尺寸：(batch_size, num_classes)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#输出层为log_softmax,即概率对数值log(p(x)),采用log_softmax可以使后面交叉熵计算更快</span></span><br><span class="line">        x = F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">retrieve_features</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 该函数用于提取卷积神经网络的特征图，返回feature_map1, feature_map2为前两层卷积层的特征图</span></span><br><span class="line">        feature_map1 = F.relu(self.conv1(x)) <span class="comment">#完成第一层卷积</span></span><br><span class="line">        x = self.pool(feature_map1)</span><br><span class="line">        <span class="comment">#第二层卷积，两层特征图都存储到了feature_map1, feature_map2中</span></span><br><span class="line">        feature_map2 = F.relu(self.conv2(x))</span><br><span class="line">        <span class="keyword">return</span> (feature_map1, feature_map2)</span><br></pre></td></tr></table></figure>
<p>  在以上代码中用到了dropout()函数，该函数用来防止神经网络的过拟合情况。在训练过程中，根据一定的概率随机将其中的一些神经元暂时丢弃，最后在测试的时候再使用全部的神经元，增强模型的泛化能力。</p>
<h2 id="运行模型"><a href="#运行模型" class="headerlink" title="运行模型"></a>运行模型</h2><p>  构建好ConvNet之后，就可以读取数据并训练模型了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = ConvNet() <span class="comment">#新建一个卷积神经网络的实例，此时ConvNet的__init__()函数会被自动调用</span></span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss() <span class="comment">#损失函数的定义，交叉熵</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>) <span class="comment">#定义优化器，普通的随机梯度下降算法</span></span><br><span class="line"></span><br><span class="line">record = [] <span class="comment">#记录准确率等数值的容器</span></span><br><span class="line">weights = [] <span class="comment">#每若干步就记录一次卷积核</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rightness</span>(<span class="params">output, target</span>):</span></span><br><span class="line">    preds = output.data.max(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> preds.eq(target.data.view_as(preds)).cpu().sum(), len(target)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始训练循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line"></span><br><span class="line">    train_rights = [] <span class="comment">#记录训练数据集准确率的容器</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;下面的enumerate起到构造枚举器的作用，在对train_loader做循环迭代时，enumerate会自动输出一个数字指示循环了几次</span></span><br><span class="line"><span class="string">    并记录在batch_idx中，它就等于0，1，2，...train_loader每迭代一次，就会输出一对数据data和target,分别对应一个批中的</span></span><br><span class="line"><span class="string">    手写数字图及对应的标签。&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> enumerate(train_loader): <span class="comment">#针对容器中的每一批进行循环</span></span><br><span class="line">        <span class="comment">#将Tensor转化为Variable，data为一批图像，target为一批标签</span></span><br><span class="line">        data, target = Variable(data), Variable(target)</span><br><span class="line">        <span class="comment">#给网络模型做标记，标志着模型在训练集上训练</span></span><br><span class="line">        <span class="comment">#这种区分主要是为了打开关闭net的training标志，从而决定是否运行dropout</span></span><br><span class="line">        net.train()</span><br><span class="line"></span><br><span class="line">        output = net(data) <span class="comment">#神经网络完成一次前馈的计算过程，得到预测输出output</span></span><br><span class="line">        loss = criterion(output, target) <span class="comment">#将output与标签target比较，计算误差</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment">#清空梯度</span></span><br><span class="line">        loss.backward() <span class="comment">#反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment">#一步随机梯度下降算法</span></span><br><span class="line">        right = rightness(output, target) <span class="comment">#计算准确率所需数值，返回数值为（正确样例数，总样本数）</span></span><br><span class="line">        train_rights.append(right) <span class="comment">#将计算结果装到列表容器train_rights中</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">100</span> == <span class="number">0</span>: <span class="comment">#每隔100个batch执行一次打印操作</span></span><br><span class="line"></span><br><span class="line">            net.eval() <span class="comment">#给网络模型做标记，标志着模型在训练集上训练</span></span><br><span class="line">            val_rights = [] <span class="comment">#记录校验数据集准确率的标签</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#开始在校验集上做循环，计算校验集上的准确度</span></span><br><span class="line">            <span class="keyword">for</span> (data, target) <span class="keyword">in</span> validation_loader:</span><br><span class="line">                data, target = Variable(data), Variable(target)</span><br><span class="line">                <span class="comment">#完成一次前馈计算过程，得到目前训练得到的模型net在校验集上的表现</span></span><br><span class="line">                output = net(data)</span><br><span class="line">                <span class="comment">#计算准确率所需数值，返回正确的数值为（正确样例数，总样本数）</span></span><br><span class="line">                right = rightness(output, target)</span><br><span class="line">                val_rights.append(right)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#分别计算目前已经计算过的测试集以及全部校验集上模型的表现：分类准确率</span></span><br><span class="line">            <span class="comment">#train_r为一个二元组，分别记录经历过的所有训练集中分类正确的数量和该集合中总的样本数</span></span><br><span class="line">            <span class="comment">#train_r[0]/train_r[1]是训练集的分类准确度，val_r[0]/val_r[1]是校验集的分类准确度</span></span><br><span class="line">            train_r = (sum([tup[<span class="number">0</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> train_rights]),sum([tup[<span class="number">1</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> train_rights]))</span><br><span class="line">            <span class="comment">#val_r为一个二元组，分别记录校验集中分类正确的数量和该集合中的总样本数</span></span><br><span class="line">            val_r = (sum([tup[<span class="number">0</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> val_rights]),sum([tup[<span class="number">1</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> val_rights]))</span><br><span class="line"></span><br><span class="line">            <span class="comment">#打印准确率等数值，其中正确率为样本训练周期epoch开始后到目前批的正确率的平均值</span></span><br><span class="line">            <span class="comment">#print(&#x27;训练周期：&#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\t, Loss:&#123;:.6f&#125;\t, 训练正确率：&#123;:.2f&#125;%\t, 校验正确率：&#123;:.2f&#125;%&#x27;.format(</span></span><br><span class="line">            <span class="comment">#     epoch, batch_idx * len(data), len(train_loader.dataset),</span></span><br><span class="line">            <span class="comment">#     100. * batch_idx / len(train_loader), loss.data[0],</span></span><br><span class="line">            <span class="comment">#     100. * train_r[0] / train_r[1],</span></span><br><span class="line">            <span class="comment">#     100. * val_r[0] / val_r[1]))</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#将准确率和权重等数值加载到容器中，方便后续处理</span></span><br><span class="line">            record.append((<span class="number">100</span> - <span class="number">100.</span> * train_r[<span class="number">0</span>] / train_r[<span class="number">1</span>], <span class="number">100</span> - <span class="number">100.</span> * val_r[<span class="number">0</span>] / val_r[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">            <span class="comment">#wights记录了训练中其中所有卷积演化的过程，net.conv1.weight提取出了第一层卷积核的权重</span></span><br><span class="line">            <span class="comment">#clone是将weight.data中的数据做一个备份放到列表中</span></span><br><span class="line">            <span class="comment">#否则放weight.data变化时，列表中的每一项数值也会联动</span></span><br><span class="line">            <span class="comment">#这里使用clone这个函数很重要</span></span><br><span class="line">            weights.append([net.conv1.weight.data.clone(), net.conv1.bias.data.clone(),</span><br><span class="line">                            net.conv2.weight.data.clone(), net.conv2.bias.data.clone()])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  以上代码中，net.train()会打开所有的dropout层，而net.eval()会关闭它们。</p>
<h2 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#在训练集上分批运行，并计算总的正确率</span></span><br><span class="line">net.eval() <span class="comment">#标志着模型当前的运行阶段</span></span><br><span class="line">vals = [] <span class="comment">#记录准确率所用列表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#对测试数据集进行循环</span></span><br><span class="line"><span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        data = Variable(data)</span><br><span class="line">    target = Variable(target)</span><br><span class="line">    output = net(data) <span class="comment">#将特征数据输入网络，得到分类的输出</span></span><br><span class="line">    val = rightness(output, target) <span class="comment">#获得正确样本数以及总样本数</span></span><br><span class="line">    vals.append(val) <span class="comment">#记录结果</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算准确率</span></span><br><span class="line">rights = (sum([tup[<span class="number">0</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> vals]),sum([tup[<span class="number">1</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> vals]))</span><br><span class="line">right_rate = <span class="number">1.0</span> * rights[<span class="number">0</span>] / rights[<span class="number">1</span>]</span><br><span class="line">print(right_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制训练过程的误差曲线，校验集和测试集上的错误率</span></span><br><span class="line">plt.figure(figsize = (<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">plt.plot(record) <span class="comment">#record记录了每一个打印周期记录的训练集和校验集上的准确度</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Steps&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Error rate&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>  最后，将训练过程中的误差曲线绘制出来。</p>
<p><img src="pic1.png" alt="1"></p>
<p>  图中左边浅色的为校验数据错误率曲线，右边深色的为测试数据错误率曲线。模型在测试集和校验集上的表现都很好，卷积神经网络的泛化能力也很强。</p>
]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
</search>
