<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello！</title>
    <url>/2021/09/24/hello-world/</url>
    <content><![CDATA[<p>Welcome to my blog!</p>
]]></content>
  </entry>
  <entry>
    <title>语义建图性能评估</title>
    <url>/2021/04/20/%E8%AF%AD%E4%B9%89%E5%BB%BA%E5%9B%BE%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0/</url>
    <content><![CDATA[<h2 id="MISD-SLAM预测轨迹与ground-truth对比"><a href="#MISD-SLAM预测轨迹与ground-truth对比" class="headerlink" title="MISD-SLAM预测轨迹与ground truth对比"></a>MISD-SLAM预测轨迹与ground truth对比</h2><h3 id="1-利用EVO工具评估"><a href="#1-利用EVO工具评估" class="headerlink" title="1.利用EVO工具评估"></a>1.利用EVO工具评估</h3><p>（1）安装evo工具</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install evo --upgrade --no-binary evo --user</span><br></pre></td></tr></table></figure>
<p>（2）拷贝MISD_CameraTrajectory.txt、ORBSLAM3_CameraTrajectory.txt和groundtruth.txt早评估文件夹下</p>
<p>（3）轨迹对比</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">evo_traj tum MISD_CameraTrajectory.txt ORBSLAM3_CameraTrajectory.txt --ref=groundtruth.txt -as --plot --plot_mode xy</span><br></pre></td></tr></table></figure>
<p>得到三条轨迹对比结果：</p>
<p><img src="png0.png" alt="1"></p>
<p>可以看到，动态物体剔除很大程度上提高了SLAM轨迹预测的准确度。</p>
<p>（4）计算绝对位姿误差</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">evo_ape tum groundtruth.txt MISD_CameraTrajectory.txt -p --plot_mode=xy -as</span><br></pre></td></tr></table></figure>
<p>（5）计算相对位姿误差</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">evo_rpe tum groundtruth.txt MISD_CameraTrajectory.txt -p --plot_mode=xy -as</span><br></pre></td></tr></table></figure>
<h3 id="2-利用TUM-RGB-D评估工具"><a href="#2-利用TUM-RGB-D评估工具" class="headerlink" title="2.利用TUM RGB-D评估工具"></a>2.利用TUM RGB-D评估工具</h3><p>（1）下载测评工具</p>
<p><a href="https://svncvpr.in.tum.de/cvpr-ros-pkg/trunk/rgbd_benchmark/rgbd_benchmark_tools/">https://svncvpr.in.tum.de/cvpr-ros-pkg/trunk/rgbd_benchmark/rgbd_benchmark_tools/</a></p>
<p>（2）运行evaluate_ate.py</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">python evaluate_ate.py groundtruth.txt ORBSLAM3_CameraTrajectory.txt --plot ORBSLAM3_result.png</span><br><span class="line">python evaluate_ate.py groundtruth.txt MISD_CameraTrajectory.txt --plot MISD_result.png</span><br></pre></td></tr></table></figure>
<p>可以看到：</p>
<p><img src="png1.png" alt="2"><img src="png2.png" alt="3"></p>
<h3 id="MISD-SLAM建图效果"><a href="#MISD-SLAM建图效果" class="headerlink" title="MISD-SLAM建图效果"></a>MISD-SLAM建图效果</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">pcl_viewer SemanticMappingResult.pcd -cam test1.cam</span><br><span class="line">pcl_viewer SemanticMappingResult.pcd -cam test2.cam</span><br><span class="line">pcl_viewer SemanticMappingResult.pcd -cam test3.cam</span><br></pre></td></tr></table></figure>
<p>MISD-SLAM通过将动态物体剔除，能够建立环境的静态地图，以下是在TUM RGB-D数据集的fr3_walking_halfsphere序列的建图效果。</p>
<p><img src="png3.png" alt="4"><img src="png4.png" alt="5"><img src="png5.png" alt="6"></p>
<h3 id=""><a href="#" class="headerlink" title=" "></a> </h3>]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>实例分割</tag>
        <tag>SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLACT++与ORBSLAM2的语义建图</title>
    <url>/2021/04/10/YOLACT++%E4%B8%8EORBSLAM2%E7%9A%84%E8%AF%AD%E4%B9%89%E5%BB%BA%E5%9B%BE/</url>
    <content><![CDATA[<h3 id="服务端"><a href="#服务端" class="headerlink" title="服务端"></a>服务端</h3><p>语义分割作为服务端，在终端1：</p>
<p>1.调整~/.bashrc至anaconda3</p>
<p>2.打开conda的yolact虚拟环境</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda activate yolact</span><br><span class="line"><span class="built_in">cd</span> ~/catkin_ws/src/YOLACT_ORBSLAM2/yolact_server/</span><br><span class="line">python server.py</span><br></pre></td></tr></table></figure>
<h3 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h3><p>ORB_SLAM2作为客户端，在终端2：</p>
<p>1.调整~/.bashrc至ROS</p>
<p>2.输入以下命令，运行ORB_SLAM2</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM1.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg1_xyz/ ./Examples/RGB-D/associations/fr1_xyz.txt</span><br></pre></td></tr></table></figure>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_long_office_household/ ./Examples/RGB-D/associations/fr3_long_ofc.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_nostructure_texture_far/ ./Examples/RGB-D/associations/fr3_nstr_tex_far.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_sitting_static/ ./Examples/RGB-D/associations/fr3_sit_stc.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM2.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg2_desk_with_person/ ./Examples/RGB-D/associations/fr2_dsk_psn.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_static/ ./Examples/RGB-D/associations/fr3_wlk_stc.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_xyz/ ./Examples/RGB-D/associations/fr3_wlk_xyz.txt</p>
<h3 id="可能出现的问题"><a href="#可能出现的问题" class="headerlink" title="可能出现的问题"></a>可能出现的问题</h3><p>1.Traceback (most recent call last):<br>  File “server.py”, line 44, in <module><br>    tcpSerSock.bind(ADDR)<br>OSError: [Errno 98] Address already in use</p>
<p>解决方法：</p>
<p>在命令行中输入</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">netstat -tunpl</span><br></pre></td></tr></table></figure>
<p>查看占用状态</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">kill</span> -9 PID号</span><br></pre></td></tr></table></figure>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM1.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg1_xyz/ /home/youyx/catkin_ws/src/YOLACT_ORBSLAM3/Examples/RGB-D/associations/fr1_xyz.txt</p>
<p>assosiation文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;home&#x2F;youyx&#x2F;catkin_ws&#x2F;src&#x2F;YOLACT_ORBSLAM2&#x2F;Examples&#x2F;RGB-D</span><br><span class="line">python associate.py &#x2F;home&#x2F;youyx&#x2F;data&#x2F;datasets&#x2F;TUM&#x2F;rgbd_dataset_freiburg3_walking_static&#x2F;rgb.txt &#x2F;home&#x2F;youyx&#x2F;data&#x2F;datasets&#x2F;TUM&#x2F;rgbd_dataset_freiburg3_walking_static&#x2F;depth.txt &gt; .&#x2F;associations&#x2F;fr3_wlk_stc.txt</span><br></pre></td></tr></table></figure>
<p>python associate.py /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_xyz/rgb.txt /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_xyz/depth.txt &gt; ./associations/fr3_wlk_xyz.txt</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>ORB-SLAM</category>
        <category>语义建图</category>
        <category>实例分割</category>
      </categories>
      <tags>
        <tag>实例分割</tag>
        <tag>SLAM</tag>
        <tag>语义建图</tag>
        <tag>ORB-SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>pspnet+ORB_SLAM2+octomap</title>
    <url>/2021/04/07/pspnet+ORB_SLAM2+octomap/</url>
    <content><![CDATA[<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> semantic_slam_floatlazer</span><br><span class="line"><span class="built_in">cd</span> ORB_SLAM2 </span><br><span class="line">./build.sh</span><br><span class="line"><span class="built_in">cd</span> ../../..</span><br><span class="line"></span><br><span class="line">catkin_make</span><br></pre></td></tr></table></figure>
<p>catkin_make的时候可能会遇到一些问题，比如</p>
<p>CMake Error at /opt/ros/indigo/share/catkin/cmake/catkinConfig.cmake:83 (find_package):<br>  Could not find a package configuration file provided by “octomap_msgs” with<br>  any of the following names:</p>
<p>​        octomap_msgsConfig.cmake<br>​        octomap_msgs-config.cmake</p>
<p>  Add the installation prefix of “octomap_msgs” to CMAKE_PREFIX_PATH or set<br>  “octomap_msgs_DIR” to a directory containing one of the above files.  If<br>  “octomap_msgs” provides a separate development package or SDK, be sure it<br>  has been installed.</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo apt install ros-indigo-octomap*</span><br></pre></td></tr></table></figure>
<p>再catkin_make一下</p>
<p>setuptools&gt;=41.0.0<br>numpy&gt;=1.15<br>scipy<br>Pillow<br>cython<br>opencv-python==3.3.1.11<br>matplotlib<br>scikit-image<br>tensorflow-gpu==1.13.1<br>keras==2.0.8<br>h5py<br>imageio==2.6.1<br>imgaug<br>pandas<br>future<br>torch<br>torchvision<br>protobuf<br>IPython[all]</p>
<p>把依赖包写入requirements.txt放在/home/youyx/catkin_ws/src/semantic_slam_floatlazer/下</p>
<p>sudo pip install -r /…/requirements.txt</p>
<h3 id="模型和数据集准备"><a href="#模型和数据集准备" class="headerlink" title="模型和数据集准备"></a>模型和数据集准备</h3><p>模型是pspnet_50_ade20k.pth，在google drive上下载</p>
<p><a href="https://drive.google.com/file/d/1u_BEWdVIYiDnpVmAxwME1z3rnWWkjxm5/view?usp=sharing">https://drive.google.com/file/d/1u_BEWdVIYiDnpVmAxwME1z3rnWWkjxm5/view?usp=sharing</a></p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /semantic_slam_floatlazer/semantic_slam/params </span><br><span class="line">vim semantic_cloud.yaml </span><br><span class="line">vim semantic_cloud_tum.yaml </span><br></pre></td></tr></table></figure>
<p>将model_path修改为模型保存的路径</p>
<p>model_path: “/home/youyx/data/semantic_slam/pspnet_50_ade20k.pth”</p>
<p>下载tum数据集rgbd_dataset_freiburg1_room.bag</p>
<p><a href="https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_room.bag">https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_room.bag</a></p>
<h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>终端1：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">roslaunch floatlazer_semantic_slam semantic_mapping_tum.launch</span><br></pre></td></tr></table></figure>
<p>终端2：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home/youyx/data/semantic_slam/TUM/freiburg1</span><br><span class="line">rosbag play --clock rgbd_dataset_freiburg1_room.bag</span><br></pre></td></tr></table></figure>
<h3 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h3><p><img src="pic1.png" alt="1"></p>
]]></content>
      <categories>
        <category>实例分割</category>
        <category>SLAM</category>
        <category>语义建图</category>
      </categories>
      <tags>
        <tag>实例分割</tag>
        <tag>SLAM</tag>
        <tag>语义建图</tag>
      </tags>
  </entry>
  <entry>
    <title>ORB_SLAM2构建稠密地图-高博版</title>
    <url>/2021/03/18/ORB_SLAM2%E6%9E%84%E5%BB%BA%E7%A8%A0%E5%AF%86%E5%9C%B0%E5%9B%BE-%E9%AB%98%E5%8D%9A%E7%89%88/</url>
    <content><![CDATA[<h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>操作系统： Ubuntu 18.04</p>
<p>1.Opencv（3.2.0版本）</p>
<p>2.PCL</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install libpcl-dev</span><br></pre></td></tr></table></figure>
<p>3.Eigen3（3.2版本）</p>
<p>4.Pangolin</p>
<h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><p>1.下载源文件</p>
<p>从高博github上<a href="https://github.com/gaoxiang12/ORBSLAM2_with_pointcloud_map.git下载ORB_SLAM2_modified文件夹，将其中的ORB_SLAM2_modified子文件夹放到~/catkin_ws/src/下">https://github.com/gaoxiang12/ORBSLAM2_with_pointcloud_map.git下载ORB_SLAM2_modified文件夹，将其中的ORB_SLAM2_modified子文件夹放到~/catkin_ws/src/下</a></p>
<p>2.拷贝Vocabulary文件夹</p>
<p>将ORB_SLAM2中的Vocabulary文件夹复制到ORB_SLAM2_modified路径下</p>
<p>3.删除build文件夹</p>
<p>将~/catkin_ws/src/ORB_SLAM2_modified/build、~/catkin_ws/src/ORB_SLAM2_modified/Thirdparty/DBoW2/build 和 ~/catkin_ws/src/ORB_SLAM2_modified/Thirdparty/g2o/build删除</p>
<p>4.运行build.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ~&#x2F;catkin_ws&#x2F;src&#x2F;ORB_SLAM2_modified&#x2F;</span><br><span class="line">chmod +x .&#x2F;build.sh</span><br><span class="line">.&#x2F;build.sh</span><br></pre></td></tr></table></figure>
<h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;rgbd_tum Vocabulary&#x2F;ORBvoc.bin Examples&#x2F;RGB-D&#x2F;TUM1.yaml &#x2F;home&#x2F;youyx&#x2F;data&#x2F;datasets&#x2F;TUM&#x2F;rgbd_dataset_freiburg1_xyz&#x2F; &#x2F;home&#x2F;youyx&#x2F;catkin_ws&#x2F;src&#x2F;ORB_SLAM2_modified&#x2F;Examples&#x2F;RGB-D&#x2F;associations&#x2F;fr1_xyz.txt</span><br></pre></td></tr></table></figure>
<h3 id="彩色地图"><a href="#彩色地图" class="headerlink" title="彩色地图"></a>彩色地图</h3><p>运行后我们发现得到稠密点云地图是黑白地图。这里来构建彩色地图。</p>
<p>1.修改Tracking.h文件</p>
<p>Frame mCurrentFrame;<br>cv::Mat mImRGB;//declared<br>cv::Mat mImGray;</p>
<p>2.修改Tracking.cc文件</p>
<p>Modified place 1:<br>cv::Mat Tracking::GrabImageRGBD(const cv::Mat &amp;imRGB,const cv::Mat &amp;imD, const double &amp;timestamp)<br>{<br>mImRGB = imRGB;//Modified place 1<br>mImGray = imRGB;<br>……</p>
<p>Modified place 2:<br>mpPointCloudMapping-&gt;insertKeyFrame( pKF, this-&gt;mImGray, this-&gt;mImDepth );//change the mImGray to mImRGB as next row<br>mpPointCloudMapping-&gt;insertKeyFrame( pKF, this-&gt;mImRGB, this-&gt;mImDepth );//Modified place 2</p>
<p>3.修改后重新编译</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/ORB_SLAM2_modified/build</span><br><span class="line">make -j8</span><br></pre></td></tr></table></figure>
<h3 id="保存地图"><a href="#保存地图" class="headerlink" title="保存地图"></a>保存地图</h3><p>高博的程序只能实时查看地图，不能保存。这里修改文件pointcloudmapping.cc，调用 PCL 库的 pcl::io::savePCDFileBinary 函数就可以保存点云地图了</p>
<p>1.加入头文件</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;pcl/io/pcd_io.h&gt;</span></span></span><br></pre></td></tr></table></figure>
<p>2.调用pcl::io::savePCDFileBinary 函数</p>
<p>在 void PointCloudMapping::viewer() 函数中（ 123 行附近）加入保存地图的命令：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">for</span> ( <span class="keyword">size_t</span> i=lastKeyframeSize; i&lt;N ; i++ )</span><br><span class="line">&#123;</span><br><span class="line">    PointCloud::Ptr p = generatePointCloud( keyframes[i], colorImgs[i], depthImgs[i] );</span><br><span class="line">    *globalMap += *p;</span><br><span class="line">&#125;</span><br><span class="line">pcl::io::savePCDFileBinary(<span class="string">&quot;vslam.pcd&quot;</span>, *globalMap);   <span class="comment">// 只需要加入这一句</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>3.修改后重新编译</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/ORB_SLAM2_modified/build</span><br><span class="line">make -j8</span><br></pre></td></tr></table></figure>
<p>4.运行建图命令</p>
<p>就在 ~/ORB_SLAM2_modified 路径下产生一个名为 vslam.pcd 的点云文件。</p>
<h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>1.若运行fr2和fr3数据集，要把参数PointCloudMapping.Resolution: 0.01加入到TUMX.yaml配置文件里</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>ORB-SLAM</category>
        <category>点云地图</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>ORB-SLAM</tag>
        <tag>点云地图</tag>
      </tags>
  </entry>
  <entry>
    <title>deeplab+ORB_SLAM2的语义建图</title>
    <url>/2021/03/15/deeplab+ORB_SLAM2%E7%9A%84%E8%AF%AD%E4%B9%89%E5%BB%BA%E5%9B%BE/</url>
    <content><![CDATA[<h3 id="服务端"><a href="#服务端" class="headerlink" title="服务端"></a>服务端</h3><p>语义分割作为服务端，在终端1：</p>
<p>1.调整~/.bashrc至anaconda3</p>
<p>2.打开conda的deeplab虚拟环境</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda activate deeplab-pytorch</span><br><span class="line"><span class="built_in">cd</span> ~/catkin_ws/src/Semantic_Mapping_on_ORBSLAM2/deeplabv2_server/</span><br><span class="line">python inference.py</span><br></pre></td></tr></table></figure>
<h3 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h3><p>ORB_SLAM2作为客户端，在终端2：</p>
<p>1.调整~/.bashrc至ROS</p>
<p>2.输入以下命令，运行ORB_SLAM2</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM1.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg1_xyz/ /home/youyx/catkin_ws/src/Semantic_Mapping_on_ORBSLAM2/Examples/RGB-D/associations/fr1_xyz.txt</span><br></pre></td></tr></table></figure>
<h3 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h3><p><img src="pic1.png" alt="1"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>ORB-SLAM</category>
        <category>语义建图</category>
        <category>语义分割</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>语义建图</tag>
        <tag>ORB-SLAM</tag>
        <tag>语义分割</tag>
      </tags>
  </entry>
  <entry>
    <title>ORB_SLAM3配置</title>
    <url>/2021/03/15/ORB_SLAM3%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h3 id="安装下载工具"><a href="#安装下载工具" class="headerlink" title="安装下载工具"></a>安装下载工具</h3><p>cmake、gcc、g++和git工具</p>
<h3 id="安装Pangolin"><a href="#安装Pangolin" class="headerlink" title="安装Pangolin"></a>安装Pangolin</h3><h3 id="安装OpenCV"><a href="#安装OpenCV" class="headerlink" title="安装OpenCV"></a>安装OpenCV</h3><h3 id="安装Eigen3"><a href="#安装Eigen3" class="headerlink" title="安装Eigen3"></a>安装Eigen3</h3><p>以上详细过程见ORB_SLAM2配置</p>
<h3 id="建立ORB-SLAM3"><a href="#建立ORB-SLAM3" class="headerlink" title="建立ORB_SLAM3"></a>建立ORB_SLAM3</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/catkin_ws/src</span><br></pre></td></tr></table></figure>
<p>复制ORB_SLAM3到src文件夹下，两种方法</p>
<p>方法一：git复制</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/UZ-SLAMLab/ORB_SLAM3.git ORB_SLAM3</span><br></pre></td></tr></table></figure>
<p>方法二：手动下载，然后解压到src文件夹</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ORB_SLAM3</span><br><span class="line">chmod +x build.sh</span><br><span class="line">./build.sh</span><br></pre></td></tr></table></figure>
<h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>下载EuRoC数据集</p>
<p><a href="https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets#the_euroc_mav_dataset">https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets#the_euroc_mav_dataset</a></p>
<p>修改ORB-SLAM3/euroc_examples.sh文件中的数据集路径</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="comment">#pathDatasetEuroc=&#x27;/Datasets/EuRoC&#x27; #Example, it is necesary to change it by the dataset path</span></span><br><span class="line">pathDatasetEuroc=<span class="string">&#x27;/home/youyx/data/datasets/EuRoC/&#x27;</span> <span class="comment">#Example, it is necesary to change it by the dataset path</span></span><br></pre></td></tr></table></figure>
<h4 id="Monocular"><a href="#Monocular" class="headerlink" title="Monocular"></a>Monocular</h4><p>MH</p>
<p>./Examples/Monocular/mono_euroc ./Vocabulary/ORBvoc.txt ./Examples/Monocular/EuRoC.yaml /home/youyx/data/datasets/EuRoC/MH01 ./Examples/Monocular/EuRoC_TimeStamps/MH01.txt dataset-MH01_mono</p>
<p>./Examples/Monocular/mono_euroc ./Vocabulary/ORBvoc.txt ./Examples/Monocular/EuRoC.yaml /home/youyx/data/datasets/EuRoC/MH02 ./Examples/Monocular/EuRoC_TimeStamps/MH02.txt dataset-MH02_mono</p>
<p>./Examples/Monocular/mono_euroc ./Vocabulary/ORBvoc.txt ./Examples/Monocular/EuRoC.yaml /home/youyx/data/datasets/EuRoC/MH03 ./Examples/Monocular/EuRoC_TimeStamps/MH03.txt dataset-MH03_mono</p>
<p>./Examples/Monocular/mono_euroc ./Vocabulary/ORBvoc.txt ./Examples/Monocular/EuRoC.yaml /home/youyx/data/datasets/EuRoC/MH04 ./Examples/Monocular/EuRoC_TimeStamps/MH04.txt dataset-MH04_mono</p>
<p>./Examples/Monocular/mono_euroc ./Vocabulary/ORBvoc.txt ./Examples/Monocular/EuRoC.yaml /home/youyx/data/datasets/EuRoC/MH05 ./Examples/Monocular/EuRoC_TimeStamps/MH05.txt dataset-MH05_mono</p>
<p>V1</p>
<p>./Examples/Monocular/mono_euroc ./Vocabulary/ORBvoc.txt ./Examples/Monocular/EuRoC.yaml /home/youyx/data/datasets/EuRoC/V101 ./Examples/Monocular/EuRoC_TimeStamps/V101.txt dataset-V101_mono</p>
<p>V2</p>
<p>./Examples/Monocular/mono_euroc ./Vocabulary/ORBvoc.txt ./Examples/Monocular/EuRoC.yaml /home/youyx/data/datasets/EuRoC/V201 ./Examples/Monocular/EuRoC_TimeStamps/V201.txt dataset-V201_mono</p>
<p>MultiSession Monocular</p>
<p>./Examples/Monocular/mono_euroc ./Vocabulary/ORBvoc.txt ./Examples/Monocular/EuRoC.yaml /home/youyx/data/datasets/EuRoC/V101 ./Examples/Monocular/EuRoC_TimeStamps/V101.txt /home/youyx/data/datasets/EuRoC/V102 ./Examples/Monocular/EuRoC_TimeStamps/V102.txt /home/youyx/data/datasets/EuRoC/V103 ./Examples/Monocular/EuRoC_TimeStamps/V103.txt dataset-V101_to_V103_mono</p>
<h4 id="Stereo"><a href="#Stereo" class="headerlink" title="Stereo"></a>Stereo</h4><p>./Examples/Stereo/stereo_euroc ./Vocabulary/ORBvoc.txt ./Examples/Stereo/EuRoC.yaml /home/youyx/data/datasets/EuRoC/V101 ./Examples/Stereo/EuRoC_TimeStamps/V101.txt dataset-V101_stereo</p>
<h3 id="RGB-D"><a href="#RGB-D" class="headerlink" title="RGB-D"></a>RGB-D</h3><p>cd ./catkin_ws/src/ORB_SLAM3-dev_bugs_fixed</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM1.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg1_xyz/ ./Examples/RGB-D/associations/fr1_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_xyz/ ./Examples/RGB-D/associations/fr3_wlk_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_static/ ./Examples/RGB-D/associations/fr3_wlk_stc.txt</p>
<p>(sharon)<br>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM3.yaml /home/sharon/Documents/TUM/rgbd_dataset_freiburg3_walking_static/ ./Examples/RGB-D/associations/fr3_wlk_stc.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM3.yaml /home/sharon/Documents/TUM/rgbd_dataset_freiburg3_walking_xyz ./Examples/RGB-D/associations/fr3_wlk_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM1.yaml /home/sharon/Documents/TUM/rgbd_dataset_freiburg1_xyz/ ./Examples/RGB-D/associations/fr1_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM3.yaml /home/sharon/Documents/TUM/rgbd_dataset_freiburg3_nostructure_texture_far/ ./Examples/RGB-D/associations/fr3_nst_far.txt</p>
<h3 id="ORB-SLAM3-amp-YOLACT-的语义建图"><a href="#ORB-SLAM3-amp-YOLACT-的语义建图" class="headerlink" title="ORB_SLAM3&amp;YOLACT++的语义建图"></a>ORB_SLAM3&amp;YOLACT++的语义建图</h3><p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM1.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg1_xyz/ ./Examples/RGB-D/associations/fr1_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_static/ ./Examples/RGB-D/associations/fr3_wlk_stc.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_xyz/ ./Examples/RGB-D/associations/fr3_wlk_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_sitting_static/ ./Examples/RGB-D/associations/fr3_sit_stc.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_xyz/ ./Examples/RGB-D/associations/fr3_wlk_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_static/ ./Examples/RGB-D/associations/fr3_wlk_stc.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_rpy/ ./Examples/RGB-D/associations/fr3_wlk_rpy.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_halfsphere/ ./Examples/RGB-D/associations/fr3_wlk_half.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_sitting_xyz/ ./Examples/RGB-D/associations/fr3_sit_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_sitting_static/ ./Examples/RGB-D/associations/fr3_sit_stc.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_sitting_rpy/ ./Examples/RGB-D/associations/fr3_sit_rpy.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_sitting_halfsphere/ ./Examples/RGB-D/associations/fr3_sit_half.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt config/TUM2.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg2_desk_with_person/ ./Examples/RGB-D/associations/fr2_dsk_psn.txt</p>
]]></content>
      <categories>
        <category>ORB-SLAM</category>
      </categories>
      <tags>
        <tag>ORB-SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>ORB_SLAM2配置</title>
    <url>/2021/03/09/ORB_SLAM2%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h3 id="安装工具"><a href="#安装工具" class="headerlink" title="安装工具"></a>安装工具</h3><p>下载cmake、gcc、g++和git工具</p>
<p>下载cmake</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install cmake</span><br></pre></td></tr></table></figure>
<p>下载git</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install git</span><br></pre></td></tr></table></figure>
<p>下载gcc和g++</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install gcc g++  </span><br></pre></td></tr></table></figure>
<h3 id="安装Pangolin"><a href="#安装Pangolin" class="headerlink" title="安装Pangolin"></a>安装Pangolin</h3><p>先安装依赖项</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libglew-dev</span><br><span class="line">sudo apt-get install libpython2.7-dev</span><br></pre></td></tr></table></figure>
<p>下载Pangolin</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;stevenlovegrove&#x2F;Pangolin.git</span><br><span class="line">cd Pangolin</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake -DCPP11_NO_BOOSR&#x3D;1 ..</span><br></pre></td></tr></table></figure>
<p>开始编译</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo make -j8</span><br></pre></td></tr></table></figure>
<p>编译好了后安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<h3 id="安装OpenCV"><a href="#安装OpenCV" class="headerlink" title="安装OpenCV"></a>安装OpenCV</h3><p>我装的是opencv 3.2.0</p>
<p>1.先安装依赖包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install libgtk2.0-dev  </span><br><span class="line">sudo apt install pkg-config</span><br></pre></td></tr></table></figure>
<p>2.到opencv网站下载opencv 3.2.0</p>
<p><a href="https://link.zhihu.com/?target=https%3A//opencv.org/releases.html">https://link.zhihu.com/?target=https%3A//opencv.org/releases.html</a></p>
<p>3.开始安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">unzip opencv-3.2.0.zip</span><br><span class="line">cd opencv-3.2.0</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br></pre></td></tr></table></figure>
<p>4.开始编译</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cmake -D CMAKE_BUILD_TYPE&#x3D;Release -D CMAKE_INSTALL_PREFIX&#x3D;&#x2F;usr&#x2F;local -D ENABLE_PRECOMPILED_HEADERS&#x3D;OFF ..</span><br></pre></td></tr></table></figure>
<p>这里可能会出现 以下问题</p>
<p>Downloading ippicv_linux_20151201.tgz…<br>CMake Error at 3rdparty/ippicv/downloader.cmake:73 (file):<br>  file DOWNLOAD HASH mismatch</p>
<p>手动下载ippicv_linux_20151201.tgz，替换到opencv-3.2.0/3rdparty/ippicv/downloads/linux-808b791a6eac9ed78d32a7666804320e文件下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo make -j8</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>5.添加路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo vim &#x2F;etc&#x2F;ld.so.conf.d&#x2F;opencv.conf   </span><br></pre></td></tr></table></figure>
<p>内容：</p>
<p>/usr/local/lib</p>
<p>6.添加环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo vim ~&#x2F;.profile</span><br></pre></td></tr></table></figure>
<p>.profile最后一行添加：</p>
<p>PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig<br>export PKG_CONFIG_PATH</p>
<p>7.测试</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ..&#x2F;samples&#x2F;cpp&#x2F;example_cmake</span><br><span class="line">cmake .</span><br><span class="line">make</span><br><span class="line">.&#x2F;opencv_example</span><br></pre></td></tr></table></figure>
<p>结果如下说明opencv安装成功</p>
<p><img src="pic1.png" alt="1"></p>
<h3 id="安装Eigen3"><a href="#安装Eigen3" class="headerlink" title="安装Eigen3"></a>安装Eigen3</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libeigen3-dev</span><br></pre></td></tr></table></figure>
<h3 id="安装ROS-melodic"><a href="#安装ROS-melodic" class="headerlink" title="安装ROS-melodic"></a>安装ROS-melodic</h3><p>参考以下博文：</p>
<p><a href="https://blog.csdn.net/qq_41450811/article/details/99079041">https://blog.csdn.net/qq_41450811/article/details/99079041</a></p>
<p>但是在初始化rosdep的时候，遇到了大麻烦：</p>
<p>ERROR: cannot download default sources list from:<br><a href="https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/sources.list.d/20-default.list">https://raw.githubusercontent.com/ros/rosdistro/master/rosdep/sources.list.d/20-default.list</a><br>Website may be down.</p>
<p>网上修改hosts的方法也行不通，最后根据以下博文手动把包下载下来，并修改源码：</p>
<p><a href="https://blog.csdn.net/nanianwochengshui/article/details/105702188">https://blog.csdn.net/nanianwochengshui/article/details/105702188</a></p>
<h3 id="建立ORB-SLAM2"><a href="#建立ORB-SLAM2" class="headerlink" title="建立ORB-SLAM2"></a>建立ORB-SLAM2</h3><p>建立ROS工作区catkin_ws</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ~</span><br><span class="line">mkdir .&#x2F;catkin_ws</span><br><span class="line">cd catkin_ws</span><br><span class="line">mkdir src</span><br></pre></td></tr></table></figure>
<p>方法1：复制项目<br>cd ~/catkin_ws/src<br>git clone <a href="https://github.com/raulmur/ORB_SLAM2.git">https://github.com/raulmur/ORB_SLAM2.git</a> ORB_SLAM2</p>
<p>但是经常会下载不下来，所以可以用方法2</p>
<p>或者<br>方法2：将ORB_SLAM2项目下载到~/catkin_ws/src下，解压</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ORB_SLAM2</span><br><span class="line">chmod +x build.sh</span><br><span class="line">.&#x2F;build.sh</span><br></pre></td></tr></table></figure>
<p>这时候可能会出现以下问题：</p>
<p>error: ‘usleep’ was not declared in this scope<br>             usleep(3000);</p>
<p>方法：在./include/System.h文件中添加#include<unistd.h></p>
<h3 id="跑MONO-TUM数据集"><a href="#跑MONO-TUM数据集" class="headerlink" title="跑MONO_TUM数据集"></a>跑MONO_TUM数据集</h3><p>下载tum数据集<a href="http://vision.in.tum.de/data/datasets/rgbd-dataset/download，解压">http://vision.in.tum.de/data/datasets/rgbd-dataset/download，解压</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar zxvf rgbd_dataset_freiburg3_nostructure_texture_far.tgz -C .&#x2F;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd ~&#x2F;catkin_ws&#x2F;src&#x2F;ORB_SLAM2&#x2F;</span><br></pre></td></tr></table></figure>
<p>(youyx)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;Examples&#x2F;Monocular&#x2F;mono_tum Vocabulary&#x2F;ORBvoc.txt .&#x2F;Examples&#x2F;Monocular&#x2F;TUM3.yaml &#x2F;home&#x2F;youyx&#x2F;data&#x2F;tum&#x2F;rgbd_dataset_freiburg3_nostructure_texture_far</span><br></pre></td></tr></table></figure>
<p>(sharon)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;Examples&#x2F;Monocular&#x2F;mono_tum Vocabulary&#x2F;ORBvoc.txt Examples&#x2F;Monocular&#x2F;TUM3.yaml &#x2F;home&#x2F;sharon&#x2F;Documents&#x2F;rgbd_dataset_freiburg3_nostructure_texture_far&#x2F;</span><br></pre></td></tr></table></figure>
<p>(weip)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;Examples&#x2F;Monocular&#x2F;mono_tum Vocabulary&#x2F;ORBvoc.txt Examples&#x2F;Monocular&#x2F;TUM3.yaml &#x2F;home&#x2F;youyx&#x2F;data&#x2F;tum&#x2F;rgbd_dataset_freiburg3_nostructure_texture_far&#x2F;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod +x build_ros.sh</span><br><span class="line">.&#x2F;build_ros.sh</span><br></pre></td></tr></table></figure>
<p>(PS:运行到这里的时候才发现这台新电脑还没装ROS)</p>
<p>fatal error: Eigen3/Core: 没有那个文件或目录</p>
<p>建立软链接</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo ln -s &#x2F;usr&#x2F;include&#x2F;eigen3&#x2F;Eigen &#x2F;usr&#x2F;include&#x2F;Eigen</span><br></pre></td></tr></table></figure>
<p>重新</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;build_ros.sh</span><br></pre></td></tr></table></figure>
<p>又遇到问题：undefined reference to symbol ‘_ZN5boost6system15system_categoryEv’<br>方法：在Examples/ROS/ORB_SLAM2/文件夹下的CMakeLists.txt文件中<br>set(LIBS<br>${OpenCV_LIBS}<br>${EIGEN3_LIBS}<br>${Pangolin_LIBRARIES}<br>${PROJECT_SOURCE_DIR}/../../../Thirdparty/DBoW2/lib/libDBoW2.so<br>${PROJECT_SOURCE_DIR}/../../../Thirdparty/g2o/lib/libg2o.so<br>${PROJECT_SOURCE_DIR}/../../../lib/libORB_SLAM2.so<br>改为<br>set(LIBS<br>${OpenCV_LIBS}<br>${EIGEN3_LIBS}<br>${Pangolin_LIBRARIES}<br>${PROJECT_SOURCE_DIR}/../../../Thirdparty/DBoW2/lib/libDBoW2.so<br>${PROJECT_SOURCE_DIR}/../../../Thirdparty/g2o/lib/libg2o.so<br>${PROJECT_SOURCE_DIR}/../../../lib/libORB_SLAM2.so<br>-lboost_system<br>)</p>
<p>完成！</p>
<p>用ROS启动ORB_SLAM2</p>
<p>（sharon）</p>
<p>rosrun ORB_SLAM2 Mono /home/sharon/catkin_ws/src/ORB_SLAM2/Vocabulary/ORBvoc.txt /home/sharon/catkin_ws/src/ORB_SLAM2/Examples/Monocular/TUM2.yaml</p>
<p>（youyx）</p>
<p>rosrun ORB_SLAM2 Mono /home/sharon/catkin_ws/src/ORB_SLAM2/Vocabulary/ORBvoc.txt /home/sharon/catkin_ws/src/ORB_SLAM2/Examples/Monocular/TUM2.yaml</p>
<h3 id="跑RGBD-TUM数据集"><a href="#跑RGBD-TUM数据集" class="headerlink" title="跑RGBD_TUM数据集"></a>跑RGBD_TUM数据集</h3><p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM2.yaml /home/sharon/Documents/rgbd_dataset_freiburg2_pioneer_360 associations.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM2.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg2_desk_with_person/ ./Examples/RGB-D/associations/fr2_dsk_psn.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_walking_xyz/ ./Examples/RGB-D/associations/fr3_wlk_xyz.txt</p>
<p>./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt ./Examples/RGB-D/TUM3.yaml /home/youyx/data/datasets/TUM/rgbd_dataset_freiburg3_sitting_xyz/ ./Examples/RGB-D/associations/fr3_sit_xyz.txt</p>
]]></content>
      <categories>
        <category>ROS</category>
        <category>ORB_SLAM2</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>ORB_SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title>ROS-launch-camera</title>
    <url>/2021/03/08/ROS-launch-camera/</url>
    <content><![CDATA[<h2 id="ROS启动摄像头"><a href="#ROS启动摄像头" class="headerlink" title="ROS启动摄像头"></a>ROS启动摄像头</h2><h3 id="设置usb-cam节点"><a href="#设置usb-cam节点" class="headerlink" title="设置usb_cam节点"></a>设置usb_cam节点</h3><p>cd ~/catkin_ws/src<br>git clone <a href="https://github.com/bosch-ros-pkg/usb_cam.git">https://github.com/bosch-ros-pkg/usb_cam.git</a> usb_cam<br>cd ..<br>catkin_make</p>
<p>source ./devel/setup.bash</p>
<p>roslaunch usb_cam usb_cam-test.launch<br><img src="pic1.png" alt="1"></p>
<h3 id="编译ORB-SLAM2"><a href="#编译ORB-SLAM2" class="headerlink" title="编译ORB-SLAM2"></a>编译ORB-SLAM2</h3><p>1.将/home/sharon/catkin_ws/src/ORB_SLAM2/Examples/ROS/ORB_SLAM2/src的ros_mono.cc和/home/sharon/catkin_ws/src/ORB_SLAM2/Examples/ROS/ORB_SLAM2/src/AR的ros_mono_ar.cc中的ros::Subscriber sub = nodeHandler.subscribe(“/camera/image_raw”, 1, &amp;ImageGrabber::GrabImage,&amp;igb);<br>改为<br>ros::Subscriber sub = nodeHandler.subscribe(“/usb_cam/image_raw”, 1, &amp;ImageGrabber::GrabImage,&amp;igb);<br>因为ORB_SLAM默认订阅的话题为/camera/image_raw，而usb_cam节点发布的话题为/usb_cam/image_raw<br>2.改好之后再重新编译ORB_SLAM2<br>chmod +x build_ros.sh<br>./build_ros.sh</p>
<h3 id="运行单目摄像头节点"><a href="#运行单目摄像头节点" class="headerlink" title="运行单目摄像头节点"></a>运行单目摄像头节点</h3><p>roslaunch usb_cam usb_cam-test.launch<br>rosrun ORB_SLAM2 Mono /home/sharon/catkin_ws/src/ORB_SLAM2/Vocabulary/ORBvoc.txt /home/sharon/catkin_ws/src/ORB_SLAM2/Examples/Monocular/TUM1.yaml<br><img src="pic2.png" alt="2"></p>
]]></content>
      <categories>
        <category>ROS</category>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>ORB-SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title>ORB-SLAM2错误及解决方法</title>
    <url>/2021/03/08/ORB-SLAM2%E9%94%99%E8%AF%AF%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>在ubantu 14.04上跑ORB-SLAM2，遇到了一些问题记录下来。（后面师兄给我换了ubantu18.08，问题果然就少了很多）</p>
<h2 id="装OpenCV遇到的问题"><a href="#装OpenCV遇到的问题" class="headerlink" title="装OpenCV遇到的问题"></a>装OpenCV遇到的问题</h2><h3 id="1"><a href="#1" class="headerlink" title="1"></a>1</h3><p>问题：make编译时In file included from /home/youyx/Downloads/opencv-3.2.0/modules/core/src/hal_internal.cpp:49:0: /home/youyx/Downloads/opencv-3.2.0/build/opencv_lapack.h:2:45: fatal error: LAPACKE_H_PATHNOTFOUND/lapacke.h: No such file or directory #include “LAPACKE_H_PATH-NOTFOUND/lapacke.h”</p>
<p>方法：</p>
<p>sudo apt-get install liblapacke-dev checkinstall</p>
<p>然后将opencv-3.2.0/build/opencv_lapack.h第二行中的<code>#include&quot;LAPACKE_H_PATH-NOTFOUND/lapacke.h&quot;</code> 改为<code>#include&quot;lapacke.h&quot;</code></p>
<h3 id="2"><a href="#2" class="headerlink" title="2"></a>2</h3><p>问题：make编译时libopencv_highgui.so:undefined reference to `TIFFIsTiled@LIBTIFF_4.0’</p>
<p>原因：OpenCV需要libtiff4库，然而Ubuntu14.04系统安装不会自带libtiff4,因此当以OpenCV为接口时编译可能会出现这个问题。</p>
<p>方法：在cmake 编译OpenCV时</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cmake -D BUILD_TIFF=ON</span><br></pre></td></tr></table></figure>
<p>便会自动编译libtiff4，然后再以OpenCV做接口编译时，Bug消除</p>
<h3 id="3"><a href="#3" class="headerlink" title="3"></a>3</h3><p>问题：../../lib/libopencv_core.so.3.2.0: undefined reference to <code>dpotrf_&#39; ../../lib/libopencv_core.so.3.2.0: undefined reference to</code>dgesv_’<br>../../lib/libopencv_core.so.3.2.0: undefined reference to <code>sgels_&#39; ../../lib/libopencv_core.so.3.2.0: undefined reference to</code>sgesv_’<br>../../lib/libopencv_core.so.3.2.0: undefined reference to <code>sposv_&#39; ../../lib/libopencv_core.so.3.2.0: undefined reference to</code>dgetrf_’<br>../../lib/libopencv_core.so.3.2.0: undefined reference to <code>sgetrf_&#39; ../../lib/libopencv_core.so.3.2.0: undefined reference to</code>dgels_’<br>../../lib/libopencv_core.so.3.2.0: undefined reference to <code>dgeqrf_&#39; ../../lib/libopencv_core.so.3.2.0: undefined reference to</code>spotrf_’<br>../../lib/libopencv_core.so.3.2.0: undefined reference to <code>sgeqrf_&#39; ../../lib/libopencv_core.so.3.2.0: undefined reference to</code>sgesdd_’<br>../../lib/libopencv_core.so.3.2.0: undefined reference to <code>dgesdd_&#39; ../../lib/libopencv_core.so.3.2.0: undefined reference to</code>dposv_’</p>
<p>方法：在cmake 编译OpenCV时</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">-D WITH_LAPACK=OFF</span><br></pre></td></tr></table></figure>
<h3 id="4"><a href="#4" class="headerlink" title="4"></a>4</h3><p>问题：/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>PKCS12_PBE_add@OPENSSL_1.0.0&#39;                     
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>OCSP_basic_verify@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>BIO_puts@OPENSSL_1.0.0&#39;                           
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>SSL_get_peer_certificate@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>BIO_free@OPENSSL_1.0.0&#39;                           
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>SSLv3_client_method@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>ENGINE_get_id@OPENSSL_1.0.0&#39;                      
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>RAND_status@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>MD5_Final@OPENSSL_1.0.0&#39;                          
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>SSL_CTX_set_verify@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>ASN1_TIME_print@OPENSSL_1.0.0&#39;                    
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>ENGINE_ctrl@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>CONF_modules_free@OPENSSL_1.0.0&#39;                  
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>DES_set_key@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>SSL_set_session@OPENSSL_1.0.0&#39;                    
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>X509_EXTENSION_get_data@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>ERR_error_string_n@OPENSSL_1.0.0&#39;                 
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>OCSP_cert_status_str@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>SSL_CTX_free@OPENSSL_1.0.0&#39;                       
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>X509_check_issued@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>SSL_CTX_set_default_passwd_cb_userdata@OPENSSL_1.0
.0&#39;                                                                                                             
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>OCSP_RESPONSE_free@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>X509_get_pubkey@OPENSSL_1.0.0&#39;                    
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>CRYPTO_malloc@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>SSL_CTX_use_certificate_chain_file@OPENSSL_1.0.0&#39; 
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>SSLeay@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>UI_method_get_opener@OPENSSL_1.0.0&#39;               
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>X509_load_crl_file@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>ENGINE_free@OPENSSL_1.0.0&#39;                        
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>ASN1_STRING_type@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>ASN1_STRING_data@OPENSSL_1.0.0&#39;                   
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>i2t_ASN1_OBJECT@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>SSL_get_error@OPENSSL_1.0.0&#39;                      
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>X509_NAME_get_entry@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>SSL_CTX_add_client_CA@OPENSSL_1.0.0&#39;              
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>SSL_get_privatekey@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>OPENSSL_load_builtin_modules@OPENSSL_1.0.0&#39;       
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>CRYPTO_free@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>RAND_add@OPENSSL_1.0.0&#39;                           
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>ASN1_STRING_length@OPENSSL_1.0.0’<br>/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to <code>SSL_CIPHER_get_name@OPENSSL_1.0.0&#39;                
/usr/lib/x86_64-linux-gnu/libcurl.so: undefined reference to</code>ERR_peek_error@OPENSSL_1.0.0’                     </p>
<p>方法：anaconda的库和系统默认的库冲突了，我暴力地把anaconda的文件名改了哈哈哈哈哈哈哈哈哈哈哈哈</p>
<h3 id="5-ubantu18-04"><a href="#5-ubantu18-04" class="headerlink" title="5(ubantu18.04)"></a>5(ubantu18.04)</h3><p>问题：error: ‘usleep’ was not declared in this scope<br>             usleep(3000);</p>
<p>方法：在/include/System.h文件中添加#include<unistd.h></p>
<h3 id="6"><a href="#6" class="headerlink" title="6"></a>6</h3><p>问题：build_ros.sh遇到undefined reference to symbol ‘_ZN5boost6system15system_categoryEv’<br>方法：在Examples/ROS/ORB_SLAM2/文件夹下的CMakeLists.txt文件中<br>set(LIBS<br>${OpenCV_LIBS}<br>${EIGEN3_LIBS}<br>${Pangolin_LIBRARIES}<br>${PROJECT_SOURCE_DIR}/../../../Thirdparty/DBoW2/lib/libDBoW2.so<br>${PROJECT_SOURCE_DIR}/../../../Thirdparty/g2o/lib/libg2o.so<br>${PROJECT_SOURCE_DIR}/../../../lib/libORB_SLAM2.so<br>改为<br>set(LIBS<br>${OpenCV_LIBS}<br>${EIGEN3_LIBS}<br>${Pangolin_LIBRARIES}<br>${PROJECT_SOURCE_DIR}/../../../Thirdparty/DBoW2/lib/libDBoW2.so<br>${PROJECT_SOURCE_DIR}/../../../Thirdparty/g2o/lib/libg2o.so<br>${PROJECT_SOURCE_DIR}/../../../lib/libORB_SLAM2.so<br>-lboost_system<br>)</p>
<h3 id="7"><a href="#7" class="headerlink" title="7"></a>7</h3><p>问题：[ERROR] [1571614395.918484166]: [registerPublisher] Failed to contact master at [localhost:11311]<br>方法：ros服务没开，再开一个终端输入<br>roscore</p>
<h3 id="8"><a href="#8" class="headerlink" title="8"></a>8</h3><p>ORB-SLAM2编译报错：rospack found package “ORB_SLAM2” at “/opt/ros/kinetic/share/ORB_SLAM2”, but the….</p>
<p><a href="https://blog.csdn.net/weixin_44401286/article/details/102752767">https://blog.csdn.net/weixin_44401286/article/details/102752767</a></p>
<p>“原因分析及解决办法：<br>这是由于在ROS环境下编译使用了多个版本的ORB-SLAM2工程造成的，比如我运行了原版的ORB-SLAM2，然后拷贝了一份改了下文件名字，添加稠密重建模块，再对新的工程编译，就报了这个错误。因为原工程已经在/opt/ros/kinetic/share文件夹下建立了一个软连接ORB_SLAM2，这里包含有原工程的一些信息，我们需要做的就是把该软连接，替换为现在工程的软连接。”<br>(1)先cd到/opt/ros/kinetic/share目录下，删除原来的ORB_SLAM2软连接</p>
<p>sudo rm -r ORB_SLAM2</p>
<p>(2)对当前工程在/opt/ros/kinetic/share目录下建立软连接</p>
<p>sudo ln -s /home/bruce/study/slam/orb/point_map/Examples/ROS/ORB_SLAM2 /opt/ros/kinetic/share/ORB_SLAM2</p>
<h3 id="9"><a href="#9" class="headerlink" title="9"></a>9</h3><p>问题：catkin_make的时候’libavcodec’没找到</p>
<p>Checking for module ‘libavcodec’<br>—   No package ‘libavcodec’ found<br>CMake Error at /usr/local/share/cmake-3.7/Modules/FindPkgConfig.cmake:415 (message):<br>  A required package was not found<br>Call Stack (most recent call first):<br>  /usr/local/share/cmake-3.7/Modules/FindPkgConfig.cmake:588 (_pkg_check_modules_internal)<br>  usb_cam/CMakeLists.txt:11 (pkg_check_modules)</p>
<p>方法：1.sudo apt install libavcodec-dev</p>
<p>如果1没用，那可能是它不在PKG_CONFIG_PATH，所以把它加进来。</p>
<p>echo $PKG_CONFIG_PATH</p>
<p>果然不在，那么</p>
<p>export PKG_CONFIG_PATH=/usr/lib/x86_64-linux-gnu/pkgconfig/:$PKG_CONFIG_PATH</p>
<h3 id="10"><a href="#10" class="headerlink" title="10"></a>10</h3><p>若在CMake中遇到该nullptr问题：</p>
<p>要在CMake中使用C++11，只要在CMakeLists.txt中添加一行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">add_definitions(-std&#x3D;c++11)</span><br></pre></td></tr></table></figure>
<p>OR</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SET(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std&#x3D;c++0x&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="11"><a href="#11" class="headerlink" title="11"></a>11</h3><p>运行 # rosrun robot_sim_demo robot_keyboard_teleop.py 时出现错误：</p>
<p>[rosrun] Found the following, but they’re either not files, [rosrun] or not</p>
<p>这是因为robot_keyboard_teleop.py权限不足</p>
<p>chmod +x /home/youyx/catkin_ws/src/ROS-Academy-for-Beginners/robot_sim_demo/scripts/robot_keyboard_teleop.py</p>
]]></content>
      <categories>
        <category>ROS</category>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>ORB-SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title>【转载】三维数据集整理</title>
    <url>/2021/03/02/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E4%B8%89%E7%BB%B4%E6%95%B0%E6%8D%AE%E9%9B%86%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>毕设需要用到视频序列的三维数据集，刚好看到一篇文章做了整理，就转了过来。</p>
<p>《Review: deep learning on 3D point clouds》<a href="https://arxiv.org/pdf/2001.06280.pdf">https://arxiv.org/pdf/2001.06280.pdf</a></p>
<h2 id="基准数据集"><a href="#基准数据集" class="headerlink" title="基准数据集"></a>基准数据集</h2><p>  近年来，已经发表了大量的点云数据集。现有的大部分数据集都是由大学和行业提供的。它们可以为测试各种方法提供一个公平的比较。这些公共基准数据集由虚拟场景或真实场景组成，其重点在于特别是在点云分类、分割、检索方面和目标检测。它们用深度学习方法特别有用，因为它们可以为训练网络提供大量的真实标签。点云可以通过不同的平台/方法获得，如Structure from Motion (SfM)、红绿蓝-深度(RGB-D)相机和光探测和测距（LiDAR）系统。随着大小和复杂程度增加，基准数据集的可用性通常会降低。在本节中，我们将介绍一些流行的用于3D研究的数据集。</p>
<h3 id="3D模型数据集"><a href="#3D模型数据集" class="headerlink" title="3D模型数据集"></a>3D模型数据集</h3><p><strong>ModelNet [13]：</strong>该数据集由普林斯顿大学视觉与机器人实验室开发。ModelNet40有40个人造物体形状的类别（如飞机、书架和椅子）用于分类和识别。它由12311个CAD模型组成。其中分为9,843个训练和2,468个测试形状。ModelNet10数据集是ModelNet40的一个子集，该子集包括只包含10个类别。它还分为3991个训练和908个测试形状。</p>
<p><strong>ShapeNet [48]：</strong>大规模数据集是由斯坦福大学等人开发，它提供了每个模型的语义类别标签，固定的走线、部件和双边对称性平面、物理尺寸、关键词，以及其他计划中的注解。ShapeNet已经为近3,000,000个模型编制了索引。当数据集公布后，有22万个模型被分为3135个类别。ShapeNetCore是ShapeNet的一个子集，其中包括近51,300个独特的3D模型。它提供了55个常见的对象类别和注释。ShapeNetSem也是ShapeNet的一个子集，它包含有12,000个模型。它的规模更小，但覆盖面更广，包括270类。</p>
<p><strong>Augmenting ShapeNet：</strong>[49]创建了详细的部件标签为来自ShapeNetCore数据集的31963个模型提供服务。它提供了16个形状类别进行部件分割。[50]已经提供了1200个来自ShapeNet数据集的虚拟局部模型。[51]提出了一种自动生成逼真3D形状的方法，它是建立在ShapeNetCore数据集。[52]是一个具有细粒度和层次性的部件注释的大规模数据集，它包括24个对象类别和26,671个三维模型，提供了573,585个部件实例标签。[53]贡献了一个大规模的3D物体识别数据集。该数据集有100个类别，其中包括有201,888个对象（来自ImageNet[54]）的90,127张图像和44,147个3D形状（来自ShapeNet）。</p>
<p><strong>Shape2Motion [55]：</strong>Shape2Motion是由北航和国防科技大学开发的。它已经创建了一个新的基准数据集，用于3D形状流动性分析。该基准包括45个形状类别与2440个模型，其中的形状是来自于ShapeNet和3D仓库[56]。所提出的方法输入单一的3D形状，然后预测运动部件的分割结果和运动对应的共同属性。</p>
<p><strong>ScanObjectNN [57]：</strong>ScanObjectNN是由香港科技大学等单位提出，是第一个点云分类的真实世界数据集。约15,000对象是从室内数据集选择出的（SceneNN[58]和ScanNet[30]）。 并将对象分为15类，其中有2902个唯一的对象实例。</p>
<h3 id="3D室内数据集"><a href="#3D室内数据集" class="headerlink" title="3D室内数据集"></a>3D室内数据集</h3><p><strong>NYUDv2 [59]：</strong>纽约大学深度数据集v2（NYUDv2）是由纽约大学等人开发的。该数据集提供了从464个各种室内场景中捕捉到的1449张RGB-D（由Kinect v1获得）图像。所有的图像是分布式分割标签。这个数据集主要是有助于了解3D线索对于室内物体如何产生更好的分割。</p>
<p><strong>SUN3D [60]：</strong>该数据集由普林斯顿大学开发。这是一个RGB-D视频数据集，其中的视频是从41栋建筑的254个不同空间中捕捉到的。SUN3D提供了415个带有摄像机姿势和物体标签的序列。点云是由运动结构(SfM)生成的。</p>
<p><strong>S3DIS [61]：</strong>斯坦福大学3D大型室内空间（S3DIS）是由斯坦福大学等人开发的，S3DIS是从3栋不同的建筑物271个房间中收集到的数据。覆盖面积在6000平方米以上。它包含超过2.15亿点，并且每个点都提供了实例级语义分割标签（13个类别）；</p>
<p><strong>SceneNN [58]：</strong>新加坡科技与设计大学等人开发了这个数据集。SceneNN是一个RGB-D（获得Kinect v2)场景数据集，它收集了101个室内场景的数据。<br>  它为室内场景提供了40个语义类，并且所有的语义标签与NYUDv2数据集相同。</p>
<p><strong>ScanNet [30]：</strong>ScanNet是一个大尺度的室内数据集，它的开发目的是为了让人们能够更清楚地了解自己的生活。由斯坦福大学等人拍摄，包含1513个扫描场景。包括近2.5M的RGB-D（由 Occipital Structure公司获得传感器）图像，来自707个不同的室内环境。该数据集为三维物体分类提供了地面真实标签。17个类别，语义分割有20个类别类别：<br>  对于对象分类，ScanNet将所有的实例划分为9,677个实例用于训练，2,606个实例用于测试。而且ScanNet将所有扫描分成1201个训练场景和312个测试场景进行语义分割。</p>
<p><strong>Matterport3D [62]：</strong>Matterport3D是普林斯顿大学等人开发的最大的室内数据集。的。该数据集的覆盖面积面积为来自2056个房间的219,399mm2，，建筑面积为46,561mm2。它包括10,800个全景视图，其中视图来自90大型建筑的194,400张RGB-D图像。标签包含表面重建、摄像机姿势和语义分割。这个数据集研究现场理解的5个任务，分别是关键点匹配、视图重叠预测、表面法线估计、区域类型分类，以及语义分割。</p>
<p><strong>3DMatch [63]：</strong>这个基准数据集是由普林斯顿大学等，它是现有数据集的一个大集合。，如Analysisby-Synthesis[64]、7-cenes[65]。SUN3D[60]、RGB-D Scenes v.2[66]和Halber等人[67]。3DMatch基准由62个场景组成，分别为54个训练场景和8个测试场景。它利用对应标签从RGB-D场景重建数据集，然后提供点云检索的地面真相标签。</p>
<p><strong>Multisensor Indoor Mapping and Positioning Dataset [68]：</strong>这个室内数据集（房间、走廊和室内停车场）是由厦门大学等人开发的。由多传感器获得，如激光扫描仪、摄像头、WIFI、蓝牙和IMU。该数据集提供了密集的激光扫描点云进行室内测绘和定位。同时。他们还提供基于多传感器校准的彩色激光扫描和SLAM映射过程。</p>
<h3 id="3D室外数据集"><a href="#3D室外数据集" class="headerlink" title="3D室外数据集"></a>3D室外数据集</h3><p><strong>KITTI [69] [70]：</strong>KITTI数据集是在自动驾驶领域最著名的数据之一。它是由卡尔斯鲁厄理工学院等开发的，可用于立体图像、光流估计、三维检测的研究、三维跟踪、视觉测距等。数据采集平台配备了两台彩色摄像机，两台灰度相机，一台Velodyne HDL-64E 3D激光扫描仪和一个高精度的GPS/IMU系统。KITTI提供原始数据有道路、城市、住宅、校园和人等五类。深度完成和预测基准包括93000多张深度图。3D物体检测基准包含7481个训练点云和7518个测试点云。视觉测距基准由22个序列组成，有11个序列（00-10）激光雷达数据进行训练和11序列（11-21）激光雷达数据进行测试。同时，最近发表了Kitti里程数据集的语义标注[71]。SemanticKITTI包含28个类，包括地、建筑、车辆、自然、人、物等。</p>
<p><strong>ASL Dataset [72]：</strong>这组数据集是由苏黎世联邦理工学院收集于2011年8月之间至2012年1月。它提供了由北洋UTM-30LX获得的8个点云序列。每个序列约有35次扫描点云和由GPS/INS系统支持的真实姿势。该数据集涵盖了结构化和非结构化环境的领域。<br>iQmulus [73]：由Mines ParisTech等于2013年1月开发了大规模城市场景数据集。整个3D点云已被分类并划分为50个类。数据是由StereopolisII MLS收集的，该系统是法国国家测绘局（IGN）开发的一个系统。他们使用Riegl LMS-Q120i传感器采集3亿个点。</p>
<p><strong>Oxford Robotcar [74]：</strong>这个数据集是由牛津大学开发的。它由2014年5月至2015年12月期间穿过牛津市中心的大约100次轨迹组成(共101,046公里的轨迹)。这一长期数据集捕捉到了许多具有挑战性的环境变化，包括季节、天气、交通等等。而数据集既提供了图像、激光雷达点云、全球定位系统和用于自动汽车的INS地面实况。LIDRA的数据是由两个SICK LMS-151 2D激光雷达扫描仪和一台SICK LD-MRS 3D激光雷达扫描仪获得。<br>NCLT [75]：它是由密歇根大学开发的。它含有27次于2012年1月至2013年4月期间通过密歇根大学北校区的轨迹。该数据集还提供了图像、激光雷达、全球定位系统和用于长期自动汽车的INS地面数据。LiDRA点云是由Velodyne-32激光雷达扫描仪收集的。</p>
<p><strong>Semantic3D [76]：</strong>由苏黎世联邦理工学院开发了高质量和高密度的数据集。它包含了超过40亿的采集点云的点位，通过静态地面观测获得。激光扫描仪提供了8个语义类，其中由人工地形、自然地形、高植被、低植被、建筑物、硬地貌、扫描文物和汽车等组成。而数据集被分为15个训练场景和15个测试场景。</p>
<p><strong>DBNet [77]：</strong>这个真实世界的LiDAR-视频数据集是由厦门大学等单位开发的，旨在学习驾驶策略，因此它与以往的户外数据集不同。DBNet提供激光雷达点云、视频记录、GPS和用于进行驾驶行为研究的驾驶员行为。它包含了1,000公里的被Velodyne激光器采集的驾驶数据。</p>
<p><strong>NPM3D [78]：</strong> NPM3D数据集是由PSL研究大学开发的。它是一个点云分类和分割的基准。所有的点云都被标记为50个不同的类别。它包含了在巴黎和里尔收集的1431万个点数据。该数据是由包括Velodyne HDL-32E激光雷达和GPS/INS系统的移动激光系统采集的。</p>
<p><strong>Apollo [79] [80]：</strong>Apollo是由百度研究开发的。它是一个大规模的自动驾驶数据集。它提供三维汽车实例理解的标签数据、激光雷达点云物体的检测和跟踪，以及基于激光雷达的定位。对于3D汽车实例理解任务，有5277张图片，6万多辆汽车实例。每辆车都有一个工业级的CAD模型。3D物体检测和跟踪基准数据集包含53分钟的训练序列和50分钟测试序列。它是在帧率为10fps/秒，标注的帧率为2fps/秒的情况下采集的。阿波罗-南湾数据集（Apollo-SouthBay dataset）提供了关于定位任务的激光雷达帧数据。它是在旧金山湾南部采集的。他们在标准林肯MKZ轿车上配备了高端的自动驾驶传感器装备(Velodyne HDL-64E、NovAtel ProPak6和IMU-ISA-100C)。</p>
<p><strong>nuScenes [81]：</strong>nuTonomy场景(nuScenes)数据集提出了一种新的三维物体检测指标，它是由nuTonomy（APTIV公司）提供。该指标由多个方面组成，分别是分类、速度、大小、定位、方向，以及对象的属性估计。这数据集是由自主车辆传感器装备在360度视野下获取的（ 6个摄像机、5个雷达和1个激光雷达）。它包含从波士顿和新加坡收集的1000个驾驶场景。其中，两座城市都是交通堵塞。在这个数据集中的对象有23个类和8个属性，它们都是标有三维边界框。</p>
<p><strong>BLVD [82]：</strong>该数据集由西安交通大学开发。并被收藏在常熟（中国）。它介绍了一个新的基准，它专注于动态4D对象跟踪、5D交互式事件识别和5D意图预测。BLVD数据集由654个视频片段组成，其中视频为120k帧，帧率为10fps/秒。所有的帧都是被注释，获得了249129个3D注释。有总共4 902个特定的跟踪对象，6 004个交互式事件识别片段，以及4900个对象用于意图预测。</p>
<p><img src="pic1.png" alt="1"></p>
<p>表1：基准数据集的分类（cls:分类，seg:分割，loc:定位，reg:配准，aut:自动驾驶，det:目标检测，dri:驾驶行为，mot:运动估计，odo:里程计）</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[13] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, J. Xiao, 3d shapenets: A deep representation for volumetric shapes, in: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, IEEE Computer Society, 2015, pp. 1912–1920. URL: <a href="https://doi.org/10.1109/CVPR.2015.7298801">https://doi.org/10.1109/CVPR.2015.7298801</a>. doi:10.1109/CVPR. 2015.7298801.</p>
<p>[30] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, M. Nießner, Scannet: Richly-annotated 3d reconstructions of indoor scenes, in: Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017.</p>
<p>[48] A. X. Chang, T. A. Funkhouser, L. J. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, F. Yu, Shapenet: An informationrich 3d model repository, CoRR abs/1512.03012 (2015). URL: <a href="http://arxiv.org/abs/1512.03012">http://arxiv.org/abs/1512.03012</a>. arXiv:1512.03012.</p>
<p>[49] L. Yi, V. G. Kim, D. Ceylan, I. Shen, M. Yan, H. Su, C. Lu, Q. Huang, A. Sheffer, L. Guibas, et al., A scalable active framework for region annotation in 3d shape collections, ACM Transactions on Graphics (TOG) 35 (2016) 210.</p>
<p>[50] A. Dai, C. R. Qi, M. Nießner, Shape completion using 3d-encoder-predictor cnns and shape synthesis, in: Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017.</p>
<p>[51] K. Park, K. Rematas, A. Farhadi, S. M. Seitz, Photoshape: Photorealistic materials for large-scale shape collections, ACM Trans. Graph. 37 (2018).</p>
<p>[52] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J. Guibas, H. Su, PartNet: A large-scale benchmark for fine-grained and hierarchical part-level 3D object understanding, in: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.<br>[53] Y. Xiang, W. Kim, W. Chen, J. Ji, C. Choy, H. Su, R. Mottaghi, L. Guibas, S. Savarese, Objectnet3d: A large scale database for 3d object recognition, in: European Conference Computer Vision (ECCV), 2016.</p>
<p>[54] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A large-scale hierarchical image database, in: 2009 IEEE conference on computer vision and pattern recognition, Ieee, 2009, pp. 248–255.</p>
<p>[55] X. Wang, B. Zhou, Y. Shi, X. Chen, Q. Zhao, K. Xu, Shape2motion: Joint analysis of motion parts and attributes from 3d shapes, in: CVPR, 2019, p. to appear.</p>
<p>[56] 3d warehouse, ???? URL: <a href="https://3dwarehouse.sketchup.com/">https://3dwarehouse.sketchup.com/</a>.</p>
<p>[57] M. A. Uy, Q.-H. Pham, B.-S. Hua, D. T. Nguyen, S.K. Yeung, Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data, in: International Conference on Computer Vision (ICCV), 2019.</p>
<p>[58] B.-S. Hua, Q.-H. Pham, D. T. Nguyen, M.-K. Tran, L.-F. Yu, S.-K. Yeung, Scenenn: A scene meshes dataset with annotations, in: International Conference on 3D Vision (3DV), 2016.</p>
<p>[59] N. Silberman, D. Hoiem, P. Kohli, R. Fergus, Indoor segmentation and support inference from rgbd images, in: European Conference on Computer Vision, Springer, 2012, pp. 746–760.</p>
<p>[60] J. Xiao, A. Owens, A. Torralba, Sun3d: A database of big spaces reconstructed using sfm and object labels, in: Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 1625–1632.</p>
<p>[61] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. K. Brilakis, M. Fischer, S. Savarese, 3d semantic parsing of large-scale indoor spaces, in: 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, IEEE Computer Society, 2016, pp. 1534–1543. URL: <a href="https://doi.org/10.1109/CVPR.2016.170">https://doi.org/10.1109/CVPR.2016.170</a>. doi:10.1109/CVPR.2016. 170.</p>
<p>[62] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, Y. Zhang, Matter port3d: Learning from rgb-d data in indoor environments, International Conference on 3D Vision (3DV) (2017).</p>
<p>[63] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, T. Funkhouser, 3dmatch: Learning local geometric descriptors from rgb-d reconstructions, in: CVPR, 2017.</p>
<p>[64] J. Valentin, A. Dai, M. Nießner, P. Kohli, P. Torr, S. Izadi, C. Keskin, Learning to navigate the energy landscape, arXiv preprint arXiv:1603.05772 (2016).</p>
<p>[65] J. Shotton, B. Glocker, C. Zach, S. Izadi, A. Criminisi, A. Fitzgibbon, Scene coordinate regression forests for camera relocalization in rgb-d images, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 2930–2937.</p>
<p>[66] M. De Deuge, A. Quadros, C. Hung, B. Douillard, Unsupervised feature learning for classification of outdoor 3d scans, in: Australasian Conference on Robitics and Automation, volume 2, 2013, p. 1.</p>
<p>[67] M. Halber, T. A. Funkhouser, Structured global registration of rgb-d scans in indoor environments, ArXiv abs/1607.08539 (2016).</p>
<p>[68] C. Wang, S. Hou, C. Wen, Z. Gong, Q. Li, X. Sun, J. Li, Semantic line framework-based indoor building modeling using backpacked laser scanning point cloud, ISPRS journal of photogrammetry and remote sensing 143 (2018) 150–166.</p>
<p>[69] A. Geiger, P. Lenz, R. Urtasun, Are we ready for autonomous driving? the kitti vision benchmark suite, in: Conference on Computer Vision and Pattern Recognition (CVPR), 2012.</p>
<p>[70] A. Geiger, P. Lenz, C. Stiller, R. Urtasun, Vision meets robotics: The kitti dataset, International Journal of Robotics Research (IJRR) (2013).</p>
<p>[71] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, J. Gall, SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences, in: Proc. of the IEEE/CVF International Conf. on Computer Vision (ICCV), 2019.</p>
<p>[72] F. Pomerleau, M. Liu, F. Colas, R. Siegwart, Challenging data sets for point cloud registration algorithms, The International Journal of Robotics Research 31 (2012) 1705–1711.</p>
<p>[73] M. Br´edif, B. Vallet, A. Serna, B. Marcotegui, N. Paparoditis, Terramobilita/iqmulus urban point cloud classification benchmark, 2014.</p>
<p>[74] W. Maddern, G. Pascoe, C. Linegar, P. Newman, 1 Year, 1000km: The Oxford RobotCar<br>Dataset, The International Journal of Robotics Research (IJRR) 36 (2017) 3–15. URL: <a href="http://dx.doi.org/10.1177/0278364916679498">http://dx.doi.org/10.1177/0278364916679498</a>. doi:10.1177/0278364916679498.</p>
<p>[75] N. Carlevaris-Bianco, A. K. Ushani, R. M. Eustice, University of michigan north campus long-term vision and lidar dataset, The International Journal of Robotics Research 35 (2016) 1023–1035.</p>
<p>[76] T. Hackel, N. Savinov, L. Ladicky, J. D. Wegner, K. Schindler, M. Pollefeys, Semantic3d. net: A new large-scale point cloud classification benchmark, arXiv preprint arXiv:1704.03847 (2017).</p>
<p>[77] Y. Chen, J. Wang, J. Li, C. Lu, Z. Luo, H. Xue, C. Wang, Lidar-video driving dataset: Learning driving policies effectively, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 5870–5878.</p>
<p>[78] X. Roynard, J.-E. Deschaud, F. Goulette, Paris-lille-3d: A large and high-quality ground-truth urban point cloud dataset for automatic segmentation and classi-fication, The International Journal of Robotics Research 37 (2018) 545–557. URL: <a href="https://doi.org/10.1177/0278364918767506">https://doi.org/10.1177/0278364918767506</a>. doi:10.1177/0278364918767506.</p>
<p>[79] X. Song, P. Wang, D. Zhou, R. Zhu, C. Guan, Y. Dai, H. Su, H. Li, R. Yang, Apollocar3d: A large 3d car instance understanding benchmark for autonomous driving, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 5452–5462.</p>
<p>[80] W. Lu, Y. Zhou, G. Wan, S. Hou, S. Song, L3-net: Towards learning based lidar localization for autonomous driving, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp.6389–6398.</p>
<p>[81] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, O. Beijbom, nuscenes: A multimodal dataset for autonomous driving, arXiv preprint arXiv:1903.11027 (2019).</p>
<p>[82] J. Xue, J. Fang, T. Li, B. Zhang, P. Zhang, Z. Ye, J. Dou, BLVD: Building a large-scale 5d semantics benchmark for autonomous driving, in: Proc. International Conference on Robotics and Automation, in press, 2019.</p>
]]></content>
      <categories>
        <category>数据集</category>
      </categories>
      <tags>
        <tag>实例分割</tag>
        <tag>深度学习</tag>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title>Gazebo仿真场景搭建+配置</title>
    <url>/2021/01/31/Gazebo%E4%BB%BF%E7%9C%9F%E5%9C%BA%E6%99%AF%E6%90%AD%E5%BB%BA+%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h2 id="搭建场景"><a href="#搭建场景" class="headerlink" title="搭建场景"></a>搭建场景</h2><h3 id="打开Gazebo"><a href="#打开Gazebo" class="headerlink" title="打开Gazebo"></a>打开Gazebo</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">gazebo</span><br></pre></td></tr></table></figure>
<h3 id="打开建筑编辑器"><a href="#打开建筑编辑器" class="headerlink" title="打开建筑编辑器"></a>打开建筑编辑器</h3><p>点击“Edit”-&gt;“Building Editor”或者使用快捷键“ctrl+B”</p>
<p><img src="pic1.png" alt="pic1"></p>
<h3 id="图形界面"><a href="#图形界面" class="headerlink" title="图形界面"></a>图形界面</h3><ul>
<li>左栏可以选择建筑材料和特征</li>
<li>上方的界面是二维视图，导入的floor plan可以在这里看到</li>
<li>下方的界面是三维视图，能够预览建筑</li>
</ul>
<h3 id="导入floor-plan"><a href="#导入floor-plan" class="headerlink" title="导入floor plan"></a>导入floor plan</h3><p>导入一个建筑模板</p>
<p>1.点击左栏的“import”</p>
<p>2.选择电脑中的一张floor plan图片</p>
<p><img src="pic2.png" alt="pic2"></p>
<p>3.设置尺度</p>
<p><img src="D:\yyx\Hexo-Blog\source\_posts\Gazebo仿真场景搭建+配置\pic3.png" alt="pic3"></p>
<p>在图片中标记一段直线，并在左边输入其实际长度</p>
<p>4.图片出现在二维界面中</p>
<h3 id="添加墙、窗户和门"><a href="#添加墙、窗户和门" class="headerlink" title="添加墙、窗户和门"></a>添加墙、窗户和门</h3><p><img src="pic5.png" alt="pic5"></p>
<p>添加窗户和门的时候有个bug，只能添加横向的窗和门，若添加纵向的gazebo就会闪退，关闭硬件加速或是升级gazebo都没解决。</p>
<h3 id="编辑建筑"><a href="#编辑建筑" class="headerlink" title="编辑建筑"></a>编辑建筑</h3><p>双击墙、窗户和门，或者右键选择，即出现参数框</p>
<p><img src="pic6.png" alt="pic6"></p>
<p>可以调整位置、大小，还可以设置墙的颜色和纹理</p>
<h3 id="保存建筑"><a href="#保存建筑" class="headerlink" title="保存建筑"></a>保存建筑</h3><p>保存会创建一个建筑的目录，SDF和配置文件。</p>
<p>点击“File”-&gt;“Save”（ctrl+s）。注意：保存的位置要在gazebo/models下</p>
<p><img src="pic7.png" alt="pic7"></p>
<p>保存后就可以退出，一旦退出这个建筑就不能再编辑，只能往里面插入模型。</p>
<p><img src="pic8.png" alt="pic8"></p>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>在/usr/share/gazebo-9/worlds目录下创建house.world文件，添加以下代码</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; ?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">sdf</span> <span class="attr">version</span>=<span class="string">&quot;1.5&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">world</span> <span class="attr">name</span>=<span class="string">&quot;default&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">include</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">uri</span>&gt;</span>model://ground_plane<span class="tag">&lt;/<span class="name">uri</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">include</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">uri</span>&gt;</span>model://sun<span class="tag">&lt;/<span class="name">uri</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">include</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">uri</span>&gt;</span>model://house<span class="tag">&lt;/<span class="name">uri</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">world</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">sdf</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>在/opt/ros/melodic/share/gazebo_ros/launch目录下创建house.launch文件，添加以下代码</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">launch</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- We resume the logic in empty_world.launch, changing only the name of the world to be launched --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">include</span> <span class="attr">file</span>=<span class="string">&quot;$(find gazebo_ros)/launch/empty_world.launch&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;world_name&quot;</span> <span class="attr">value</span>=<span class="string">&quot;worlds/house.world&quot;</span>/&gt;</span> <span class="comment">&lt;!-- <span class="doctag">Note:</span> the world_name is with respect to GAZEBO_RESOURCE_PATH environmental variable --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;paused&quot;</span> <span class="attr">value</span>=<span class="string">&quot;false&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;use_sim_time&quot;</span> <span class="attr">value</span>=<span class="string">&quot;true&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;gui&quot;</span> <span class="attr">value</span>=<span class="string">&quot;true&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;headless&quot;</span> <span class="attr">value</span>=<span class="string">&quot;false&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">arg</span> <span class="attr">name</span>=<span class="string">&quot;debug&quot;</span> <span class="attr">value</span>=<span class="string">&quot;false&quot;</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">launch</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>在命令行中输入</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">roslaunch gazebo_ros house.launch</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>gazebo</category>
      </categories>
      <tags>
        <tag>gazebo</tag>
        <tag>ROS</tag>
        <tag>仿真</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLACT</title>
    <url>/2021/01/30/YOLACT/</url>
    <content><![CDATA[<h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>因为项目的需要，就试着跑了YOLACT，把过程记录在这儿。</p>
<p><strong>YOLACT：</strong><a href="https://github.com/dbolya/yolact">https://github.com/dbolya/yolact</a> </p>
<h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><h3 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a>创建虚拟环境</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda create -n yolact python=3.7</span><br><span class="line"><span class="built_in">source</span> activate</span><br><span class="line">conda activate yolact</span><br></pre></td></tr></table></figure>
<ul>
<li>用conda创建一个yolact虚拟环境，这样可以搭建独立的python运行环境，使的每个项目的运行互不影响；</li>
<li>激活该虚拟环境。</li>
<li>add|新电脑上source ~/anaconda3/bin/activate</li>
</ul>
<h3 id="安装pytorch和其他的包"><a href="#安装pytorch和其他的包" class="headerlink" title="安装pytorch和其他的包"></a>安装pytorch和其他的包</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda install pytorch torchvision cudatoolkit=10.0 </span><br><span class="line">pip install cython opencv-python pillow  matplotlib </span><br></pre></td></tr></table></figure>
<ul>
<li>cudatoolkit=x.x取决cuda版本，因为我用的是师兄的服务器，cuda是10.0的版本，所以cudatoolkit=10.0；</li>
<li>conda install的好处是能够根据cudatoolkit的版本安装相对应版本的pytorch和torchvision，不用我们自己匹配。</li>
</ul>
<h3 id="安装COCOAPI"><a href="#安装COCOAPI" class="headerlink" title="安装COCOAPI"></a>安装COCOAPI</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> cocoapi/PythonAPI</span><br><span class="line">python setup.py build_ext install</span><br></pre></td></tr></table></figure>
<p>【注意】setup.py文件里第12行的</p>
<p>​        extra_compile_args={‘gcc’: [‘/Qstd=c99’]},</p>
<p>  改为<br>        extra_compile_args=[‘-std=c99’],</p>
<h3 id="编译可变性卷积层（如果要使用YOLACT-）"><a href="#编译可变性卷积层（如果要使用YOLACT-）" class="headerlink" title="*编译可变性卷积层（如果要使用YOLACT++）"></a>*编译可变性卷积层（如果要使用YOLACT++）</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ../../yolact-master/external/DCNv2</span><br><span class="line">python setup.py build develop</span><br></pre></td></tr></table></figure>
<h2 id="运行YOLACT"><a href="#运行YOLACT" class="headerlink" title="运行YOLACT"></a>运行YOLACT</h2><h3 id="GPU设置"><a href="#GPU设置" class="headerlink" title="GPU设置"></a>GPU设置</h3><p>YOLACT源码默认使用多GPU，而我只用了一个GPU，所以在运行前要设置一下。</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> CUDA_VISIBLE_DEVICES=[gpus]</span><br></pre></td></tr></table></figure>
<p>比如我用的是2号GPU，所以命令就是</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> CUDA_VISIBLE_DEVICES=2</span><br></pre></td></tr></table></figure>
<p>add|换了新电脑就可以用多GPUS啦!</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> CUDA_VISIBLE_DEVICES=0,1,2,3</span><br></pre></td></tr></table></figure>
<h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>运行前把github上的<a href="https://drive.google.com/file/d/15id0Qq5eqRbkD-N3ZjDZXdCvRyIaHpFB/view?usp=sharing">yolact_plus_base_54_800000.pth</a>下载到/weights文件夹里</p>
<p>如果运行yolact，进入eval.py所在文件夹    </p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /..../yolact-master</span><br><span class="line">python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=test.jpg</span><br></pre></td></tr></table></figure>
<p>add|如果运行yolact++：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /..../yolact-master</span><br><span class="line">python eval.py --trained_model=weights/yolact_plus_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=test.jpg</span><br></pre></td></tr></table></figure>
<p>python eval.py —trained_model=weights/yolact_plus_base_54_800000.pth —score_threshold=0.15 —top_k=15 —images=/home/youyx/data/datasets/TUM/rgbd_dataset_freiburg1_xyz/rgb/:/home/youyx/data/yolact_out_fr1_xyz</p>
<p>python eval.py —trained_model=weights/yolact_plus_base_54_800000.pth —score_threshold=0.15 —top_k=15 —images=/home/youyx/data/datasets/double_3_1/:/home/youyx/data/double_3_1_out</p>
<p>python eval.py —trained_model=weights/yolact_plus_resnet50_54_800000.pth —score_threshold=0.15 —top_k=15 —images=/home/youyx/data/datasets/double_3_1/:/home/youyx/data/double_3_1_out</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="pic1.png" alt="1"></p>
<p>可以看到识别出了一些车，速度也是比较快的。</p>
<h3 id="command"><a href="#command" class="headerlink" title="command"></a>command</h3><p>配置好环境后，以后再次运行yolact就只需要使用以下命令</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> activate</span><br><span class="line"></span><br><span class="line">conda activate yolact</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /home/youyx/yolacte_tutorials/yolact-master/</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> CUDA_VISIBLE_DEVICES=2</span><br><span class="line"></span><br><span class="line">python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=test.jpg</span><br></pre></td></tr></table></figure>
<p>公众号“小鸡炖技术”整理了完整的压缩包，百度云链接：<a href="https://pan.baidu.com/s/1ZKxm9L4fqT0CqwPjEUaYBg">https://pan.baidu.com/s/1ZKxm9L4fqT0CqwPjEUaYBg</a> 提取码pket。</p>
]]></content>
      <categories>
        <category>实例分割</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>实例分割</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵求导</title>
    <url>/2020/10/07/matrix-derivative/</url>
    <content><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>第一次接触矩阵导数是暑期课Frontier Approaches of Control Science的作业用最小二乘法做线性回归。在控制理论和机器学习领域，矩阵求导更是一个常用的数学工具。</p>
<h2 id="矩阵求导的本质"><a href="#矩阵求导的本质" class="headerlink" title="矩阵求导的本质"></a>矩阵求导的本质</h2><p>矩阵A对矩阵B求导，本质上是多元函数求导，也就是矩阵A中的每一个元素对矩阵B中的每一个元素求导，再将求导的结果排列成矩阵的形式。到这里，矩阵求导似乎就讲完了，剩下的就是复合函数求导和求偏导的计算。但是我们很快就发现这种逐元素求导的方法很复杂，随着元素的增加，计算量也极大地增加。那么，有没有直接用矩阵运算，从整体出发的算法。</p>
<h2 id="矩阵求导的形式"><a href="#矩阵求导的形式" class="headerlink" title="矩阵求导的形式"></a>矩阵求导的形式</h2><p>常见的矩阵求导有以下六种，分别是标量对标量求导、标量对向量求导、标量对矩阵求导、向量对标量求导、向量对向量求导和矩阵对标量求导。</p>
<p><img src="pic1.png" alt="pic1"></p>
<h2 id="两种布局"><a href="#两种布局" class="headerlink" title="两种布局"></a>两种布局</h2><p>我们上面提到矩阵求导的本质是矩阵A中的每一个元素对矩阵B中的每一个元素求导，再将求导结果排列成矩阵的形式。对于两个向量的求导结果一般有两种排列方式，分别是分子布局（XY拉伸术）和分母布局（YX拉伸术）。</p>
<p>$\frac{\partial Y}{\partial X}$的布局规则：1.标量不变，向量拉伸 2.前面横向拉，后面纵向拉</p>
<h3 id="分子布局（XY拉伸术）"><a href="#分子布局（XY拉伸术）" class="headerlink" title="分子布局（XY拉伸术）"></a>分子布局（XY拉伸术）</h3><p>对于$\frac{\partial Y}{\partial X}$，分子布局的方法是XY拉伸术。先判断X和Y是不是向量，若二者都是向量，根据布局规则，X在前所以横向拉伸，Y在后所以纵向拉伸，具体过程如下：</p>
<p>标量/向量（在分子布局下，Y是标量，不变；X是向量，横向拉伸）</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial \mathbf x}=[\frac{\partial y}{\partial x_1}...\frac{\partial y}{\partial x_n}]</script><p>向量/标量（在分子布局下，Y是向量，纵向拉伸；X是标量，不变）</p>
<script type="math/tex; mode=display">
\frac{\partial \mathbf y}{\partial x}=\large \begin{bmatrix} \frac{\partial y_1}{\partial x}\\ \vdots\\ \frac{\partial y_m}{\partial x}\end{bmatrix}</script><p>向量/向量（在分子布局下，Y是向量，纵向拉伸；X也是向量，横向拉伸）</p>
<script type="math/tex; mode=display">
\frac{\partial \mathbf y}{\partial \mathbf x}=\large \begin{bmatrix} \frac{\partial y_1}{\partial x_1} \dotso \frac{\partial y_1}{\partial x_n} \\ \vdots \: \: \:  \ddots \: \: \: \vdots \\ \frac{\partial y_m}{\partial x_1} \dotso \frac{\partial y_m}{\partial x_n}\end{bmatrix}</script><h3 id="分母布局（YX拉伸术）"><a href="#分母布局（YX拉伸术）" class="headerlink" title="分母布局（YX拉伸术）"></a>分母布局（YX拉伸术）</h3><p>同样对于$\frac{\partial Y}{\partial X}$，分母布局的方法是YX拉伸术。若二者都是向量，根据布局规则，Y在前所以横向拉伸，X在后所以纵向拉伸，具体过程如下：</p>
<p>标量/向量（在分母布局下，Y是标量，不变；X是向量，纵向拉伸）</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial \mathbf x}=\large \begin{bmatrix} \frac{\partial y}{\partial x_1}\\ \vdots\\ \frac{\partial y}{\partial x_n}\end{bmatrix}</script><p>向量/标量（在分母布局下，Y是向量，横向拉伸；X是标量，不变）</p>
<script type="math/tex; mode=display">
\frac{\partial \mathbf y}{\partial x}=[\frac{\partial y_1}{\partial x}...\frac{\partial y_m}{\partial x}]</script><p>向量/向量（在分母布局下，Y是向量，横向拉伸；X也是向量，纵向拉伸）</p>
<script type="math/tex; mode=display">
\frac{\partial \mathbf y}{\partial \mathbf x}=\large \begin{bmatrix} \frac{\partial y_1}{\partial x_1} \dotso \frac{\partial y_m}{\partial x_1} \\ \vdots \: \: \:  \ddots \: \: \: \vdots \\ \frac{\partial y_1}{\partial x_n} \dotso \frac{\partial y_m}{\partial x_n}\end{bmatrix}</script><p>分子布局和分母布局互为转置的关系：</p>
<ul>
<li>(分子布局)$^{T}$=分母布局</li>
<li>(分母布局)$^{T}$=分子布局</li>
</ul>
<p>在控制理论等领域的雅可比矩阵采用的是分子布局</p>
<p>在机器学习的梯度矩阵中采用的是分母布局</p>
<h2 id="常用的公式"><a href="#常用的公式" class="headerlink" title="常用的公式"></a>常用的公式</h2><p>（a, <strong>a</strong>, A分别是与标量x和向量<strong>x</strong>无关的标量、向量和矩阵）</p>
<p><img src="pic2.png" alt="pic2"></p>
<h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p>这里利用最小二乘法的例子来演示矩阵的整体求导。</p>
<p><img src="pic3.png" alt="pic3"></p>
<p>损失函数定义为y的实际值与拟合曲线对应值之差的平方：</p>
<script type="math/tex; mode=display">
L(\mathbf b)=\sum_{i=1}^n(y_i-\mathbf x_i^T\mathbf b)^2</script><p>用矩阵表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned}L(\mathbf b) &=(\mathbf Y-\mathbf x\mathbf b)^T(\mathbf Y-\mathbf x\mathbf b)\\ &=(\mathbf Y^T-\mathbf b^T \mathbf x^T)(\mathbf Y-\mathbf x\mathbf b)\\ &=\mathbf Y^T\mathbf Y-\mathbf Y^T \mathbf x \mathbf b-\mathbf b^T \mathbf x^T \mathbf Y+\mathbf b^T \mathbf x^T\mathbf x\mathbf b\\ &=\mathbf Y^T\mathbf Y-2\mathbf Y^T \mathbf x \mathbf b+\mathbf b^T \mathbf x^T\mathbf x\mathbf b
\end{aligned}</script><p>要找到一组系数向量<strong>b</strong>使得损失函数最小，将损失函数对<strong>b</strong>求导：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\text{d}L(\mathbf b)}{\text{d}\mathbf b} &=\frac{\text{d}\mathbf Y^T \mathbf Y}{\text{d}\mathbf b}-2\frac{\text{d}\mathbf Y^T \mathbf x \mathbf b}{\text{d}\mathbf b}+\frac{\text{d}\mathbf b^T \mathbf x^T \mathbf x \mathbf b}{\text{d}\mathbf b}\\ &=\mathbf 0-2(\mathbf Y^T \mathbf x)^T+2\mathbf x^T \mathbf x\mathbf b\\ &=-2 \mathbf x^T \mathbf Y+2\mathbf x^T \mathbf x \mathbf b=\mathbf 0
\end{aligned}</script><p>得到</p>
<script type="math/tex; mode=display">
\hat b=(\mathbf x^T \mathbf x)^{-1}\mathbf x^T\mathbf Y</script>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>矩阵</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>【论文阅读】HDNet</title>
    <url>/2021/09/24/HDNet/</url>
    <content><![CDATA[<h1 id="Learning-High-Fidelity-Depths-of-Dressed-Humans-by-Watching-Social-Media-Dance-Videos"><a href="#Learning-High-Fidelity-Depths-of-Dressed-Humans-by-Watching-Social-Media-Dance-Videos" class="headerlink" title="Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos"></a>Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos</h1><p>项目：<a href="https://www.yasamin.page/hdnet_tiktok">https://www.yasamin.page/hdnet_tiktok</a></p>
<p>github：<a href="https://github.com/yasaminjafarian/HDNet_TikTok">https://github.com/yasaminjafarian/HDNet_TikTok</a></p>
<p>这篇文章提出了一个端到端的自监督网络，计算视频中人的表面法向量，来得到高保真的深度值，再进行人体重建。该算法在真实场景和渲染场景都有SOTA性能。</p>
<h3 id="引入："><a href="#引入：" class="headerlink" title="引入："></a>引入：</h3><ol>
<li><p>目前的人体三维重建缺点方法有：</p>
<p>（1）设备要求高，计算复杂</p>
<p>（2）单帧图像，数据量太少</p>
<p>（3）能够建整体，但细节失败</p>
</li>
<li><p>作者提出了着衣人体的高保真3D几何建图方法，使用单一视角图像来预测深度和表面法向量。</p>
</li>
<li><p>舞蹈视频的特点：</p>
<p>（1）单人的，包括各种姿态</p>
<p>（2）没有ground truth，不能使用监督方法，</p>
<p>（3）刚体假设，使得可以利用几何一致性来学习</p>
</li>
<li><p>作者的方法的特点：</p>
<p>（1）表面法向量对细节敏感，将深度曲率和表面法向量匹配</p>
<p>（2）端到端，输入是RGB，输出是高保真深度</p>
<p>（3）HDNet：学习图像和UV坐标的<strong>空间关系</strong>，来产生中间<strong>表面法向量</strong>，预测的表面法向量用来预测<strong>高保真深度</strong></p>
</li>
<li><p>论文的贡献：</p>
<p>（1）制作了TikTok数据集：300个移动平台上公开媒体的舞蹈视频，包括人体掩膜和人体UV坐标</p>
<p>（2）提出一个扭转公式：把三维几何从一个图像扭转到另一个图像，并测量自洽性</p>
<p>（3）HDNet：通过加强几何一致性，来预测表面法向量反应出来的深度</p>
<p>作者的主要贡献在于：人体三维重建、单视图深度估计和人体三维数据集</p>
</li>
</ol>
<h3 id="相关工作："><a href="#相关工作：" class="headerlink" title="相关工作："></a>相关工作：</h3><ol>
<li><p>人体三维重建</p>
<p>（1）参数模型：SCAPE、SMPL，可以从单视图重建人体，但是分辨率不高。通过残差集合来精细化参数模型。</p>
<p>（2）非参数模型：可以用来描述着意人体，但数据集的获取困难。</p>
</li>
<li><p>单视图深度估计</p>
<p>（1）引入表面法向量来精华深度细节</p>
<p>（2）迭代最小二乘、核回归：融合深度和表面法向量</p>
<p>（3）从粗糙到精细的学习方法</p>
<p>（4）融合表面法向量到深度估计</p>
<p>（5）作者的工作：表面法向量和深度一起学习</p>
</li>
<li><p>人体三维数据集：</p>
<p>（1）缺少用于几何预测的数据集，大部分是静态模型</p>
<p>（2）作者提出真实场景的舞蹈视频，来产生深度估计从不同的视角、外表、衣服风格和位姿</p>
</li>
</ol>
<h3 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h3><p>深度是关于像素点位置和图像的一个函数<em>g</em>，现有的方法直接用训练集学习这个函数，存在以下两个缺点：</p>
<p>（1）尽管能够获得整体的深度，但是不能获得精细的局部深度；</p>
<p>（2）需要大量的3d数据，但这样大量的数据集并不存在</p>
<ol>
<li><p>自监督人体深度</p>
<p>（1）假设坐标转换函数<em>h</em>将人体表面坐标<em>u</em>映射到像素坐标<em>x</em>，重建三维点<em>p</em>，p由关于深度z、相机内参、像素坐标的函数得到。三维点<em>p</em>的随时间的变换，由转换函数<em>w</em>得到，转换包括旋转和平移。</p>
<p>（2）为什么w函数有效：（1）人体的部位大多数满足刚体变换（2）对于形变的部分，就扩大时间范围</p>
<p>（3）深度估计原本是稀疏的，通过双线性插值来获得稠密的深度</p>
<p>（4）最小化所有时间的所有点的实际三维位置和预测三维位置的距离和，损失函数<em>Lw</em>。这个让估计出的深度用来监督转换函数，所以使得转换是自监督的</p>
</li>
<li><p>表面法向量和深度的联合学习</p>
<p>（1）表面法向量对局部纹理、皱纹和阴影高度敏感。表面法向量垂直于三维点的切平面，是x轴和y轴单位向量的叉乘，单位向量与像素坐标和深度有关，因此又能够二者能够相互监督</p>
<p>（2）总损失函数为：深度误差<em>Lz</em> + 法向量误差<em>Ln</em> + 自监督法向量夹角误差<em>Ls</em> + 自监督三维点距离误差<em>Lw</em></p>
</li>
<li><p>网络设计和细节</p>
<p>（1）HDNet = 表面法向量预测器 + 深度预测器</p>
<p>表面法向量预测器</p>
<p>输入：2种数据—RGB图像和前景掩膜</p>
<p>输出：预测的表面法向量</p>
<p>深度预测器</p>
<p>输入：3种数据—RGB图像、前景掩膜和UV坐标</p>
<p>输出：预测的深度</p>
<p>两个预测器都采用堆叠沙漏网络（ stacked hourglasses network）作为骨架</p>
</li>
</ol>
<p>   （2）孪生神经网络（Siamese network）</p>
<p>   作用：同一视频两个不同时刻i和j的图像经过深度预测后，进行i-&gt;j的深度转换，来计算自监督三维点距离误差<em>Lw</em>。</p>
<p>   （3）图像对选取方法</p>
<p>   同一视频中，随机选取满足有5个共同的身体部位课件，且每个身体部位至少有50个UV坐标重合。</p>
<p>   （4）使用Adam optimizer优化</p>
<h3 id="数据集："><a href="#数据集：" class="headerlink" title="数据集："></a>数据集：</h3><p>作者做的TikTok Dataset</p>
<h3 id="实验："><a href="#实验：" class="headerlink" title="实验："></a>实验：</h3><p>在服务器上跑了一下，应该是tensorflow和cuda版本的问题，没跑起来。最后在colab上跑了一下demo</p>
<p>效果：</p>
<p><img src="pic1.png" alt="pic1"></p>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>人体三维重建</category>
      </categories>
      <tags>
        <tag>人体三维重建</tag>
        <tag>深度估计</tag>
        <tag>表面法向量</tag>
      </tags>
  </entry>
  <entry>
    <title>手写数字识别器</title>
    <url>/2020/10/11/num-recognizer/</url>
    <content><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>  手写数字识别器是一个经典的卷积神经网络问题，这里将利用PyTorch实验手写数字识别器的任务。</p>
<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>  首先导入所有需要的库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> dsets</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]=<span class="string">&quot;TRUE&quot;</span></span><br></pre></td></tr></table></figure>
<p>  接着定义一些训练用的超参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">image_size = <span class="number">28</span> <span class="comment">#训练图像的总尺寸为28*28</span></span><br><span class="line">num_classes = <span class="number">10</span> <span class="comment">#标签种类数</span></span><br><span class="line">num_epochs = <span class="number">20</span> <span class="comment">#训练的总循环周期</span></span><br><span class="line">batch_size = <span class="number">64</span> <span class="comment">#一个批次的大小，64张图片</span></span><br></pre></td></tr></table></figure>
<p>  然后导入数据，Pytorch中自带了我们需要的手写数据集MNIST。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#加载MNIST数据，如果没有下载过，系统会在当前路径下新建/data子目录，并把文件存放在其中（压缩的格式）</span></span><br><span class="line"><span class="comment">#MNIST数据属于torchvision包自带的数据，可以直接调用</span></span><br><span class="line">train_dataset = dsets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">                            train=<span class="literal">True</span>,</span><br><span class="line">                            transform=transforms.ToTensor(),</span><br><span class="line">                            download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载测试数据集</span></span><br><span class="line">test_dataset = dsets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">                           train=<span class="literal">False</span>,</span><br><span class="line">                           transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练数据集的加载器，自动将数据分成批，顺序随机打乱</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset,</span><br><span class="line">                                           batch_size=batch_size,</span><br><span class="line">                                           shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将数据分为两部分，一部分作为校验数据，用于检测模型是否过拟合并调整参数</span></span><br><span class="line"><span class="comment">#另一部分作为测试数据，检验整个模型</span></span><br><span class="line"><span class="comment">#首先，定义下标数组indices，相当于test_dataset中数据的编码</span></span><br><span class="line"><span class="comment">#然后，定义下表indices_val表示校验集数据的下标，indices_test表示测试集的下标</span></span><br><span class="line">indices = range(len(test_dataset))</span><br><span class="line">indices_val = indices[:<span class="number">5000</span>]</span><br><span class="line">indices_test = indices[<span class="number">5000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment">#根据两个下标构造两个数据集的SubsetRandomSampler采样器，他会对下标进行采样</span></span><br><span class="line">sampler_val = torch.utils.data.sampler.SubsetRandomSampler(indices_val)</span><br><span class="line">sampler_test = torch.utils.data.sampler.SubsetRandomSampler(indices_test)</span><br><span class="line"></span><br><span class="line"><span class="comment">#根据两个采样器定义加载器</span></span><br><span class="line"><span class="comment">#将sampler_val和sampler_test分别赋值给validation_loader和test_loader</span></span><br><span class="line">validation_loader = torch.utils.data.DataLoader(dataset=test_dataset,</span><br><span class="line">                                                batch_size=batch_size,</span><br><span class="line">                                                shuffle=<span class="literal">False</span>,</span><br><span class="line">                                                sampler=sampler_val)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset,</span><br><span class="line">                                          batch_size=batch_size,</span><br><span class="line">                                          shuffle=<span class="literal">False</span>,</span><br><span class="line">                                          sampler=sampler_test)</span><br></pre></td></tr></table></figure>
<ul>
<li>数据集（dataset）是对整个数据的封装，无论原始数据是图像还是张量，数据集都将对其进行统一处理。</li>
<li>加载器（dataloader）主要负责在程序中对数据集的使用。</li>
<li>采样器（sampler）为加载器提供了一个每一批抽取数据集中样本的方法。</li>
</ul>
<h2 id="构建网络"><a href="#构建网络" class="headerlink" title="构建网络"></a>构建网络</h2><p>  这里主要利用PyTorch的nn.Module类来构建卷积神经网络。</p>
<p>  首先，构造ConvNet类，它是对nn.Module类的继承。</p>
<p>  其次，复写init()和forward()两个函数。init()为构造函数，每当类ConvNet被具体化一个实例的时候就会被调用。forward()函数则是在正向运行神经网络时被自动调用，负责数据的向前传递，并同时构造计算图。</p>
<p>  然后，定义一个retrieve_features()函数，用来提取网络中各个卷积层的权重。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义卷积神经网络：4和8为认为指定的两个卷积层的厚度（feature map的数量）</span></span><br><span class="line">depth = [<span class="number">4</span>, <span class="number">8</span>]</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 该函数在创建一个ConvNet对象，即调用语句net=ConvNet()时就会被调用</span></span><br><span class="line">        <span class="comment"># 首先调用父类相应的构造函数</span></span><br><span class="line">        super(ConvNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#其次构造ConvNet要用到的各个神经网络模块</span></span><br><span class="line">        <span class="comment">#注意，构造组件并不是真正搭建组件，只是把基本建筑砖块先找好</span></span><br><span class="line">        <span class="comment">#定义一个卷积层，输入通道为1，输出通道为4，窗口大小为5，padding为2</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>, padding = <span class="number">2</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)<span class="comment">#定义一个池化层，一个窗口为2x2的池化运算</span></span><br><span class="line">        <span class="comment">#第二个卷积层，输入通道为depth[0],输出通道为depth[1]，窗口为5，padding为2</span></span><br><span class="line">        self.conv2 = nn.Conv2d(depth[<span class="number">0</span>], depth[<span class="number">1</span>], <span class="number">5</span>, padding = <span class="number">2</span>)</span><br><span class="line">        <span class="comment">#一个线性连接层，输入尺寸为最后一层立方体的线性平铺，输出层512个节点</span></span><br><span class="line">        self.fc1 = nn.Linear(image_size // <span class="number">4</span> * image_size // <span class="number">4</span> * depth[<span class="number">1</span>], <span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">512</span>, num_classes) <span class="comment">#最后一层线性分类单元，输入为512，输出为要做分类的类别数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span> <span class="comment">#该函数完成神经网络真正的前向运算，在这里把各个组件进行实际的拼装</span></span><br><span class="line">        <span class="comment"># x的尺寸：（batch_size, image_channels, image_width, image_height）</span></span><br><span class="line">        x = self.conv1(x)    <span class="comment">#第一层卷积</span></span><br><span class="line">        x = F.relu(x)      <span class="comment">#激活函数用ReLU，防止过拟合</span></span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, num_filters, image_width, image_height）</span></span><br><span class="line"></span><br><span class="line">        x = self.pool(x) <span class="comment">#第二层池化，把图片变小</span></span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, depth[0], image_width/2, image_height/2）</span></span><br><span class="line"></span><br><span class="line">        x = self.conv2(x) <span class="comment">#第三层卷积，窗口为5，输入输出通道分别为depth[0]=4, depth[1]=8</span></span><br><span class="line">        x = F.relu(x) <span class="comment">#非线性函数</span></span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, depth[0], image_width/2, image_height/2）</span></span><br><span class="line"></span><br><span class="line">        x = self.pool(x) <span class="comment">#第四层池化，将图片缩小到原来的1/4</span></span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, depth[1], image_width/4, image_heigth/4）</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#将立体的特征图tensor压成一个一维向量</span></span><br><span class="line">        <span class="comment">#view函数可以将一个tensor按指定的方式重新排布</span></span><br><span class="line">        <span class="comment">#下面这个命令就是要让x按照batch_size * (image_size//4)^2*depth[1]的方式来排布向量</span></span><br><span class="line">        x = x.view(<span class="number">-1</span>, image_size // <span class="number">4</span> * image_size // <span class="number">4</span> *depth[<span class="number">1</span>])</span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, depth[1]*image_width/4*image_height/4）</span></span><br><span class="line"></span><br><span class="line">        x = F.relu(self.fc1(x))<span class="comment">#第五层为全连接，ReLU激活函数</span></span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, 512）</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#以默认0.5的概率对这一层进行dropout操作，防止过拟合</span></span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.fc2(x) <span class="comment">#全连接</span></span><br><span class="line">        <span class="comment">#x的尺寸：(batch_size, num_classes)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#输出层为log_softmax,即概率对数值log(p(x)),采用log_softmax可以使后面交叉熵计算更快</span></span><br><span class="line">        x = F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">retrieve_features</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 该函数用于提取卷积神经网络的特征图，返回feature_map1, feature_map2为前两层卷积层的特征图</span></span><br><span class="line">        feature_map1 = F.relu(self.conv1(x)) <span class="comment">#完成第一层卷积</span></span><br><span class="line">        x = self.pool(feature_map1)</span><br><span class="line">        <span class="comment">#第二层卷积，两层特征图都存储到了feature_map1, feature_map2中</span></span><br><span class="line">        feature_map2 = F.relu(self.conv2(x))</span><br><span class="line">        <span class="keyword">return</span> (feature_map1, feature_map2)</span><br></pre></td></tr></table></figure>
<p>  在以上代码中用到了dropout()函数，该函数用来防止神经网络的过拟合情况。在训练过程中，根据一定的概率随机将其中的一些神经元暂时丢弃，最后在测试的时候再使用全部的神经元，增强模型的泛化能力。</p>
<h2 id="运行模型"><a href="#运行模型" class="headerlink" title="运行模型"></a>运行模型</h2><p>  构建好ConvNet之后，就可以读取数据并训练模型了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = ConvNet() <span class="comment">#新建一个卷积神经网络的实例，此时ConvNet的__init__()函数会被自动调用</span></span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss() <span class="comment">#损失函数的定义，交叉熵</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>) <span class="comment">#定义优化器，普通的随机梯度下降算法</span></span><br><span class="line"></span><br><span class="line">record = [] <span class="comment">#记录准确率等数值的容器</span></span><br><span class="line">weights = [] <span class="comment">#每若干步就记录一次卷积核</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rightness</span>(<span class="params">output, target</span>):</span></span><br><span class="line">    preds = output.data.max(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> preds.eq(target.data.view_as(preds)).cpu().sum(), len(target)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始训练循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line"></span><br><span class="line">    train_rights = [] <span class="comment">#记录训练数据集准确率的容器</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;下面的enumerate起到构造枚举器的作用，在对train_loader做循环迭代时，enumerate会自动输出一个数字指示循环了几次</span></span><br><span class="line"><span class="string">    并记录在batch_idx中，它就等于0，1，2，...train_loader每迭代一次，就会输出一对数据data和target,分别对应一个批中的</span></span><br><span class="line"><span class="string">    手写数字图及对应的标签。&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> enumerate(train_loader): <span class="comment">#针对容器中的每一批进行循环</span></span><br><span class="line">        <span class="comment">#将Tensor转化为Variable，data为一批图像，target为一批标签</span></span><br><span class="line">        data, target = Variable(data), Variable(target)</span><br><span class="line">        <span class="comment">#给网络模型做标记，标志着模型在训练集上训练</span></span><br><span class="line">        <span class="comment">#这种区分主要是为了打开关闭net的training标志，从而决定是否运行dropout</span></span><br><span class="line">        net.train()</span><br><span class="line"></span><br><span class="line">        output = net(data) <span class="comment">#神经网络完成一次前馈的计算过程，得到预测输出output</span></span><br><span class="line">        loss = criterion(output, target) <span class="comment">#将output与标签target比较，计算误差</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment">#清空梯度</span></span><br><span class="line">        loss.backward() <span class="comment">#反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment">#一步随机梯度下降算法</span></span><br><span class="line">        right = rightness(output, target) <span class="comment">#计算准确率所需数值，返回数值为（正确样例数，总样本数）</span></span><br><span class="line">        train_rights.append(right) <span class="comment">#将计算结果装到列表容器train_rights中</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">100</span> == <span class="number">0</span>: <span class="comment">#每隔100个batch执行一次打印操作</span></span><br><span class="line"></span><br><span class="line">            net.eval() <span class="comment">#给网络模型做标记，标志着模型在训练集上训练</span></span><br><span class="line">            val_rights = [] <span class="comment">#记录校验数据集准确率的标签</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#开始在校验集上做循环，计算校验集上的准确度</span></span><br><span class="line">            <span class="keyword">for</span> (data, target) <span class="keyword">in</span> validation_loader:</span><br><span class="line">                data, target = Variable(data), Variable(target)</span><br><span class="line">                <span class="comment">#完成一次前馈计算过程，得到目前训练得到的模型net在校验集上的表现</span></span><br><span class="line">                output = net(data)</span><br><span class="line">                <span class="comment">#计算准确率所需数值，返回正确的数值为（正确样例数，总样本数）</span></span><br><span class="line">                right = rightness(output, target)</span><br><span class="line">                val_rights.append(right)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#分别计算目前已经计算过的测试集以及全部校验集上模型的表现：分类准确率</span></span><br><span class="line">            <span class="comment">#train_r为一个二元组，分别记录经历过的所有训练集中分类正确的数量和该集合中总的样本数</span></span><br><span class="line">            <span class="comment">#train_r[0]/train_r[1]是训练集的分类准确度，val_r[0]/val_r[1]是校验集的分类准确度</span></span><br><span class="line">            train_r = (sum([tup[<span class="number">0</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> train_rights]),sum([tup[<span class="number">1</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> train_rights]))</span><br><span class="line">            <span class="comment">#val_r为一个二元组，分别记录校验集中分类正确的数量和该集合中的总样本数</span></span><br><span class="line">            val_r = (sum([tup[<span class="number">0</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> val_rights]),sum([tup[<span class="number">1</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> val_rights]))</span><br><span class="line"></span><br><span class="line">            <span class="comment">#打印准确率等数值，其中正确率为样本训练周期epoch开始后到目前批的正确率的平均值</span></span><br><span class="line">            <span class="comment">#print(&#x27;训练周期：&#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\t, Loss:&#123;:.6f&#125;\t, 训练正确率：&#123;:.2f&#125;%\t, 校验正确率：&#123;:.2f&#125;%&#x27;.format(</span></span><br><span class="line">            <span class="comment">#     epoch, batch_idx * len(data), len(train_loader.dataset),</span></span><br><span class="line">            <span class="comment">#     100. * batch_idx / len(train_loader), loss.data[0],</span></span><br><span class="line">            <span class="comment">#     100. * train_r[0] / train_r[1],</span></span><br><span class="line">            <span class="comment">#     100. * val_r[0] / val_r[1]))</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#将准确率和权重等数值加载到容器中，方便后续处理</span></span><br><span class="line">            record.append((<span class="number">100</span> - <span class="number">100.</span> * train_r[<span class="number">0</span>] / train_r[<span class="number">1</span>], <span class="number">100</span> - <span class="number">100.</span> * val_r[<span class="number">0</span>] / val_r[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">            <span class="comment">#wights记录了训练中其中所有卷积演化的过程，net.conv1.weight提取出了第一层卷积核的权重</span></span><br><span class="line">            <span class="comment">#clone是将weight.data中的数据做一个备份放到列表中</span></span><br><span class="line">            <span class="comment">#否则放weight.data变化时，列表中的每一项数值也会联动</span></span><br><span class="line">            <span class="comment">#这里使用clone这个函数很重要</span></span><br><span class="line">            weights.append([net.conv1.weight.data.clone(), net.conv1.bias.data.clone(),</span><br><span class="line">                            net.conv2.weight.data.clone(), net.conv2.bias.data.clone()])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  以上代码中，net.train()会打开所有的dropout层，而net.eval()会关闭它们。</p>
<h2 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#在训练集上分批运行，并计算总的正确率</span></span><br><span class="line">net.eval() <span class="comment">#标志着模型当前的运行阶段</span></span><br><span class="line">vals = [] <span class="comment">#记录准确率所用列表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#对测试数据集进行循环</span></span><br><span class="line"><span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        data = Variable(data)</span><br><span class="line">    target = Variable(target)</span><br><span class="line">    output = net(data) <span class="comment">#将特征数据输入网络，得到分类的输出</span></span><br><span class="line">    val = rightness(output, target) <span class="comment">#获得正确样本数以及总样本数</span></span><br><span class="line">    vals.append(val) <span class="comment">#记录结果</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算准确率</span></span><br><span class="line">rights = (sum([tup[<span class="number">0</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> vals]),sum([tup[<span class="number">1</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> vals]))</span><br><span class="line">right_rate = <span class="number">1.0</span> * rights[<span class="number">0</span>] / rights[<span class="number">1</span>]</span><br><span class="line">print(right_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制训练过程的误差曲线，校验集和测试集上的错误率</span></span><br><span class="line">plt.figure(figsize = (<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">plt.plot(record) <span class="comment">#record记录了每一个打印周期记录的训练集和校验集上的准确度</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Steps&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Error rate&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>  最后，将训练过程中的误差曲线绘制出来。</p>
<p><img src="pic1.png" alt="1"></p>
<p>  图中左边浅色的为校验数据错误率曲线，右边深色的为测试数据错误率曲线。模型在测试集和校验集上的表现都很好，卷积神经网络的泛化能力也很强。</p>
]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
</search>
