<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello 2021！</title>
    <url>/2021/01/26/hello2021/</url>
    <content><![CDATA[<p>Welcome to 2021!</p>
]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/10/11/hello-world/</url>
    <content><![CDATA[<p>Welcome to my blog!</p>
]]></content>
  </entry>
  <entry>
    <title>矩阵求导</title>
    <url>/2020/10/07/matrix-derivative/</url>
    <content><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>第一次接触矩阵导数是暑期课Frontier Approaches of Control Science的作业用最小二乘法做线性回归。在控制理论和机器学习领域，矩阵求导更是一个常用的数学工具。</p>
<h2 id="矩阵求导的本质"><a href="#矩阵求导的本质" class="headerlink" title="矩阵求导的本质"></a>矩阵求导的本质</h2><p>矩阵A对矩阵B求导，本质上是多元函数求导，也就是矩阵A中的每一个元素对矩阵B中的每一个元素求导，再将求导的结果排列成矩阵的形式。到这里，矩阵求导似乎就讲完了，剩下的就是复合函数求导和求偏导的计算。但是我们很快就发现这种逐元素求导的方法很复杂，随着元素的增加，计算量也极大地增加。那么，有没有直接用矩阵运算，从整体出发的算法。</p>
<h2 id="矩阵求导的形式"><a href="#矩阵求导的形式" class="headerlink" title="矩阵求导的形式"></a>矩阵求导的形式</h2><p>常见的矩阵求导有以下六种，分别是标量对标量求导、标量对向量求导、标量对矩阵求导、向量对标量求导、向量对向量求导和矩阵对标量求导。</p>
<p><img src="pic1.png" alt="pic1"></p>
<h2 id="两种布局"><a href="#两种布局" class="headerlink" title="两种布局"></a>两种布局</h2><p>我们上面提到矩阵求导的本质是矩阵A中的每一个元素对矩阵B中的每一个元素求导，再将求导结果排列成矩阵的形式。对于两个向量的求导结果一般有两种排列方式，分别是分子布局（XY拉伸术）和分母布局（YX拉伸术）。</p>
<p>$\frac{\partial Y}{\partial X}$的布局规则：1.标量不变，向量拉伸 2.前面横向拉，后面纵向拉</p>
<h3 id="分子布局（XY拉伸术）"><a href="#分子布局（XY拉伸术）" class="headerlink" title="分子布局（XY拉伸术）"></a>分子布局（XY拉伸术）</h3><p>对于$\frac{\partial Y}{\partial X}$，分子布局的方法是XY拉伸术。先判断X和Y是不是向量，若二者都是向量，根据布局规则，X在前所以横向拉伸，Y在后所以纵向拉伸，具体过程如下：</p>
<p>标量/向量（在分子布局下，Y是标量，不变；X是向量，横向拉伸）</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial \mathbf x}=[\frac{\partial y}{\partial x_1}...\frac{\partial y}{\partial x_n}]</script><p>向量/标量（在分子布局下，Y是向量，纵向拉伸；X是标量，不变）</p>
<script type="math/tex; mode=display">
\frac{\partial \mathbf y}{\partial x}=\large \begin{bmatrix} \frac{\partial y_1}{\partial x}\\ \vdots\\ \frac{\partial y_m}{\partial x}\end{bmatrix}</script><p>向量/向量（在分子布局下，Y是向量，纵向拉伸；X也是向量，横向拉伸）</p>
<script type="math/tex; mode=display">
\frac{\partial \mathbf y}{\partial \mathbf x}=\large \begin{bmatrix} \frac{\partial y_1}{\partial x_1} \dotso \frac{\partial y_1}{\partial x_n} \\ \vdots \: \: \:  \ddots \: \: \: \vdots \\ \frac{\partial y_m}{\partial x_1} \dotso \frac{\partial y_m}{\partial x_n}\end{bmatrix}</script><h3 id="分母布局（YX拉伸术）"><a href="#分母布局（YX拉伸术）" class="headerlink" title="分母布局（YX拉伸术）"></a>分母布局（YX拉伸术）</h3><p>同样对于$\frac{\partial Y}{\partial X}$，分母布局的方法是YX拉伸术。若二者都是向量，根据布局规则，Y在前所以横向拉伸，X在后所以纵向拉伸，具体过程如下：</p>
<p>标量/向量（在分母布局下，Y是标量，不变；X是向量，纵向拉伸）</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial \mathbf x}=\large \begin{bmatrix} \frac{\partial y}{\partial x_1}\\ \vdots\\ \frac{\partial y}{\partial x_n}\end{bmatrix}</script><p>向量/标量（在分母布局下，Y是向量，横向拉伸；X是标量，不变）</p>
<script type="math/tex; mode=display">
\frac{\partial \mathbf y}{\partial x}=[\frac{\partial y_1}{\partial x}...\frac{\partial y_m}{\partial x}]</script><p>向量/向量（在分母布局下，Y是向量，横向拉伸；X也是向量，纵向拉伸）</p>
<script type="math/tex; mode=display">
\frac{\partial \mathbf y}{\partial \mathbf x}=\large \begin{bmatrix} \frac{\partial y_1}{\partial x_1} \dotso \frac{\partial y_m}{\partial x_1} \\ \vdots \: \: \:  \ddots \: \: \: \vdots \\ \frac{\partial y_1}{\partial x_n} \dotso \frac{\partial y_m}{\partial x_n}\end{bmatrix}</script><p>分子布局和分母布局互为转置的关系：</p>
<ul>
<li>(分子布局)$^{T}$=分母布局</li>
<li>(分母布局)$^{T}$=分子布局</li>
</ul>
<p>在控制理论等领域的雅可比矩阵采用的是分子布局</p>
<p>在机器学习的梯度矩阵中采用的是分母布局</p>
<h2 id="常用的公式"><a href="#常用的公式" class="headerlink" title="常用的公式"></a>常用的公式</h2><p>（a, <strong>a</strong>, A分别是与标量x和向量<strong>x</strong>无关的标量、向量和矩阵）</p>
<p><img src="pic2.png" alt="pic2"></p>
<h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p>这里利用最小二乘法的例子来演示矩阵的整体求导。</p>
<p><img src="pic3.png" alt="pic3"></p>
<p>损失函数定义为y的实际值与拟合曲线对应值之差的平方：</p>
<script type="math/tex; mode=display">
L(\mathbf b)=\sum_{i=1}^n(y_i-\mathbf x_i^T\mathbf b)^2</script><p>用矩阵表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned}L(\mathbf b) &=(\mathbf Y-\mathbf x\mathbf b)^T(\mathbf Y-\mathbf x\mathbf b)\\ &=(\mathbf Y^T-\mathbf b^T \mathbf x^T)(\mathbf Y-\mathbf x\mathbf b)\\ &=\mathbf Y^T\mathbf Y-\mathbf Y^T \mathbf x \mathbf b-\mathbf b^T \mathbf x^T \mathbf Y+\mathbf b^T \mathbf x^T\mathbf x\mathbf b\\ &=\mathbf Y^T\mathbf Y-2\mathbf Y^T \mathbf x \mathbf b+\mathbf b^T \mathbf x^T\mathbf x\mathbf b
\end{aligned}</script><p>要找到一组系数向量<strong>b</strong>使得损失函数最小，将损失函数对<strong>b</strong>求导：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\text{d}L(\mathbf b)}{\text{d}\mathbf b} &=\frac{\text{d}\mathbf Y^T \mathbf Y}{\text{d}\mathbf b}-2\frac{\text{d}\mathbf Y^T \mathbf x \mathbf b}{\text{d}\mathbf b}+\frac{\text{d}\mathbf b^T \mathbf x^T \mathbf x \mathbf b}{\text{d}\mathbf b}\\ &=\mathbf 0-2(\mathbf Y^T \mathbf x)^T+2\mathbf x^T \mathbf x\mathbf b\\ &=-2 \mathbf x^T \mathbf Y+2\mathbf x^T \mathbf x \mathbf b=\mathbf 0
\end{aligned}</script><p>得到</p>
<script type="math/tex; mode=display">
\hat b=(\mathbf x^T \mathbf x)^{-1}\mathbf x^T\mathbf Y</script>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>矩阵</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>手写数字识别器</title>
    <url>/2020/10/11/num-recognizer/</url>
    <content><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>  手写数字识别器是一个经典的卷积神经网络问题，这里将利用PyTorch实验手写数字识别器的任务。</p>
<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>  首先导入所有需要的库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> dsets</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]=<span class="string">&quot;TRUE&quot;</span></span><br></pre></td></tr></table></figure>
<p>  接着定义一些训练用的超参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">image_size = <span class="number">28</span> <span class="comment">#训练图像的总尺寸为28*28</span></span><br><span class="line">num_classes = <span class="number">10</span> <span class="comment">#标签种类数</span></span><br><span class="line">num_epochs = <span class="number">20</span> <span class="comment">#训练的总循环周期</span></span><br><span class="line">batch_size = <span class="number">64</span> <span class="comment">#一个批次的大小，64张图片</span></span><br></pre></td></tr></table></figure>
<p>  然后导入数据，Pytorch中自带了我们需要的手写数据集MNIST。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#加载MNIST数据，如果没有下载过，系统会在当前路径下新建/data子目录，并把文件存放在其中（压缩的格式）</span></span><br><span class="line"><span class="comment">#MNIST数据属于torchvision包自带的数据，可以直接调用</span></span><br><span class="line">train_dataset = dsets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">                            train=<span class="literal">True</span>,</span><br><span class="line">                            transform=transforms.ToTensor(),</span><br><span class="line">                            download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载测试数据集</span></span><br><span class="line">test_dataset = dsets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">                           train=<span class="literal">False</span>,</span><br><span class="line">                           transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练数据集的加载器，自动将数据分成批，顺序随机打乱</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset,</span><br><span class="line">                                           batch_size=batch_size,</span><br><span class="line">                                           shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将数据分为两部分，一部分作为校验数据，用于检测模型是否过拟合并调整参数</span></span><br><span class="line"><span class="comment">#另一部分作为测试数据，检验整个模型</span></span><br><span class="line"><span class="comment">#首先，定义下标数组indices，相当于test_dataset中数据的编码</span></span><br><span class="line"><span class="comment">#然后，定义下表indices_val表示校验集数据的下标，indices_test表示测试集的下标</span></span><br><span class="line">indices = range(len(test_dataset))</span><br><span class="line">indices_val = indices[:<span class="number">5000</span>]</span><br><span class="line">indices_test = indices[<span class="number">5000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment">#根据两个下标构造两个数据集的SubsetRandomSampler采样器，他会对下标进行采样</span></span><br><span class="line">sampler_val = torch.utils.data.sampler.SubsetRandomSampler(indices_val)</span><br><span class="line">sampler_test = torch.utils.data.sampler.SubsetRandomSampler(indices_test)</span><br><span class="line"></span><br><span class="line"><span class="comment">#根据两个采样器定义加载器</span></span><br><span class="line"><span class="comment">#将sampler_val和sampler_test分别赋值给validation_loader和test_loader</span></span><br><span class="line">validation_loader = torch.utils.data.DataLoader(dataset=test_dataset,</span><br><span class="line">                                                batch_size=batch_size,</span><br><span class="line">                                                shuffle=<span class="literal">False</span>,</span><br><span class="line">                                                sampler=sampler_val)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset,</span><br><span class="line">                                          batch_size=batch_size,</span><br><span class="line">                                          shuffle=<span class="literal">False</span>,</span><br><span class="line">                                          sampler=sampler_test)</span><br></pre></td></tr></table></figure>
<ul>
<li>数据集（dataset）是对整个数据的封装，无论原始数据是图像还是张量，数据集都将对其进行统一处理。</li>
<li>加载器（dataloader）主要负责在程序中对数据集的使用。</li>
<li>采样器（sampler）为加载器提供了一个每一批抽取数据集中样本的方法。</li>
</ul>
<h2 id="构建网络"><a href="#构建网络" class="headerlink" title="构建网络"></a>构建网络</h2><p>  这里主要利用PyTorch的nn.Module类来构建卷积神经网络。</p>
<p>  首先，构造ConvNet类，它是对nn.Module类的继承。</p>
<p>  其次，复写init()和forward()两个函数。init()为构造函数，每当类ConvNet被具体化一个实例的时候就会被调用。forward()函数则是在正向运行神经网络时被自动调用，负责数据的向前传递，并同时构造计算图。</p>
<p>  然后，定义一个retrieve_features()函数，用来提取网络中各个卷积层的权重。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义卷积神经网络：4和8为认为指定的两个卷积层的厚度（feature map的数量）</span></span><br><span class="line">depth = [<span class="number">4</span>, <span class="number">8</span>]</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 该函数在创建一个ConvNet对象，即调用语句net=ConvNet()时就会被调用</span></span><br><span class="line">        <span class="comment"># 首先调用父类相应的构造函数</span></span><br><span class="line">        super(ConvNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#其次构造ConvNet要用到的各个神经网络模块</span></span><br><span class="line">        <span class="comment">#注意，构造组件并不是真正搭建组件，只是把基本建筑砖块先找好</span></span><br><span class="line">        <span class="comment">#定义一个卷积层，输入通道为1，输出通道为4，窗口大小为5，padding为2</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">4</span>, <span class="number">5</span>, padding = <span class="number">2</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)<span class="comment">#定义一个池化层，一个窗口为2x2的池化运算</span></span><br><span class="line">        <span class="comment">#第二个卷积层，输入通道为depth[0],输出通道为depth[1]，窗口为5，padding为2</span></span><br><span class="line">        self.conv2 = nn.Conv2d(depth[<span class="number">0</span>], depth[<span class="number">1</span>], <span class="number">5</span>, padding = <span class="number">2</span>)</span><br><span class="line">        <span class="comment">#一个线性连接层，输入尺寸为最后一层立方体的线性平铺，输出层512个节点</span></span><br><span class="line">        self.fc1 = nn.Linear(image_size // <span class="number">4</span> * image_size // <span class="number">4</span> * depth[<span class="number">1</span>], <span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">512</span>, num_classes) <span class="comment">#最后一层线性分类单元，输入为512，输出为要做分类的类别数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span> <span class="comment">#该函数完成神经网络真正的前向运算，在这里把各个组件进行实际的拼装</span></span><br><span class="line">        <span class="comment"># x的尺寸：（batch_size, image_channels, image_width, image_height）</span></span><br><span class="line">        x = self.conv1(x)    <span class="comment">#第一层卷积</span></span><br><span class="line">        x = F.relu(x)      <span class="comment">#激活函数用ReLU，防止过拟合</span></span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, num_filters, image_width, image_height）</span></span><br><span class="line"></span><br><span class="line">        x = self.pool(x) <span class="comment">#第二层池化，把图片变小</span></span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, depth[0], image_width/2, image_height/2）</span></span><br><span class="line"></span><br><span class="line">        x = self.conv2(x) <span class="comment">#第三层卷积，窗口为5，输入输出通道分别为depth[0]=4, depth[1]=8</span></span><br><span class="line">        x = F.relu(x) <span class="comment">#非线性函数</span></span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, depth[0], image_width/2, image_height/2）</span></span><br><span class="line"></span><br><span class="line">        x = self.pool(x) <span class="comment">#第四层池化，将图片缩小到原来的1/4</span></span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, depth[1], image_width/4, image_heigth/4）</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#将立体的特征图tensor压成一个一维向量</span></span><br><span class="line">        <span class="comment">#view函数可以将一个tensor按指定的方式重新排布</span></span><br><span class="line">        <span class="comment">#下面这个命令就是要让x按照batch_size * (image_size//4)^2*depth[1]的方式来排布向量</span></span><br><span class="line">        x = x.view(<span class="number">-1</span>, image_size // <span class="number">4</span> * image_size // <span class="number">4</span> *depth[<span class="number">1</span>])</span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, depth[1]*image_width/4*image_height/4）</span></span><br><span class="line"></span><br><span class="line">        x = F.relu(self.fc1(x))<span class="comment">#第五层为全连接，ReLU激活函数</span></span><br><span class="line">        <span class="comment">#x的尺寸：（batch_size, 512）</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#以默认0.5的概率对这一层进行dropout操作，防止过拟合</span></span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.fc2(x) <span class="comment">#全连接</span></span><br><span class="line">        <span class="comment">#x的尺寸：(batch_size, num_classes)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#输出层为log_softmax,即概率对数值log(p(x)),采用log_softmax可以使后面交叉熵计算更快</span></span><br><span class="line">        x = F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">retrieve_features</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 该函数用于提取卷积神经网络的特征图，返回feature_map1, feature_map2为前两层卷积层的特征图</span></span><br><span class="line">        feature_map1 = F.relu(self.conv1(x)) <span class="comment">#完成第一层卷积</span></span><br><span class="line">        x = self.pool(feature_map1)</span><br><span class="line">        <span class="comment">#第二层卷积，两层特征图都存储到了feature_map1, feature_map2中</span></span><br><span class="line">        feature_map2 = F.relu(self.conv2(x))</span><br><span class="line">        <span class="keyword">return</span> (feature_map1, feature_map2)</span><br></pre></td></tr></table></figure>
<p>  在以上代码中用到了dropout()函数，该函数用来防止神经网络的过拟合情况。在训练过程中，根据一定的概率随机将其中的一些神经元暂时丢弃，最后在测试的时候再使用全部的神经元，增强模型的泛化能力。</p>
<h2 id="运行模型"><a href="#运行模型" class="headerlink" title="运行模型"></a>运行模型</h2><p>  构建好ConvNet之后，就可以读取数据并训练模型了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = ConvNet() <span class="comment">#新建一个卷积神经网络的实例，此时ConvNet的__init__()函数会被自动调用</span></span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss() <span class="comment">#损失函数的定义，交叉熵</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>) <span class="comment">#定义优化器，普通的随机梯度下降算法</span></span><br><span class="line"></span><br><span class="line">record = [] <span class="comment">#记录准确率等数值的容器</span></span><br><span class="line">weights = [] <span class="comment">#每若干步就记录一次卷积核</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rightness</span>(<span class="params">output, target</span>):</span></span><br><span class="line">    preds = output.data.max(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> preds.eq(target.data.view_as(preds)).cpu().sum(), len(target)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始训练循环</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line"></span><br><span class="line">    train_rights = [] <span class="comment">#记录训练数据集准确率的容器</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;下面的enumerate起到构造枚举器的作用，在对train_loader做循环迭代时，enumerate会自动输出一个数字指示循环了几次</span></span><br><span class="line"><span class="string">    并记录在batch_idx中，它就等于0，1，2，...train_loader每迭代一次，就会输出一对数据data和target,分别对应一个批中的</span></span><br><span class="line"><span class="string">    手写数字图及对应的标签。&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> enumerate(train_loader): <span class="comment">#针对容器中的每一批进行循环</span></span><br><span class="line">        <span class="comment">#将Tensor转化为Variable，data为一批图像，target为一批标签</span></span><br><span class="line">        data, target = Variable(data), Variable(target)</span><br><span class="line">        <span class="comment">#给网络模型做标记，标志着模型在训练集上训练</span></span><br><span class="line">        <span class="comment">#这种区分主要是为了打开关闭net的training标志，从而决定是否运行dropout</span></span><br><span class="line">        net.train()</span><br><span class="line"></span><br><span class="line">        output = net(data) <span class="comment">#神经网络完成一次前馈的计算过程，得到预测输出output</span></span><br><span class="line">        loss = criterion(output, target) <span class="comment">#将output与标签target比较，计算误差</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment">#清空梯度</span></span><br><span class="line">        loss.backward() <span class="comment">#反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment">#一步随机梯度下降算法</span></span><br><span class="line">        right = rightness(output, target) <span class="comment">#计算准确率所需数值，返回数值为（正确样例数，总样本数）</span></span><br><span class="line">        train_rights.append(right) <span class="comment">#将计算结果装到列表容器train_rights中</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">100</span> == <span class="number">0</span>: <span class="comment">#每隔100个batch执行一次打印操作</span></span><br><span class="line"></span><br><span class="line">            net.eval() <span class="comment">#给网络模型做标记，标志着模型在训练集上训练</span></span><br><span class="line">            val_rights = [] <span class="comment">#记录校验数据集准确率的标签</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#开始在校验集上做循环，计算校验集上的准确度</span></span><br><span class="line">            <span class="keyword">for</span> (data, target) <span class="keyword">in</span> validation_loader:</span><br><span class="line">                data, target = Variable(data), Variable(target)</span><br><span class="line">                <span class="comment">#完成一次前馈计算过程，得到目前训练得到的模型net在校验集上的表现</span></span><br><span class="line">                output = net(data)</span><br><span class="line">                <span class="comment">#计算准确率所需数值，返回正确的数值为（正确样例数，总样本数）</span></span><br><span class="line">                right = rightness(output, target)</span><br><span class="line">                val_rights.append(right)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#分别计算目前已经计算过的测试集以及全部校验集上模型的表现：分类准确率</span></span><br><span class="line">            <span class="comment">#train_r为一个二元组，分别记录经历过的所有训练集中分类正确的数量和该集合中总的样本数</span></span><br><span class="line">            <span class="comment">#train_r[0]/train_r[1]是训练集的分类准确度，val_r[0]/val_r[1]是校验集的分类准确度</span></span><br><span class="line">            train_r = (sum([tup[<span class="number">0</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> train_rights]),sum([tup[<span class="number">1</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> train_rights]))</span><br><span class="line">            <span class="comment">#val_r为一个二元组，分别记录校验集中分类正确的数量和该集合中的总样本数</span></span><br><span class="line">            val_r = (sum([tup[<span class="number">0</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> val_rights]),sum([tup[<span class="number">1</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> val_rights]))</span><br><span class="line"></span><br><span class="line">            <span class="comment">#打印准确率等数值，其中正确率为样本训练周期epoch开始后到目前批的正确率的平均值</span></span><br><span class="line">            <span class="comment">#print(&#x27;训练周期：&#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\t, Loss:&#123;:.6f&#125;\t, 训练正确率：&#123;:.2f&#125;%\t, 校验正确率：&#123;:.2f&#125;%&#x27;.format(</span></span><br><span class="line">            <span class="comment">#     epoch, batch_idx * len(data), len(train_loader.dataset),</span></span><br><span class="line">            <span class="comment">#     100. * batch_idx / len(train_loader), loss.data[0],</span></span><br><span class="line">            <span class="comment">#     100. * train_r[0] / train_r[1],</span></span><br><span class="line">            <span class="comment">#     100. * val_r[0] / val_r[1]))</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#将准确率和权重等数值加载到容器中，方便后续处理</span></span><br><span class="line">            record.append((<span class="number">100</span> - <span class="number">100.</span> * train_r[<span class="number">0</span>] / train_r[<span class="number">1</span>], <span class="number">100</span> - <span class="number">100.</span> * val_r[<span class="number">0</span>] / val_r[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">            <span class="comment">#wights记录了训练中其中所有卷积演化的过程，net.conv1.weight提取出了第一层卷积核的权重</span></span><br><span class="line">            <span class="comment">#clone是将weight.data中的数据做一个备份放到列表中</span></span><br><span class="line">            <span class="comment">#否则放weight.data变化时，列表中的每一项数值也会联动</span></span><br><span class="line">            <span class="comment">#这里使用clone这个函数很重要</span></span><br><span class="line">            weights.append([net.conv1.weight.data.clone(), net.conv1.bias.data.clone(),</span><br><span class="line">                            net.conv2.weight.data.clone(), net.conv2.bias.data.clone()])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  以上代码中，net.train()会打开所有的dropout层，而net.eval()会关闭它们。</p>
<h2 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#在训练集上分批运行，并计算总的正确率</span></span><br><span class="line">net.eval() <span class="comment">#标志着模型当前的运行阶段</span></span><br><span class="line">vals = [] <span class="comment">#记录准确率所用列表</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#对测试数据集进行循环</span></span><br><span class="line"><span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        data = Variable(data)</span><br><span class="line">    target = Variable(target)</span><br><span class="line">    output = net(data) <span class="comment">#将特征数据输入网络，得到分类的输出</span></span><br><span class="line">    val = rightness(output, target) <span class="comment">#获得正确样本数以及总样本数</span></span><br><span class="line">    vals.append(val) <span class="comment">#记录结果</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算准确率</span></span><br><span class="line">rights = (sum([tup[<span class="number">0</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> vals]),sum([tup[<span class="number">1</span>] <span class="keyword">for</span> tup <span class="keyword">in</span> vals]))</span><br><span class="line">right_rate = <span class="number">1.0</span> * rights[<span class="number">0</span>] / rights[<span class="number">1</span>]</span><br><span class="line">print(right_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制训练过程的误差曲线，校验集和测试集上的错误率</span></span><br><span class="line">plt.figure(figsize = (<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">plt.plot(record) <span class="comment">#record记录了每一个打印周期记录的训练集和校验集上的准确度</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Steps&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Error rate&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>  最后，将训练过程中的误差曲线绘制出来。</p>
<p><img src="pic1.png" alt="1"></p>
<p>  图中左边浅色的为校验数据错误率曲线，右边深色的为测试数据错误率曲线。模型在测试集和校验集上的表现都很好，卷积神经网络的泛化能力也很强。</p>
]]></content>
      <categories>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
</search>
